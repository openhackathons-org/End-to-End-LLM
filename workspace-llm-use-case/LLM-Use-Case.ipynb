{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3fa16d",
   "metadata": {},
   "source": [
    "# End-To-End LLM \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a916c",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "\n",
    "The End-to-End LLM (Large Language Model) Bootcamp is designed from a real-world perspective that follows the data processing, development, and deployment pipeline paradigm. Attendees walk through the workflow of preprocessing the openassistant-guanaco dataset for the Text Generation task and training the dataset using the LLAMA 2 7Billion Model (a pre-trained and fine-tuned Large Language Model). Attendees will also learn to optimize an LLM using NVIDIA TensorRT™ LLM, an SDK for high-performance large language model inference, guardrail prompts and responses from the LLM model using NeMo Guardrails, and deploy the AI pipeline using NVIDIA TensorRT™ LLM Backend (powered by Triton™ Inference Server), open-source software that standardizes LLM deployment and execution across every workload. Furthermore, we introduced a challenge notebook to test your understanding of the material and solidify your experience in the Text Generation domain.\n",
    "\n",
    "\n",
    "### Why End-to-End LLM?\n",
    "\n",
    "Solving real-world problems in the AI domain requires using a set of tools (software stacks and frameworks), and the solution process always follows the `data processing`, `development`, and `deployment` pattern. This material is to:\n",
    "- assist AI hackathon participants to learn and apply the knowledge to solve their tasks using NVIDIA software stacks and frameworks\n",
    "- enables bootcamp attendees to solve real-world problem using end-to-end approach (data processing --> development --> deployment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf6b5a",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "From financial services to eCommerce to telecom and health services, customer care services receive many inquiries from customers or users about their products. Responding to every question is sometimes impossible or may lead to long waiting hours for a face-to-face scenario. The solution is to develop a generative AI-based solution that can efficiently and accurately respond to customers' inquiries using custom, non-static information/data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a45d0",
   "metadata": {},
   "source": [
    "<img src=\"jupyter_notebook/llm-use-case/images/inference-visual-tensor-rt-llm.png\" height=\"800px\" width=\"800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ec875",
   "metadata": {},
   "source": [
    "The table of contents below will walk you through a solution prototype using the `LLM Use Case` lab, and the challenge included will test your understanding of the solution concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586a7da",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "The following contents will be covered for the LLM use case:\n",
    "\n",
    "- Lab 1: [Finetuning Llama2 With Custom Data](jupyter_notebook/llm-use-case/llama-chat-finetune.ipynb)\n",
    "- Lab 2: [Building TensorRT Engine With Finetune Model](jupyter_notebook/llm-use-case/trt-llama-chat.ipynb)\n",
    "- Lab 3: [Deploying Finetune Model using Triton Inference Server](jupyter_notebook/llm-use-case/triton-llama.ipynb)\n",
    "- Lab 4: [Challenge](jupyter_notebook/llm-use-case/challenge.ipynb) \n",
    "- Application of Guardrails \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbf7b2",
   "metadata": {},
   "source": [
    "### Check your GPU\n",
    "\n",
    "Let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the nvidia-smi command. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting `Ctrl-Enter`, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e722a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2d564",
   "metadata": {},
   "source": [
    "### Tutorial Duration\n",
    "\n",
    "The material will be presented in a total of 7hrs: 30mins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f764e8",
   "metadata": {},
   "source": [
    "### Content Level\n",
    "Beginner to Advanced\n",
    "\n",
    "### Target Audience and Prerequisites\n",
    "- The target audience for these labs are researchers, graduate students, and developers interested in the End-to-End approach to solving LLM tasks via GPUs. Audiences are expected to have background Knowledge of Python programming, Pytorch, and Natural Language Processing(NLP). \n",
    "- The challenge lab requires participants to possess a Huggingface security token. Steps can be found [in the link here]( https://huggingface.co/docs/hub/en/security-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d47069",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
