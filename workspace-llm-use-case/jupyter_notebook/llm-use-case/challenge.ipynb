{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d45bec",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"triton-llama.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a>5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a>End</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0724e",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "---\n",
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please Start the Challenge using the Llama-Finetuning Container </div>\n",
    "\n",
    "This challenge notebook is vital to test your understanding and assist you in perfecting what you have learned in the previous notebooks. You are to provide a solution to the problem statement below by following the finetuning process, optimizing to generate tensorrt-llm engine, and deploying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52bf911",
   "metadata": {},
   "source": [
    "## Lab Requirement\n",
    "\n",
    "- Before attempting the challenge lab, you must generate the Huggingface (HF) security token necessary to access the challenge dataset.\n",
    "- Click on the [User Access Token](https://huggingface.co/docs/hub/en/security-tokens) link to see the steps for generating the HF security token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6c56e",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "eCommerce websites receive vast customer feedback through reviews and comments and sometimes have long chats with eCommerce agents. Manually analyzing this feedback is time-consuming, tedious, and prone to errors. The solution is to develop a generative AI-based solution that can efficiently analyze and summarize feedback and chat qualitatively.\n",
    "\n",
    "- **Use Case 1**: Customer feedback and Agent chat summarization\n",
    "- **Domain**: E-commerce\n",
    "- **Dataset**: Salesforce dialogstudio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1ebde",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are to reproduce the End-to-End approach using the `Salesforce dialogstudio` dataset. To complete the challenge, you are to implement the following steps:\n",
    "\n",
    "- Perform the finetuning process\n",
    "    - import needed libraries\n",
    "    - preprocess the dialogstudio TweetSumm (train, validation, test)\n",
    "    - set the path of your base model (Llama-2-7b)\n",
    "    - convert the model to Hugging Face Transformers Format\n",
    "    - initialize paths to the base model, tokenizer, and output checkpoint (`new_model='../../model/Llama-2-7b-hf-finetune'`)\n",
    "    - load tokenizer\n",
    "    - set the training parameter (TrainingArguments object)\n",
    "    - configure PEFT With LoRA (LoraConfig())\n",
    "    - 4-bit Quantization Configuration (BitsAndBytesConfig())\n",
    "    - load base model using AutoModelForCausalLM.from_pretrained()\n",
    "    - set the Trainer Parameter using SFTTrainer()\n",
    "    - apply trainer.train() and your new model to: `../../model/Llama-2-7b-hf-finetune`\n",
    "    - inference your model with provided test data\n",
    "    - merge the base model with the finetune checkpoint and save as `../../model/Llama-2-7b-hf-merged.` \n",
    "- Build TensorRT engine using a single GPU and apply INT8 weight-only quantization\n",
    "    - Convert Model to Tensorrt-llm Checkpoint\n",
    "    - Build Tensorrt Engine\n",
    "    - execute the `summarize.py` script to run inference \n",
    "- Deploy model on Triton server\n",
    "\n",
    "The following links are given for your references:\n",
    "- [Model training hyperparameters](https://huggingface.co/transformers/v3.5.1/model_doc/auto.html)\n",
    "- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#llama)\n",
    "- [Model Deployment](https://github.com/triton-inference-server/tensorrtllm_backend)\n",
    "\n",
    "The data preprocessing part of the solution code is written for you. You are to complete the rest by creating empty cells below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6df982",
   "metadata": {},
   "source": [
    "### Challenge Duration\n",
    "\n",
    "The challenge is expected to take  `3hrs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45882020",
   "metadata": {},
   "source": [
    "## Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a362e",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683db0e",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "- Add your Huggingface security token to the token parameter: `load_dataset(\"Salesforce/dialogstudio\",\"TweetSumm\", trust_remote_code=True, token='hf_Mms….')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Salesforce/dialogstudio\",\"TweetSumm\", trust_remote_code=True, token='')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bac7f2",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
    "        num_rows: 879\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
    "        num_rows: 110\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
    "        num_rows: 110\n",
    "    })\n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76a0ba",
   "metadata": {},
   "source": [
    "#### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5132c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[^ ]+\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def transform_conversation(data):\n",
    "    transformed_text = \"\"\n",
    "    for row in data[\"log\"]:\n",
    "        user = format_text(row[\"user utterance\"])\n",
    "        transformed_text += f\"user: {user.strip()}\\n\"\n",
    "        agent = format_text(row[\"system response\"])\n",
    "        transformed_text += f\"agent: {agent.strip()}\\n\"\n",
    "    return transformed_text\n",
    "\n",
    "\n",
    "def format_training_prompt(conversation, summary):\n",
    "    return f\"\"\"### Instruction: Write  a summary of the conversation below. ### Input: {conversation.strip()} ### Response: {summary} \"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_conversation(data):\n",
    "    summaries = json.loads(data[\"original dialog info\"])[\"summaries\"][\"abstractive_summaries\"]\n",
    "    summary = summaries[0]\n",
    "    summary = \" \".join(summary)\n",
    "    \n",
    "    transformed_text = transform_conversation(data)\n",
    "    return {\n",
    "        \"conversation\": transformed_text,\n",
    "        \"summary\": summary,\n",
    "        \"text\": format_training_prompt(transformed_text, summary),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e82d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = generate_conversation(dataset[\"train\"][0])\n",
    "print(\"summary:\\n\",train_example[\"summary\"], \"\\n\")\n",
    "print(\"conversation:\\n\",train_example[\"conversation\"], \"\\n\")\n",
    "print(\"text:\\n\",train_example[\"text\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf4a2b",
   "metadata": {},
   "source": [
    "**Expected Output :**\n",
    "\n",
    "```text\n",
    "summary:\n",
    " Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it. \n",
    "\n",
    "conversation:\n",
    " user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app?\n",
    " \n",
    "\n",
    "text:\n",
    " ### Instruction: Write  a summary of the conversation below. ### Input: user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ### Response: Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca795d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data: Dataset):\n",
    "    return (\n",
    "         data.shuffle(seed=42).map(generate_conversation).remove_columns(\n",
    "         [\n",
    "          \"original dialog id\",\n",
    "             \"new dialog id\",\n",
    "             \"dialog index\",\n",
    "             \"original dialog info\",\n",
    "             \"log\",\n",
    "             \"prompt\",\n",
    "         ]\n",
    "         )\n",
    "    \n",
    "    )\n",
    "\n",
    "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"test\"] = process_dataset(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b43dca",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['conversation', 'summary', 'text'],\n",
    "        num_rows: 879\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['conversation', 'summary', 'text'],\n",
    "        num_rows: 110\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['conversation', 'summary', 'text'],\n",
    "        num_rows: 110\n",
    "    })\n",
    "})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994091f",
   "metadata": {},
   "source": [
    "#### Download Llama-2-7b\n",
    "\n",
    "Please run the cell below to check if the Llama-2-7b model already exist in your directory otherwise, uncomment the nested cell below to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546776ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -LR ../../model/Llama-2-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342984f",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```python\n",
    "\n",
    "../../model/Llama-2-7b:\n",
    "LICENSE.txt\t\t   USE_POLICY.md\tparams.json\n",
    "README.md\t\t   checklist.chk\ttokenizer.model\n",
    "Responsible-Use-Guide.pdf  consolidated.00.pth\ttokenizer_checklist.chk\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd20a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### download Llama-2-7b model  #############\n",
    "\n",
    "\n",
    "#!python3 ../../source_code/Llama2/download-llama2.py\n",
    "\n",
    "#print(\"extracting files......\")\n",
    "#!tar -xf ../../model/Llama-2-7b.tar  -C ../../model\n",
    "\n",
    "#print(\"files extraction done! removing tar file......\")\n",
    "#!rm -rf ../../model/Llama-2-7b.tar\n",
    "#print(\"All done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/Llama2/llama/convert_llama_weights_to_hf.py \\\n",
    "--input_dir ../../model/Llama-2-7b --model_size 7B --output_dir ../../model/Llama-2-7b-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5fb42",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
    "Fetching all parameters from the checkpoint at ../../model/Llama-2-7b.\n",
    "Loading the checkpoint in a Llama model.\n",
    "Loading checkpoint shards: 100%|████████████████| 33/33 [00:06<00:00,  5.00it/s]\n",
    "Saving in the Transformers format.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailize path to the base model \n",
    "base_model = \"../../model/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100e31f",
   "metadata": {},
   "source": [
    "### Set Tokenizer\n",
    "\n",
    "A comprehensive list of Llama tokenizer parameters can be found [here](https://huggingface.co/docs/transformers/en/model_doc/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0e0f3",
   "metadata": {},
   "source": [
    "### Set Training Arguments\n",
    "\n",
    "Please note that an epoch can take up to 31 minutes. A complete reference on `TrainingArguments` can be found [here](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#trainingarguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24197ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    " output_dir=\"../../model/challenge_results\",\n",
    " num_train_epochs= ,\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace0c30",
   "metadata": {},
   "source": [
    "### Set LoraConfig\n",
    "\n",
    "See references [here](https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    \n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db150d",
   "metadata": {},
   "source": [
    "### Set `BitsAndBytesConfig`\n",
    "\n",
    "You can make use of these references: [HF](https://huggingface.co/docs/transformers/en/main_classes/quantization) and [GitHub](https://github.com/huggingface/transformers/blob/main/docs/source/en/quantization/bitsandbytes.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6549105",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    \n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a60bc",
   "metadata": {},
   "source": [
    "### Load Based Model\n",
    "A reference is provided for you [here](https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#tfautomodelforcausallm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7138222",
   "metadata": {},
   "source": [
    "### Trainer Hyperparameters\n",
    "\n",
    "A detailed reference can be found [here](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c646059",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model= model,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 4096,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b368fe",
   "metadata": {},
   "source": [
    "### Train the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e407d8",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"../../model/Llama-2-7b-hf-finetune\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08391f50",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "\n",
    "('../../model/Llama-2-7b-hf-finetune/tokenizer_config.json',\n",
    " '../../model/Llama-2-7b-hf-finetune/special_tokens_map.json',\n",
    " '../../model/Llama-2-7b-hf-finetune/tokenizer.model',\n",
    " '../../model/Llama-2-7b-hf-finetune/added_tokens.json',\n",
    " '../../model/Llama-2-7b-hf-finetune/tokenizer.json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e32b8",
   "metadata": {},
   "source": [
    "###  Run Inference\n",
    "\n",
    "The functions below format the test data for running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_inference_prompt(conversation, summary):\n",
    "    return f\"\"\"### Instruction: Write  a summary of the conversation below. ### Input: {conversation.strip()} ### Response: \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_examples = []\n",
    "for row in dataset[\"test\"].select(range(3)):\n",
    "    #print(data_point)\n",
    "    prompt_examples.append(\n",
    "        {\n",
    "          \"summary\": row[\"summary\"],\n",
    "            \"conversation\": row[\"conversation\"],\n",
    "            \"prompt\": format_inference_prompt(row[\"conversation\"], row[\"summary\"]),\n",
    "        }    \n",
    "    )\n",
    "    \n",
    "test_df = pd.DataFrame(prompt_examples)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc18c53",
   "metadata": {},
   "source": [
    "#### Create Custom Function to Summarize Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" \n",
    "def summarize(model, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=256, temperature= 1)\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313281d",
   "metadata": {},
   "source": [
    "#### Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073fd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "\n",
    "example = test_df.iloc[0]\n",
    "print(\"Grundtruth : \\n\", example.summary)\n",
    "\n",
    "print(\"Conversation : \\n\", example.conversation)\n",
    "\n",
    "\n",
    "summary = summarize(model, example.prompt)\n",
    "print(\"All Prompt: \\n\", summary,\"\\n\")\n",
    "\n",
    "print(\"Summary Generated : \\n\",summary.strip().split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85641ed",
   "metadata": {},
   "source": [
    "**Likely Output:**\n",
    "\n",
    "```python\n",
    "...\n",
    "Grundtruth : \n",
    " Customer is looking to change the flight on Friday Oct 27 is that an option and asking about cost. Agent replying that there is an difference in fare and this would include all airport taxes and fees and ticket is non refundable changeable with a fee.\n",
    " \n",
    "Conversation : \n",
    " user: looking to change my flight Friday, Oct 27. GRMSKV to DL4728 from SLC to ORD. Is that an option and what is the cost? Jess\n",
    "agent: The difference in fare is $185.30. This would include all airport taxes and fees. The ticket is non-refundable changeable with a fee, *ALS  and may result in additional fare collection for changes when making a future changes. *ALS\n",
    "user: I had a first class seat purchased for the original flight, would that be the same with this flight to Chicago?\n",
    "agent: Hello, Jess. That is the fare difference. You will have to call us at 1 800 221 1212 to make any changes. It is in First class. *TAY\n",
    "user: thx\n",
    "agent: Our pleasure. *ALS\n",
    "user: Do I have to call or is there a means to do this online?\n",
    "agent: You can call or you can login to your trip on our website to make changes. *TJE\n",
    "\n",
    "All Prompt: \n",
    " Customer is enquiring about changing his flight. Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "agent: Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "customer: Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "agent: Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "customer: Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "agent: Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer \n",
    "\n",
    "Summary Generated : \n",
    " Customer is enquiring about changing his flight. Agent updated that the difference in fare is $185.30 and that it is in first class. Also informed that customer can call or login to their trip on their website to make changes.\n",
    "CPU times: user 46.3 s, sys: 2.19 s, total: 48.5 s\n",
    "Wall time: 33.5 s\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e053f6",
   "metadata": {},
   "source": [
    "### Merge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(load_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../../model/challenge/Llama-2-7b-hf-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"../../model/challenge/Llama-2-7b-hf-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867e556",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```python\n",
    "('../../model/challenge/Llama-2-7b-hf-merged/tokenizer_config.json',\n",
    " '../../model/challenge/Llama-2-7b-hf-merged/special_tokens_map.json',\n",
    " '../../model/challenge/Llama-2-7b-hf-merged/tokenizer.model',\n",
    " '../../model/challenge/Llama-2-7b-hf-merged/added_tokens.json',\n",
    " '../../model/challenge/Llama-2-7b-hf-merged/tokenizer.json')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516300e8",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please close the jupyter notebook and switch to the TRT-LLM Container to continue with the next section of the challenge notebook</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f697d0a",
   "metadata": {},
   "source": [
    "### Build the LLaMA 7B model using a single GPU and apply INT8 weight-only quantization\n",
    "\n",
    "Build your model tensorrt engine to this directory: `../../model/challenge/trt_engines/weight_only/1-gpu/`\n",
    "\n",
    "- Convert Model to Tensorrt-llm Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "                            --model_dir /workspace/app/model/challenge/Llama-2-7b-hf-merged  \\\n",
    "                            --output_dir /workspace/app/model/challenge/tllm_checkpoint_1gpu_fp16_wq \\\n",
    "                            --\n",
    "                            --\n",
    "                            --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0ac55",
   "metadata": {},
   "source": [
    "- Build Tensorrt Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########complete the task ###################\n",
    "\n",
    "!trtllm-build --checkpoint_dir /workspace/app/model/challenge/tllm_checkpoint_1gpu_fp16_wq  \\\n",
    "              --output_dir /workspace/app/model/challenge/trt_engines/weight_only/1-gpu/ \\\n",
    "              --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a142333",
   "metadata": {},
   "source": [
    "- - Run Inference using the dataset from `ccdv/cnn_dailymail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/summarize.py \\\n",
    "                       --test_trt_llm \\\n",
    "                       --hf_model_dir /workspace/app/model/challenge/Llama-2-7b-hf-merged  \\\n",
    "                       --data_type fp16 \\\n",
    "                       --engine_dir /workspace/app/model/challenge/trt_engines/weight_only/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670f795",
   "metadata": {},
   "source": [
    "Likely Output:\n",
    "\n",
    "```python\n",
    "...\n",
    "[05/31/2024-15:43:01] [TRT-LLM] [I] Load engine takes: 5.517448663711548 sec\n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I] ---------------------------------------------------------\n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I] TensorRT-LLM Generated : \n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I]  Input : ['(CNN)James Best, best known for his portrayal of bumbling sheriff Rosco P. Coltrane on TV\\'s \"The Dukes of Hazzard,\" died Monday after a brief illness. He was 88. Best died in hospice in Hickory, North Carolina, of complications from pneumonia, said Steve Latshaw, a longtime friend and Hollywood colleague. Although he\\'d been a busy actor for decades in theater and in Hollywood, Best didn\\'t become famous until 1979, when \"The Dukes of Hazzard\\'s\" cornpone charms began beaming into millions of American homes almost every Friday night. For seven seasons, Best\\'s Rosco P. Coltrane chased the moonshine-running Duke boys back and forth across the back roads of fictitious Hazzard County, Georgia, although his \"hot pursuit\" usually ended with him crashing his patrol car. Although Rosco was slow-witted and corrupt, Best gave him a childlike enthusiasm that got laughs and made him endearing. His character became known for his distinctive \"kew-kew-kew\" chuckle and for goofy catchphrases such as \"cuff \\'em and stuff \\'em!\" upon making an arrest. Among the most popular shows on TV in the early \\'80s, \"The Dukes of Hazzard\" ran until 1985 and spawned TV movies, an animated series and video games. Several of Best\\'s \"Hazzard\" co-stars paid tribute to the late actor on social media. \"I laughed and learned more from Jimmie in one hour than from anyone else in a whole year,\" co-star John Schneider, who played Bo Duke, said on Twitter. \"Give Uncle Jesse my love when you see him dear friend.\" \"Jimmy Best was the most constantly creative person I have ever known,\" said Ben Jones, who played mechanic Cooter on the show, in a Facebook post. \"Every minute of his long life was spent acting, writing, producing, painting, teaching, fishing, or involved in another of his life\\'s many passions.\" Born Jewel Guy on July 26, 1926, in Powderly, Kentucky, Best was orphaned at 3 and adopted by Armen and Essa Best, who renamed him James and raised him in rural Indiana. Best served in the Army during World War II before launching his acting career. In the 1950s and 1960s, he accumulated scores of credits, playing a range of colorful supporting characters in such TV shows as \"The Twilight Zone,\" \"Bonanza,\" \"The Andy Griffith Show\" and \"Gunsmoke.\" He later appeared in a handful of Burt Reynolds\\' movies, including \"Hooper\" and \"The End.\" But Best will always be best known for his \"Hazzard\" role, which lives on in reruns. \"Jimmie was my teacher, mentor, close friend and collaborator for 26 years,\" Latshaw said. \"I directed two of his feature films, including the recent \\'Return of the Killer Shrews,\\' a sequel he co-wrote and was quite proud of as he had made the first one more than 50 years earlier.\" People we\\'ve lost in 2015 . CNN\\'s Stella Chan contributed to this story.']\n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I] \n",
    " Reference : ['James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 .\\n\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV .']\n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I] \n",
    " Output : [['James Best, best known for his portrayal of bumbling sheriff Rosco P. Coltrane on TV\\'s \"The Dukes of Hazzard,\" died Monday after a brief illness. He was 88. Best died in hospice in Hickory, North Carolina, of complications from pneumonia, said Steve Latshaw, a longtime friend and Hollywood colleague. Although he\\'d been a busy actor for decades in']]\n",
    "[05/31/2024-15:43:02] [TRT-LLM] [I] ---------------------------------------------------------\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I] TensorRT-LLM (total latency: 13.475271701812744 sec)\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I] TensorRT-LLM (total output tokens: 2000)\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I] TensorRT-LLM (tokens per second: 148.42001291379918)\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I] TensorRT-LLM beam 0 result\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I]   rouge1 : 23.96478385018718\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I]   rouge2 : 6.18847126286389\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I]   rougeL : 16.39895867154953\n",
    "[05/31/2024-15:43:17] [TRT-LLM] [I]   rougeLsum : 19.90993393703445\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b7419",
   "metadata": {},
   "source": [
    "---\n",
    "### Deploy Model on Triton Server\n",
    "- You may want to refresh your memory on the process by following the steps in the <a href=\"triton-llama.ipynb\">triton-llama.ipynb notebook</a>\n",
    "- Copy the TRT engine to triton_model_repo/tensorrt_llm/1 : `cp  /workspace/app/model/challenge/trt_engines/weight_only/1-gpu/* triton_model_repo/tensorrt_llm/3`\n",
    "- Modify the config files for model Preprocessing, tensorrt_llm, and Postprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950eda30",
   "metadata": {},
   "source": [
    "### Modify the model configuration\n",
    "\n",
    "If you haven't successfully run this lab (<a href=\"triton-llama.ipynb\">triton-llama.ipynb</a>) before, please execute **Option 1** otherwise, follow **Option 2**\n",
    "\n",
    "#### Option 1: Run The Cell Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e225f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensorrtllm_backend to source_code\n",
    "!cp -r /workspace/tensorrtllm_backend /workspace/app/source_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ac813",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd /workspace/app/source_code/tensorrtllm_backend/\n",
    "mkdir triton_model_repo\n",
    "\n",
    "# Copy the example models to the model repository\n",
    "mkdir -p triton_model_repo/tensorrt_llm/3\n",
    "cp -r all_models/inflight_batcher_llm/* triton_model_repo/\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/1/\n",
    "cp  /workspace/app/model/trt_engines/fp16/1-gpu/* triton_model_repo/tensorrt_llm/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5376551",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run this script to set the right key-value pairs automatically.\n",
    "\n",
    "cd /workspace/app/source_code/tensorrtllm_backend/\n",
    "\n",
    "export HF_LLAMA_MODEL=\"/workspace/app/model/challenge/Llama-2-7b-hf-merged \"\n",
    "export    ENGINE_PATH=\"/workspace/app/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/3\" \n",
    "export BACKEND=\"tensorrtllm\"\n",
    "\n",
    "\n",
    "python3 tools/fill_template.py -i triton_model_repo/preprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:64,preprocessing_instance_count:1\n",
    "python3 tools/fill_template.py -i triton_model_repo/postprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:64,postprocessing_instance_count:1\n",
    "python3 tools/fill_template.py -i triton_model_repo/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False\n",
    "python3 tools/fill_template.py -i triton_model_repo/ensemble/config.pbtxt triton_max_batch_size:64\n",
    "python3 tools/fill_template.py -i triton_model_repo/tensorrt_llm/config.pbtxt triton_backend:${BACKEND},triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:V1,max_queue_delay_microseconds:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee62ff",
   "metadata": {},
   "source": [
    "#### Option 2: Complete The Task Below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd /workspace/app/source_code/tensorrtllm_backend/\n",
    "\n",
    "# make TRT engine folder for triton \n",
    "mkdir -p triton_model_repo/tensorrt_llm/3\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/3\n",
    "cp  /workspace/app/model/challenge/trt_engines/weight_only/1-gpu/* triton_model_repo/tensorrt_llm/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe03c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- a) Make changes in the **Pre-processing config file** *[triton_model_repo/preprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/preprocessing/config.pbtxt)*\n",
    "\n",
    "| Parameters | value | \n",
    "| :----------------------: | :-----------------------------: | \n",
    "| `tokenizer_dir` | **`/workspace/app/model/challenge/Llama-2-7b-hf-merged`**|\n",
    "| `triton_max_batch_size` |64|\n",
    "| `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "- b) Make changes in the **Post-processing config file**  *[triton_model_repo/postprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/postprocessing/config.pbtxt)*\n",
    "\n",
    "| Parameters | value | \n",
    "| :----------------------: | :-----------------------------: | \n",
    "| `tokenizer_dir` | **`/workspace/app/model/challenge/Llama-2-7b-hf-merged`**|\n",
    "| `triton_max_batch_size` |64|\n",
    "| `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "\n",
    "- c)  Make changes in the **tensorrt_llm config file**  *[triton_model_repo/tensorrt_llm/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "| Name | Description\n",
    "| :----------------------: | :-----------------------------: |\n",
    "|  `triton_backend`        |    \"tensorrtllm\"                |\n",
    "|`triton_max_batch_size` | 64 |\n",
    "|`decoupled_mode` | False|\n",
    "|`max_beam_width` | 1 |\n",
    "|`engine_dir` |  **`/workspace/app/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/3`** |\n",
    "|`max_tokens_in_paged_kv_cache` | 2560|\n",
    "|max_attention_window_size|2560|\n",
    "|kv_cache_free_gpu_mem_fraction |0.5 |\n",
    "|exclude_input_in_output |True |\n",
    "|enable_kv_cache_reuse | False|\n",
    "|batching_strategy |V1 |\n",
    "|max_queue_delay_microseconds | 0|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d331b49",
   "metadata": {},
   "source": [
    "### Launch Triton server\n",
    "\n",
    "We can launch the Triton server with the following command:\n",
    "\n",
    "- Press `Crtl+Shift+L` and open a new terminal\n",
    "- On the terminal, navigate to the launch script folder by running this command: `cd /workspace/app/source_code/tensorrtllm_backend`\n",
    "- Start the Triton Server with this command: `python3 scripts/launch_triton_server.py  --world_size=1  --model_repo=/workspace/app/source_code/tensorrtllm_backend/triton_model_repo`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_infernece_prompt():\n",
    "    return f\"\"\"### Instruction: Write  a summary of the conversation below. ### Input: user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ### Response: \"\"\n",
    "\"\"\".strip()\n",
    "\n",
    "INPUT_TEXT = format_infernece_prompt() \n",
    "\n",
    "grundtruth =\"Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81075519",
   "metadata": {},
   "source": [
    "### Querying using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = os.getenv('HTTP_PORT')\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Complete the Rest of the Code. In the payload please set \"max_tokens\": 256 ########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee65d29",
   "metadata": {},
   "source": [
    "Likely Output:\n",
    "\n",
    "```python\n",
    "Input: ### Instruction: Write  a summary of the conversation below. ### Input: user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?   please read the above.\n",
    "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
    "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
    "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
    "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
    "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
    "user: Yes, everything seems fine, it’s just Health and activity.\n",
    "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ### Response: \"\"\n",
    "Output: customer is complaining that his iPhone and apple watch are not recording his steps and activity and health doesn't recognize either source anymore for some reason. Agent asked to restart both devices and asked to DM and look into this a bit more. Agent asked to let them know when this first started happening and asked to DM when reaching out in DM. Agent asked to let them know when this first started happening and asked to DM when reaching out in DM.\"\n",
    "agent: Let's move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app?\n",
    "user: It started after I updated to 11.1.2.\n",
    "agent: Got it. Let's move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ### Response: Customer is complaining that his iPhone and apple watch are not recording his steps and activity and health doesn't recognize either source anymore for some reason. Agent asked to restart both devices and asked to DM and look into this a\n",
    "\n",
    "  Summary: \n",
    "\n",
    "customer is complaining that his iPhone and apple watch are not recording his steps and activity and health doesn't recognize either source anymore for some reason. Agent asked to restart both devices and asked to DM and look into this a bit more. Agent asked to let them know when this first started happening and asked to DM when reaching out in DM. Agent asked to let them know when this first started happening and asked to DM when reaching out in DM.\"\n",
    "\n",
    "  Grundtruth: \n",
    "\n",
    "Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de213caa",
   "metadata": {},
   "source": [
    "---\n",
    "You can check for the solution prototype [<a href=\"solution.ipynb\">here</a>]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd657cf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80051c7a",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"triton-llama.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "       <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a>5</a>\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
