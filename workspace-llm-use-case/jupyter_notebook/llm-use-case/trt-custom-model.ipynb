{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0525fde",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-llama-chat.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "          <a>3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"LangChain-with-Guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"triton-llama.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e985ed3",
   "metadata": {},
   "source": [
    "# TensorRT-LLM: Adding Custom Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28c5f8",
   "metadata": {},
   "source": [
    "### Setup TRT-LLM Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5701736",
   "metadata": {},
   "source": [
    "TensorRT-LLM must be built from source depending on the Hardware environment we use it in to achieve ideal performance. \n",
    "\n",
    "**Note: This step should be skipped if you have already built this via Dockerfile or Singularity recipe or running this in a bootcamp environment.**\n",
    "\n",
    "There are multiple ways to set up TensorRT-LLM; one recommended approach is as follows: \n",
    "\n",
    "\n",
    "#### 1). Download sources \n",
    "\n",
    "```bash\n",
    "# TensorRT-LLM uses git-lfs, which needs to be installed in advance.\n",
    "apt-get update && apt-get -y install git git-lfs cmake\n",
    "\n",
    "# Clone Repository and set it up\n",
    "git clone https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "cd TensorRT-LLM\n",
    "git submodule update --init --recursive\n",
    "git lfs install\n",
    "git lfs pull\n",
    "```\n",
    "\n",
    "#### 2). Using Docker \n",
    "\n",
    "```bash\n",
    "# Using the make to build using Docker\n",
    "make -C docker release_build\n",
    "```\n",
    "\n",
    "For a complete list of options to set up TRT-LLM kindly refer [**here**](https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/installation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119b385",
   "metadata": {},
   "source": [
    "### Workflow of Integrating New Models using TensorRT-LLM: \n",
    "\n",
    "Let us start by understanding the workflow of TensorRT-LLM so we can learn how we can integrate new models into TensorRT-LLM:\n",
    "\n",
    "- **Step 1** : Convert weights from different source frameworks into TensorRT-LLM checkpoint\n",
    "\n",
    "- **Step 2** : Build the TensorRT-LLM checkpoint into TensorRT engine(s) with a unified build command\n",
    "\n",
    "- **Step 3** : Load the engine(s) to the TensorRT-LLM model runner and evaluate with different evaluation tasks\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/workflow.png\" width=\"1000\"/>\n",
    "</center></div>  \n",
    "\n",
    "\n",
    "The above image is a pictorial representation of the workflow described. Let us understand the steps in detail using the LLaMA 7b Model, as we will use the same model in the upcoming notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b643a7a",
   "metadata": {},
   "source": [
    "#### **Step 1** :  Convert to TensorRT-LLM Checkpoint format\n",
    "\n",
    "First, we start off by converting checkpoints from other sources to the TensorRT-LLM Checkpoint format. Continuing with our example, let us look at the current format of LLaMA 7b model that we currently have. \n",
    "\n",
    "We see the following files: \n",
    "\n",
    "- **README.md**: This is a markdown file typically used to provide information about the software or project, including how to install, configure, and use it, as well as any other relevant details or documentation.\n",
    "- **checklist.chk**: This file is an MD5 checksum file used to verify the integrity of the other files in the directory. It ensures that the files are not corrupted or altered from their original state.\n",
    "- **consolidated.00.pth**: This is a PyTorch model file that contains the trained model weights. PyTorch uses the .pth extension for saving model checkpoints.\n",
    "- **params.json**: A JSON file containing model parameters. For the 7B model, it includes details such as the dimensionality of the model, the number of heads, layers, and other configuration details that are necessary for initializing and running the model.\n",
    "- **tokenizer.model**: This file is associated with the tokenizer used by the LLaMA model. A tokenizer is responsible for converting text into a format that the model can understand, typically by breaking text into tokens and converting these tokens into numerical representations.\n",
    "\n",
    "\n",
    "Since the model is trained in Pytorch, we can load it and see the weight tensors below: \n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/meta-ckpt.png\" width=\"1500\"/>\n",
    "</center></div>  \n",
    "\n",
    "Before we convert this specific Meta checkpoint to TensorRT-LLM format, let us look at what is the checkpoint format. \n",
    "\n",
    "TensorRT-LLM defines its own checkpoint format. A checkpoint directory includes:\n",
    "\n",
    "- One **config json** file, which contains several model hyper-parameters\n",
    "- **One or several rank weights** files, each file contains a dictionary of tensors (weights). Different ranks will load the different files in a multi-GPU (multi-process) scenario.\n",
    "\n",
    "##### **Config**\n",
    "\n",
    "The `config.json` contains important hyper-parameters of the model. A complete hyper-parameter list can be found [here](https://nvidia.github.io/TensorRT-LLM/new_workflow.html#config). Let us look at an example of the `config.json` :\n",
    "\n",
    "```json \n",
    "\n",
    "{\n",
    "    \"architecture\": \"OPTForCausalLM\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"logits_dtype\": \"float32\",\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"vocab_size\": 50272,\n",
    "    \"position_embedding_type\": \"learned_absolute\",\n",
    "    \"max_position_embeddings\": 2048,\n",
    "    \"hidden_act\": \"relu\",\n",
    "    \"quantization\": {\n",
    "        \"use_weight_only\": false,\n",
    "        \"weight_only_precision\": \"int8\"\n",
    "    },\n",
    "    \"mapping\": {\n",
    "        \"world_size\": 2,\n",
    "        \"tp_size\": 2\n",
    "    },\n",
    "    \"use_parallel_embedding\": false,\n",
    "    \"embedding_sharding_dim\": 0,\n",
    "    \"share_embedding_table\": false,\n",
    "    \"do_layer_norm_before\": true,\n",
    "    \"use_prompt_tuning\": false\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "##### **Rank Weights**\n",
    "\n",
    "Like PyTorch, the tensor(weight) name is a string containing hierarchical information, uniquely mapped to a particular parameter of a TensorRT-LLM model.\n",
    "\n",
    "Let us look at an example through the Attention weights of the 0-th transformer layer.\n",
    "\n",
    "The Attention layer contains two Linear layers, `qkv` and `dense`; each Linear layer contains one weight and one bias. So, there are four tensors (weights) in total, whose names are:\n",
    "\n",
    "```python\n",
    "transformer.layers.0.attention.qkv.weight\n",
    "transformer.layers.0.attention.qkv.bias\n",
    "transformer.layers.0.attention.dense.weight\n",
    "transformer.layers.0.attention.dense.bias\n",
    "```\n",
    "\n",
    "where `transformer.layers.0.attention` is the prefix name, indicating that the weights/biases are in the attention module of the 0-th transformer layer. A complete example of converting various layers can be found [here](https://nvidia.github.io/TensorRT-LLM/new_workflow.html#rank-weights).\n",
    "\n",
    "With our understanding of Step 1 of converting checkpoint to the TensorRT-LLM format. Let us now look at Step 2 of the process, where we integrate the Model and its Architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e1db3",
   "metadata": {},
   "source": [
    "#### **Step 2** : Workflow of LLaMA - Implementing the Model architecture\n",
    "\n",
    "**TensorRT-LLM Python API :**\n",
    "\n",
    "Before we head into defining our Architecture, let us understand that TensorRT-LLM has a Python API that can be used to define Large Language Models. This API is built on top of the powerful TensorRT Python API to create graph representations of deep neural networks in TensorRT. The [Documentation](https://nvidia.github.io/TensorRT-LLM/python-api/tensorrt_llm.layers.html) has a list of Python API that help accelerate building the model. Below is a snippet from the documentation showing some Python APIs available to us via TensorRT-LLM. \n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/pythonapi.png\" width=\"1000\"/>\n",
    "</center></div>\n",
    "\n",
    "Let us now  go back to our example of using LLaMA. The LLaMA papers with their modification to the transformer can be found here: [LLaMA 1](https://arxiv.org/abs/2302.13971) , [LLaMA 2](https://arxiv.org/abs/2307.09288). \n",
    "\n",
    "We now visualise this and compare it to the `model.py` we currently have as part of the TensorRT-LLM repository. \n",
    "\n",
    "There are three Primary functions that we call are:\n",
    "\n",
    "- **Embedding** : The embedding layer translates tokens into a format the model can work with.\n",
    "- **Decoder Layer** : Decoder layers process and refine these representations through attention mechanisms and neural networks. We will look at them in detail below.\n",
    "- **RMS Norm** : RMSNorm ensures that the output activations are normalized in a manner that promotes stability and efficiency.\n",
    "\n",
    "\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/arch.png\" width=\"1000\"/>\n",
    "</center></div>  \n",
    "\n",
    "##### **Decoder Layer**\n",
    "\n",
    "\n",
    "The Decoder layer in LLaMA 2 is a fundamental component of the model's architecture, primarily responsible for processing and generating language. Each Decoder layer, or transformer block, is constructed from a self-attention layer and a feed-forward neural network. The decoder is built of three fundamental building blocks:\n",
    "\n",
    "- RMS Norm \n",
    "- Attention\n",
    "- MLP \n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/decode-init.png\" width=\"1300\"/>\n",
    "</center></div>  \n",
    "\n",
    "\n",
    "While the `init` method covers the functions being initialised, we can get a better idea of the flow when we look into the `forward` method of the Decoder Class. \n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/decode-forward.png\" width=\"1200\"/>\n",
    "</center></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31392d65",
   "metadata": {},
   "source": [
    "#### **Step 3** : Running the checkpoint file through ModelRunner. \n",
    "\n",
    "The Model Runner is designed to work with the TensorRT-LLM Python API, allowing users to load the engines and evaluate different tasks quickly. It supports various functionalities such as multi-GPU and multi-node configurations, enabling efficient execution of Large Language Models (LLMs) across different hardware setups. A flow of the [`run.py`](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/run.py) that can be used with all the models is given below.  \n",
    "\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/modelrunner.png\" width=\"1500\"/>\n",
    "</center></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98ef70",
   "metadata": {},
   "source": [
    "### Documentation is your friend\n",
    "\n",
    "Documentation plays a pivotal role in implementing custom models in TensorRT-LLM (TRT-LLM), acting as a comprehensive guide through the intricacies of the process. For developers looking to optimize and execute Large Language Models (LLMs) efficiently on NVIDIA GPUs, the TRT-LLM documentation provides essential insights into the architecture, APIs, and best practices for performance tuning. It covers the Python API for defining models, compiling efficient engines, and building runtimes for executing those engines alongside C++ components for more advanced use cases. Moreover, the documentation outlines the new workflow for converting model weights from various source frameworks into a TRT-LLM compatible format, building these into TensorRT engines, and finally, running inference with these optimized engines.\n",
    "\n",
    "The Documentation can be found [here](https://nvidia.github.io/TensorRT-LLM/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c546a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgment\n",
    "\n",
    "This notebook is adapt from NVIDIA's [TensorRT-LLM Github repository](https://github.com/NVIDIA/TensorRT-LLM/tree/main)\n",
    "\n",
    "## References\n",
    "\n",
    "- https://nvidia.github.io/TensorRT-LLM/architecture.html\n",
    "- https://github.com/NVIDIA/TensorRT-LLM\n",
    "\n",
    "## Licensing\n",
    "Copyright © 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ce9fd",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-llama-chat.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "          <a>3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"LangChain-with-Guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"triton-llama.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
