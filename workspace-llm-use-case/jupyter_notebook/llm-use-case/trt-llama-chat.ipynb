{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ad3ab9",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"llama-chat-finetune.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a>2</a>\n",
    "          <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"LangChain-with-Guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"trt-custom-model.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a76fe7",
   "metadata": {},
   "source": [
    "# Building TensorRT-LLM Engine With Finetuned Model  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4824a",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please note that you can run this lab only by using the TRT-LLM Container</div>\n",
    "\n",
    "The objective of this notebook is to demonstrate the use of TensorRT-LLM to optimize our finetuned Llama-2-7b-chat (`../model/Llama-2-7b-chat-hf-merged`) from the previous notebook, run inference, and examine using various advance optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178282c",
   "metadata": {},
   "source": [
    "### Overview of TensorRT-LLM \n",
    "\n",
    "[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main) is a toolkit to assemble optimized solutions to perform Large Language Model (LLM) inference. It offers a Python API to define models and compile efficiently [TensorRT](https://developer.nvidia.com/tensorrt) engines for NVIDIA GPUs. It also contains Python and C++ components to build runtimes to execute those engines as well as [backends](https://github.com/triton-inference-server/tensorrtllm_backend) for the [Triton Inference Server](https://developer.nvidia.com/triton-inference-server) to create web-based services for LLMs easily. TensorRT-LLM supports single GPU, multi-GPU and multi-node configurations (using [Tensor Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#tensor-parallelism) and/or [Pipeline Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#pipeline-parallelism)). TensorRT-LLM wraps TensorRT’s deep learning compiler—which includes optimized kernels from FasterTransformer, pre- and post-processing, and multi-GPU and multi-node communication—in a simple open-source Python API for defining, optimizing, and executing LLMs for inference in production.\n",
    "\n",
    "The Python API of TensorRT-LLM is architectured to look similar to the PyTorch API. It provides users with a functional module containing functions like `einsum`, `softmax`, `matmul`, or `view`. TensorRT-LLM maximizes performance and reduces memory footprint by allowing models to be executed using different quantization modes. Thus, it supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the [SmoothQuant technique](https://arxiv.org/abs/2211.10438)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6795d0b",
   "metadata": {},
   "source": [
    "### Key Features of TensorRT-LLM\n",
    "\n",
    "TensorRT-LLM contains examples that implement the following features.\n",
    "\n",
    "* Multi-head Attention([MHA](https://arxiv.org/abs/1706.03762))\n",
    "* Multi-query Attention ([MQA](https://arxiv.org/abs/1911.02150))\n",
    "* Group-query Attention([GQA](https://arxiv.org/abs/2307.09288))\n",
    "* In-flight Batching\n",
    "* Paged KV Cache for the Attention\n",
    "* Tensor Parallelism\n",
    "* Pipeline Parallelism\n",
    "* INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)\n",
    "* [SmoothQuant](https://arxiv.org/abs/2211.10438)\n",
    "* [GPTQ](https://arxiv.org/abs/2210.17323)\n",
    "* [AWQ](https://arxiv.org/abs/2306.00978)\n",
    "* [FP8](https://arxiv.org/abs/2209.05433)\n",
    "* Greedy-search\n",
    "* Beam-search\n",
    "* RoPE\n",
    "\n",
    "Some of the features are not enabled for all the [models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm/models). Please find a list of TensorRT-LLM supported models [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples)\n",
    "\n",
    "### Support Device\n",
    "\n",
    "TensorRT-LLM is rigorously tested on the following GPUs:\n",
    "\n",
    "- H100\n",
    "- L40S\n",
    "- A100\n",
    "- A30\n",
    "- V100 (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d97b3",
   "metadata": {},
   "source": [
    "## Building TensorRT-LLM  engine(s) for LLAMA 2\n",
    "\n",
    "This section shows how to build tensorrt engine(s) using our merged model. Firstly, we used the `convert_checkpoint.py`  script to convert our `Llama-2-7b-chat-hf-merged` into tensorrt-llm checkpoint format. We use the `trtllm-build` command to build our tensorrt engine using a single GPU and FP16. The last step is to run the inference using the `run.py` script. Before we proceed to build our engine, it is important to be aware of the supported matrixes for Llama-2 as listed below:\n",
    "\n",
    "- FP16\n",
    "- FP8\n",
    "- INT8 & INT4 Weight-Only\n",
    "- SmoothQuant\n",
    "- Groupwise quantization (AWQ/GPTQ)\n",
    "- FP8 KV CACHE\n",
    "- INT8 KV CACHE (+ AWQ/per-channel weight-only)\n",
    "- Tensor Parallel\n",
    "- STRONGLY TYPED\n",
    "\n",
    "**flag description**:\n",
    "- **model_dir**: path to the model directory \n",
    "- **output_dir**: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- **dtype**:  data type to use for model conversion to tensorrt-llm checkpoint\n",
    "- **checkpoint_dir**: path to the directory to load the tensorrt-llm checkpoint needed to build the tensorrt engine\n",
    "- **gemm_plugin**: required plugin to prevent accuracy issue\n",
    "- **gpt_attention_plugin**: GPT attention plugin\n",
    "- **weight_only_precisio**n: required weight precision to build tensorrt engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0dff8f",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and FP16 \n",
    "\n",
    "- Convert Model to Tensorrt-llm Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "                              --model_dir /workspace/app/model/Llama-2-7b-chat-hf-merged \\\n",
    "                              --output_dir /workspace/app/model/tllm_checkpoint_1gpu_fp16 \\\n",
    "                              --dtype float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a85aef0",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```python\n",
    "[TensorRT-LLM] TensorRT-LLM version: 0.9.0.dev20240206000.9.0.dev2024020600\n",
    "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:07<00:00,  2.35s/it]\n",
    "Weights loaded. Total time: 00:00:00\n",
    "Total time of converting checkpoints: 00:00:26\n",
    "\n",
    "```\n",
    "- Build Tensorrt Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cea726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!trtllm-build --checkpoint_dir /workspace/app/model/tllm_checkpoint_1gpu_fp16 \\\n",
    "            --output_dir /workspace/app/model/trt_engines/fp16/1-gpu \\\n",
    "            --gemm_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bb154",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```python\n",
    "[TensorRT-LLM] TensorRT-LLM version: 0.9.0.dev2024020600[02/16/2024-21:44:20] [TRT-LLM] [I] Set bert_attention_plugin to float16.\n",
    "[02/16/2024-21:44:20] [TRT-LLM] [I] Set gpt_attention_plugin to float16.\n",
    "[02/16/2024-21:44:20] [TRT-LLM] [I] Set gemm_plugin to float16.\n",
    "...\n",
    "[02/16/2024-21:44:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12853, now: CPU 0, GPU 12853 (MiB)\n",
    "[02/16/2024-21:44:56] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 29189 MiB\n",
    "[02/16/2024-21:44:56] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:22\n",
    "[02/16/2024-21:44:57] [TRT-LLM] [I] Serializing engine to /workspace/app/model/trt_engines/fp16/1-gpu/rank0.engine...\n",
    "[02/16/2024-21:45:06] [TRT-LLM] [I] Engine serialized. Total time: 00:00:09\n",
    "[02/16/2024-21:45:07] [TRT-LLM] [I] Total time of building all engines: 00:00:46\n",
    "```\n",
    "- Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ae692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With fp16 inference\n",
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "                  --max_output_len=200 \\\n",
    "                  --tokenizer_dir /workspace/app/model/Llama-2-7b-chat-hf-merged \\\n",
    "                  --engine_dir=/workspace/app/model/trt_engines/fp16/1-gpu \\\n",
    "                  --input_text \"explain what is astrophotography?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9195ccf8",
   "metadata": {},
   "source": [
    "Expected output: \n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "[TensorRT-LLM][INFO] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 12852 (MiB)\n",
    "[TensorRT-LLM][INFO] Allocate 6442450944 bytes for k/v cache. \n",
    "[TensorRT-LLM][INFO] Using 12288 tokens in paged KV cache.\n",
    "[TensorRT-LLM] TensorRT-LLM version: 0.9.0.dev2024020600Input [Text 0]: \"<s> explain what is astrophotography?\"\n",
    "Output [Text 0 Beam 0]: \"[/INST] Astrophotography is the branch of photography that deals with the photographing of celestial objects such as stars, galaxies, nebulae, and planets. It involves the use of specialized cameras and techniques to capture images of these objects in the night sky. Astrophotography requires a deep understanding of the principles of photography, as well as knowledge of astronomy and the behavior of celestial objects. Astrophotographers often use telescopes and other specialized equipment to capture images that are otherwise invisible to the naked eye. Astrophotography is a popular hobby and has become increasingly accessible with the advancement of digital technology, allowing more people to capture stunning images of the night sky.\n",
    "\n",
    "Astrophotography can be divided into several sub-categories, including:\n",
    "\n",
    "1. Deep-sky astrophotography: This involves the photographing of large, distant objects such\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20025bb",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "                              --model_dir /workspace/app/model/Llama-2-7b-chat-hf-merged  \\\n",
    "                              --output_dir /workspace/app/model/tllm_checkpoint_1gpu_bf16 \\\n",
    "                              --dtype bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abceae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!trtllm-build --checkpoint_dir /workspace/app/model/tllm_checkpoint_1gpu_bf16 \\\n",
    "            --output_dir /workspace/app/model/trt_engines/bf16/1-gpu \\\n",
    "            --gpt_attention_plugin bfloat16 \\\n",
    "            --gemm_plugin bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "                  --max_output_len=200 \\\n",
    "                  --tokenizer_dir /workspace/app/model/Llama-2-7b-chat-hf-merged \\\n",
    "                  --engine_dir=/workspace/app/model/trt_engines/bf16/1-gpu \\\n",
    "                  --input_text \"explain what is astrophotography?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474abf54",
   "metadata": {},
   "source": [
    "#### Build the LLaMA 7B model using a single GPU and apply INT8 weight-only quantization\n",
    "\n",
    "- Convert Model to Tensorrt-llm Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90b1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py \\\n",
    "                              --model_dir /workspace/app/model/Llama-2-7b-chat-hf-merged  \\\n",
    "                              --output_dir /workspace/app/model/tllm_checkpoint_1gpu_fp16_wq \\\n",
    "                              --dtype float16 \\\n",
    "                              --use_weight_only \\\n",
    "                              --weight_only_precision int8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9f658",
   "metadata": {},
   "source": [
    "- Build Tensorrt Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4ec52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!trtllm-build --checkpoint_dir /workspace/app/model/tllm_checkpoint_1gpu_fp16_wq  \\\n",
    "            --output_dir /workspace/app/model/trt_engines/weight_only/1-gpu/ \\\n",
    "            --gemm_plugin float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb60e2",
   "metadata": {},
   "source": [
    "- Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "                  --max_output_len=200 \\\n",
    "                  --tokenizer_dir /workspace/app/model/Llama-2-7b-chat-hf-merged  \\\n",
    "                  --engine_dir=/workspace/app/model/trt_engines/weight_only/1-gpu/ \\\n",
    "                  --input_text \"explain what is astrophotography?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e97fcc",
   "metadata": {},
   "source": [
    "#### Other methods to Build and Run LLaMA-2 7B, 30B, and 70B\n",
    "\n",
    "- 2-way tensor parallelism.\n",
    "- 2-way tensor parallelism and 2-way pipeline parallelism\n",
    "- 8-way tensor parallelism for 70B\n",
    "- 4-way tensor parallelism and 2-way pipeline parallelism for 70B\n",
    "- Build LLaMA 70B TP=8 using Meta checkpoints directly.\n",
    "\n",
    "Please find examples for the listed methods [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c67a7",
   "metadata": {},
   "source": [
    "### Advance OPtimization Techniques\n",
    "\n",
    "- **Quantization** \n",
    "\n",
    "TensorRT-LLM implements different quantization methods with support matrices for the different models. Given a matrix (2D tensor) of shape M x N (M rows and N columns) where M is the number of tokens and N is the number of channels. TensorRT-LLM has the three following modes to quantize and dequantize the elements of the tensor:\n",
    "\n",
    "    - Per-tensor: It uses a single scaling factor for all the elements,\n",
    "    - Per-token: It uses a different scaling factor for each token. There are M scaling factors in that case,\n",
    "    - Per-channel: It uses a different scaling factor for each channel. There are N scaling factors in that case.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Per-tensor scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s)\n",
    "\n",
    "# Per-token scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s[mi])\n",
    "\n",
    "# Per-channel scaling.\n",
    "for mi in range(M):\n",
    "    for ni in range(N):\n",
    "        q[mi][ni] = int8.satfinite(x[mi][ni] * s[ni])\n",
    "\n",
    "```\n",
    "Use the [link](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/precision.md) to explore more topics that include INT8 SmoothQuant, INT4 and INT8 Weight-Only, GPTQ, and AWQ.\n",
    "\n",
    "- **In-flight Batching**\n",
    "\n",
    "In-flight Batching is also known as continuous batching or iteration-level batching. The technique aims to reduce wait times in queues, eliminate the need for padding requests, and allow for higher GPU utilization. TensorRT-LLM uses the Batch Manager component to support in-flight batching of requests. More on The Batch Manager API can be found [here](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/batch_manager.md)\n",
    "\n",
    "\n",
    "- **Multi-head, Multi-query and Group-query Attention**\n",
    "\n",
    "Multi-head(MHA), Multi-query(MQA), and Group-query Attention(GQA) are variants of the attention mechanism found in most Large Language Models and are implemented and optimized in TensorRT-LLM. The [MHA](https://arxiv.org/abs/1706.03762) is the sequence of a batched matmul, a softmax, and another batched matmul while [MQA](https://arxiv.org/abs/1911.02150) and [GQA](https://arxiv.org/abs/2307.09288) are variants of MHA that use fewer, so-called, K/V head than the number of query heads. This [document](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md) summarizes those implementations in TensorRT-LLM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f756207",
   "metadata": {},
   "source": [
    "## Performance of TensorRT-LLM\n",
    "\n",
    "\n",
    "The data in the following tables is a reference point to help users validate observed performance for TensorRT-LLM on H100 (Hopper). It should not be considered as the peak performance that can be delivered by TensorRT-LLM. The different performance numbers below were collected using a single GPU, a single node with multiple GPUs, or multiple nodes with multiple GPUs for GPT, GPT-like(LLaMA/OPT/GPT-J/SmoothQuant-GPT), BERT models, and Encoder-Decoder models for Peak Throughput and Low Latency.\n",
    " \n",
    " **H100 GPUs (FP8)**\n",
    " \n",
    " |Model\t|Batch Size|\tTP (1)|\tInput Length\t|Output Length\t|Throughput (out tok/s/GPU)|\n",
    " |-|-|-|-|-|-|\n",
    " |GPT-J 6B|\t1024|\t1|\t128|\t128|26,150|\n",
    " |GPT-J 6B|\t120\t|   1|128  |   2048|8,011|\n",
    " |GPT-J 6B|\t64\t|1\t |2048 |\t128|2,551|\n",
    " |GPT-J 6B|\t64\t|1\t |2048 |   2048|3,327|\n",
    " |Mistral 7B|896| 1  |128  | 128   |20,404|\n",
    " |Mistral 7B|120| 1  |128  |2048   |8,623 |\n",
    " |Mistral 7B|84 | 1  |2048 | 128   |2,405 |\n",
    " |Mistral 7B|56 | 1  |2048 |2048   |3,731 |\n",
    " |LLaMA 7B|\t768\t|1\t |128  |\t128|19,694|\n",
    " |LLaMA 7B|\t112\t|1\t |128  |   2048|6,818|\n",
    " |LLaMA 7B|\t80\t|1\t |2048 |\t128|2,244|\n",
    " |LLaMA 7B|\t48\t|1\t |2048 |   2048|2,740|\n",
    " |LLaMA 70B|1024|\t2|\t128|\t128|2,657|\n",
    " |LLaMA 70B|480\t|4\t |128  |2048   |1,486|\n",
    " |LLaMA 70B|96\t|2\t |2048 |128\t   |306|\n",
    " |LLaMA 70B|64  |\t2|2048 |2048   |547|\n",
    " |Falcon 180B|1024|\t4|\t128|128    |987|\n",
    " |Falcon 180B|1024|\t8|\t128|2048   |724|\n",
    " |Falcon 180B|\t64|\t4|2048 |128\t   |112|\n",
    " |Falcon 180B|\t64|\t4|2048 |2048   |264|\n",
    " \n",
    "\n",
    "Please click on the [Performance](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md) and [Benchmark](https://github.com/NVIDIA/TensorRT-LLM/blob/main/benchmarks/python/README.md) links to see a detailed table on `throughput` and `low Latency` for H100, L40S (Ada) and A100 (Ampere).\n",
    "\n",
    "\n",
    "\n",
    "### Performance comparison\n",
    "\n",
    "The following benchmark shows the performance improvements brought by TensorRT-LLM on the latest NVIDIA Hopper architecture. The figure reflects the performances achieved for article summarization, one of the many applications of LLMs, using an NVIDIA A100 and NVIDIA H100 with CNN/Daily Mail, a well-known dataset for evaluating summarization performance.\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/Performance-Llama2.png\" width=\"800\"/>\n",
    "</center></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7c52b",
   "metadata": {},
   "source": [
    "---\n",
    "## Acknowledgment\n",
    "\n",
    "This notebook is adapt from NVIDIA's [TensorRT-LLM Github repository](https://github.com/NVIDIA/TensorRT-LLM/tree/main)\n",
    "\n",
    "## References\n",
    "\n",
    "- https://nvidia.github.io/TensorRT-LLM/architecture.html\n",
    "- https://github.com/NVIDIA/TensorRT-LLM\n",
    "\n",
    "## Licensing\n",
    "Copyright © 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd251b4d",
   "metadata": {},
   "source": [
    " <div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"llama-chat-finetune.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "         <a>2</a>\n",
    "          <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"LangChain-with-Guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"trt-custom-model.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
