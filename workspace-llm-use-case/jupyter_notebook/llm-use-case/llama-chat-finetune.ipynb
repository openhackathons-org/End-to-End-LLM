{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8ace02",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "        <a>1</a>\n",
    "         <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "          <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"nemo-guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"trt-llama-chat.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e37cfe",
   "metadata": {},
   "source": [
    "# Finetuning Llama-2-7B with Custom Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2257ba15",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please note that you can only run this lab using the Llama Fine-tuning container</div>\n",
    "\n",
    "In this notebook, we demonstrate how to preprocess a dataset for a text generation task and fine-tune with the Llama-2-7b-chat model using 4-bit quantization via [QLoRA](https://arxiv.org/abs/2305.14314) allows efficient fine-tuning and Parameter-Efficient Fine-Tuning (PEFT) techniques that work by only updating a small subset of the model's parameters. The last section of the notebook describes the steps to run inference on the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60faf820",
   "metadata": {},
   "source": [
    "## Overview of LLAMA 2\n",
    "\n",
    "Llama 2 is a family of pre-trained and fine-tuned Large Language Models (LLMs) from [Meta](https://llama.meta.com/llama2) that consist of Llama-2 and Llama-2-chat. The pre-trained models range in scale from 7 billion to 70 billion parameters (7B, 13B, 70B) variants. The fine-tuned models (Llama 2-Chat) are optimized for conversational applications using Reinforcement Learning from Human Feedback ([RLHF](https://arxiv.org/abs/2305.18438)). Llama 2 is trained on 2 trillion tokens and has a context length of 4K; therefore, the model can grasp and generate extensive content. To improve inferencing scalability, Llama 2 adopts the [Grouped Query Attention (GQA)](https://arxiv.org/abs/2305.13245) that caches previous token pairs to accelerate attention computation.\n",
    "\n",
    "<center><img src=\"images/llama2-chat-arc2.png\" width=\"900px\" height=\"900px\" alt-text=\"Arc\"/></center>\n",
    "<center><i>Source: </i> <a href=\"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\">Llama 2: Open Foundation and Fine-Tuned Chat Models </a> paper</center>\n",
    "    \n",
    "We focus on using a custom dataset and fine-tuning the Llama 2 Chat 7 billion (Llama-2-7b-chat) variant. The variant was created using supervised fine-tuning and refined using the RLHF technique by rejection sampling and [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). \n",
    "Please run the cell below to check if the `Llama-2-7b-chat` model already exists in your directory; otherwise, uncomment the nested cell below to download it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49653722",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -LR ../../model/Llama-2-7b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb058d",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "../../model/Llama-2-7b-chat:\n",
    "LICENSE.txt\t\t   USE_POLICY.md\tparams.json\n",
    "README.md\t\t   checklist.chk\ttokenizer.model\n",
    "Responsible-Use-Guide.pdf  consolidated.00.pth\ttokenizer_checklist.chk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### download Llama-2-7b-chat model. Remove all comments to run the cell #####################\n",
    "\n",
    "#!python3 ../../source_code/Llama2/download-llama2-chat.py   \n",
    "#print(\"extracting files......\")\n",
    "#!tar -xf ../../model/Llama-2-7b-chat.tar  -C ../../model\n",
    "#print(\"files extraction done! removing tar file......\")\n",
    "#!rm -rf ../../model/Llama-2-7b-chat.tar\n",
    "#print(\"All done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ccacd",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Text Generation is the task of generating text that closely resembles human-written text. It is a process known as Causal Language Modelling, where an AI model generates a coherent and meaningful text that imitates natural human communication. The text generation task involves using algorithms and language models trained to learn patterns/long-term dependencies and contextual information to process input data and generate new output text based on given prompts. Popular large language models like  GPT (Generative Pre-trained Transformer), Mistral, and Llama 2 are used for the task. Use cases include:\n",
    "\n",
    "- Instruction Models: adapt to follow instructions\n",
    "- Code Generation: trained on code from scratch to help the programmers with repetitive coding tasks.\n",
    "- Stories Generation: receives input text and creates story-like text based on the given text.\n",
    "You can read more about text generation task variants such as Completion Generation Models, Text-to-Text Generation Models, and Text Generation from Image and Text from [here](https://huggingface.co/tasks/text-generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f2b79",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We consider [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) as our choice dataset for finetuning. The openassistant-guanaco dataset is a subset of the [Open Assistant Conversations](https://huggingface.co/datasets/OpenAssistant/oasst1/tree/main) dataset that contains only the highest-rated paths in the conversation tree, with a total of 9,85k training samples and 518 test samples. The OpenAssistant Conversations (OASST1) is a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers ([source](https://huggingface.co/datasets/OpenAssistant/oasst1))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea427768",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "\n",
    "Each row in the `openassistant-guanaco` jsonl dataset is a text dictionary that consists of `human` instructions and `Assistant` that provide context to draw the response to the instruction. Within the `text`, the `human` and `Assistant` fields are separated with three `###` delimiters that denote the start and end of positions.  \n",
    "\n",
    "```bash\n",
    "\n",
    "{ \"text\": \"### Human: Can you write a short introduction about the relevance of the term \\\"monopsony\\\" in economics? Please use examples related to...\n",
    "           ### Assistant: \\\"Monopsony\\\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term  is...   \n",
    "           ### Human: Now explain it to a dog\"\n",
    "}\n",
    "{ \"text\": \"### Human: \\u00bfCUales son las etapas del desarrollo y en qu\\u00e9 consisten seg\\u00fan Piaget?\n",
    "           ### Assistant: Jean Piaget fue un psic\\u00f3log suizo que propuso una teor\\u00eda sobre el desarrollo cognitivo...\n",
    "           ### Human: \\u00bfHay otras teor\\u00edas sobre las etapas del desarrollo que reafirmen o contradigan a la teor\\u00eda de Piaget?\"\n",
    "}\n",
    "{\"text\": \"### Human: Can you give me an example of a python script that opens an api point and serves a string?\n",
    "          ### Assistant: Sure! Here's an example Python script that uses the Flask web framework to create a simple API endpoint that serves a string:\\n\\n`         \\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef hello_world():\\n    return 'Hello, world!'\\n\\nif __name__ ==  \\n ...   \n",
    "          ### Human: What changes would you need to make to the code above to serve a JSON object instead of a string?\n",
    "          ### Assistant: To serve a JSON object instead of a string, you can modify the \\\"hello_world()\\\" function to return a JSON response using the  Flask \\\"jsonify\\\" function. Here's an example of how to modify the previous code to serve a JSON object:\\n\\n... \"\n",
    "}\n",
    "...\n",
    "\n",
    "```\n",
    "Please run the cell below to check if the dataset exists in the data directory; otherwise, uncomment the nested cell below to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20681885",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -LR ../../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da488820",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```python\n",
    "../../data:\n",
    "README.md  openassistant_best_replies_eval.jsonl   simple_data.json\n",
    "filtered   openassistant_best_replies_train.jsonl\n",
    "...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### download dataset. Remove comment to run the cell ###########\n",
    "#!python3 ../../source_code/Llama2/download-guanco-ds.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc880f9d",
   "metadata": {},
   "source": [
    "Import all required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58100242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some cases where you have access to limited computing resources, you might have to uncomment os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\" if you run into not enough memory issue \n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from langdetect import detect\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import re\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72803eb3",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9318879",
   "metadata": {},
   "source": [
    "Let's execute the `read_jsonl` function below to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c3921",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_jsonl(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9e70f",
   "metadata": {},
   "source": [
    "Let's perform the following steps in the cell below:\n",
    "- Set the path to the train and test jsonl files\n",
    "- Read both files using the `read_jsonl` function\n",
    "- Extract 5k samples from the training set\n",
    "- Display the samples to see the content and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12179ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set path to jsonl files\n",
    "train_path = '../../data/openassistant_best_replies_train.jsonl'\n",
    "test_path = '../../data/openassistant_best_replies_eval.jsonl'\n",
    "\n",
    "# read the files\n",
    "raw_train_data = read_jsonl(train_path)\n",
    "raw_test_data = read_jsonl(test_path)\n",
    "\n",
    "# extract 5000 samples \n",
    "train_samples = raw_train_data[:5000]\n",
    "\n",
    "print(\"length of traning samples: \", len(train_samples))\n",
    "train_samples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8631f",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "\n",
    "length of traning samples:  5000\n",
    "[{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ..'},\n",
    " ...\n",
    " {'text': '### Human: ¬øCUales son las etapas del desarrollo y en qu√© consisten seg√∫n Piaget?### Assistant: Jean Piaget fue un psic√≥logo suizo que propuso una teor√≠a sobre el desarrollo cognitivo humano que consta de cuatro etapas:\\n\\nEtapa sensoriomotora (0-2 a√±os): Durante esta etapa, el ni√±o aprende a trav√©s de sus sentidos y movimientos. Descubre que sus acciones pueden tener un impacto en el entorno y comienza a formarse una idea b√°sica de objetividad y continuidad.\\n\\nEtapa preoperatoria (2-7 a√±os): En esta etapa, el ni√±o comienza a desarrollar un pensamiento simb√≥lico y a comprender que las cosas pueden representar a otras cosas. Tambi√©n comienzan a desarrollar un pensamiento l√≥gico y a comprender conceptos como la causa y el efecto.\\n\\nEtapa de ...?'},\n",
    " ...\n",
    " {'text': '### Human: Schreibe einen kurze und pr√§zise Konstruktionsbeschreibung zu einem Dreieck ABC mit c=6\\xa0cm, h_c=5\\xa0cm und Œ≥=40¬∞. Œ≥ ist hierbei der von Seite c gegen√ºberliegende Winkel.### Assistant: Dreieck ABC ist ein rechtwinkliges Dreieck mit der Seitenl√§nge c=6 cm als Hypotenuse. Die H√∂he h_c von c betr√§gt 5 cm und der Winkel Œ≥ von c gegen√ºberliegend betr√§gt 40¬∞.### Human: Vielen Dank, das hat mir sehr weitergeholfen.'},\n",
    " {'text': '### Human: –ù–∞–ø–∏—à–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏–≥—Ä–µ Hytale### Assistant: Hytale - —ç—Ç–æ –ø—Ä–µ–¥—Å—Ç–æ—è—â–∞—è –∏–≥—Ä–∞-–ø–µ—Å–æ—á–Ω–∏—Ü–∞, –ø—Ä–∏–¥—É–º–∞–Ω–Ω–∞—è Hypixel Studios. –û–±—ä–µ–¥–∏–Ω—è—è –≤ —Å–µ–±–µ –¥–∏–∞–ø–∞–∑–æ–Ω –∏–≥—Ä—ã-–ø–µ—Å–æ—á–Ω–∏—Ü—ã —Å –≥–ª—É–±–∏–Ω–æ–π —Ä–æ–ª–µ–≤–æ–π –∏–≥—Ä—ã, Hytale –ø–æ–≥—Ä—É–∂–∞–µ—Ç –∏–≥—Ä–æ–∫–æ–≤ –≤ –∂–∏–≤–æ–π –∫–æ—Å–º–æ—Å, –ø–æ–ª–Ω—ã–π —ç—Å–∫–∞–ø–∞–¥ –∏ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–∞.\\n\\n–í Hytale –≥–µ–π–º–µ—Ä—ã –º–æ–≥—É—Ç –ø—É—Ç–µ—à–µ—Å—Ç–≤–æ–≤–∞—Ç—å –ø–æ –ø—Ä–æ—Å—Ç–æ—Ä–Ω–æ–º—É, –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –º–∏—Ä—É –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏–µ –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö. –û—Ç –≤–æ–∑–≤–µ–¥–µ–Ω–∏—è –±–∞–∑ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–π –∏ —Å—Ä–∞–∂–µ–Ω–∏–π —Å –º–æ–Ω—Å—Ç—Ä–∞–º–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–º–∏ –∏–≥—Ä–æ–∫–∞–º–∏ - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Hytale –±–µ–∑–≥—Ä–∞–Ω–∏—á–Ω—ã. –ò–≥—Ä–æ–∫–∏ —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–≤–æ–∏ ..'},\n",
    " {'text': '### Human: ÁßÅ„ÅØÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Å™„Çì„Åß„Åô„Åë„Å©\\nË≤¥Êñπ„ÇÇÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Åß„Åô„Åã?### Assistant: Áå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Å™„Çì„Åß„Åô„Å≠„ÄÇ\\nÁßÅ„ÇÇÁå´„Å°„ÇÉ„Çì„ÅåÂ•Ω„Åç„Åß„Åô„ÇàÔºÅ\\nÁå´„Å°„ÇÉ„Çì„ÅÆ„Å©„Çì„Å™„Å®„Åì„Çç„ÅåÂ•Ω„Åç„Åß„Åô„ÅãÔºü### Human: Áå´„Å°„ÇÉ„Çì„ÅÆÂÖ®„Å¶„ÅåÂ•Ω„Åç„Åß„ÅôÔºÅÁå´„Å°„ÇÉ„Çì„ÅØÂ§©‰Ωø„Åß„ÅôÔºÅÔºÅ### Assistant: „Çè„Åã„Çä„Åæ„Åô„Çà„ÄÅ„Åù„ÅÆÊ∞óÊåÅ„Å°ÔºÅ\\nÁå´„Å°„ÇÉ„Çì„ÅÆÊÑõ„Åè„Çã„Åó„ÅÑÂßø„ÅØ„ÄÅÂ§©‰Ωø„Åø„Åü„ÅÑ„Å´ÂèØÊÑõ„ÅÑ„Åß„Åô„Çà„Å≠ÔºÅ\\n„ÅÇ„Å™„Åü„ÅÆ„ÅäÂÆ∂„Å´„ÄÅÁå´„Å°„ÇÉ„Çì„ÅØ„ÅÑ„Çã„Çì„Åß„Åô„ÅãÔºü### Human: ÂãøË´ñ„Åß„ÅôÔºÅ„Å®„Å£„Å¶„ÇÇÂèØÊÑõ„Åè„Å¶„ÄÅÊØéÊó•„ÅåÂπ∏„Åõ„Åß„ÅôÔºÅ'},\n",
    "\n",
    " {'text': \"### Human: Quins sin√≤nims amb altres dialectes d'aquesta llengua t√© nen o nena?### Assistant: Al¬∑lot o al¬∑lota, vailet o vaileta, manyac o manyaga, nin o nina, xiquet o xiqueta, xic o xica, marrec, miny√≥ o minyona.\"},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6573baa",
   "metadata": {},
   "source": [
    "We can see from the content displayed that the file contains text written in different languages. For demonstration purposes, we want to consider only English text for training and prompting. Therefore, the train and test sets will be processed to filter out non-English text samples using a `detect` feature from the `spacy-langdetect` library. The English-only texts (`1778 training samples`) are saved as the new training set. The same process is applied to the test samples. Run the three two cells below to execute the filter process.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter non-Emglish word\n",
    "def remove_nonEnglish_rows(ds):\n",
    "    new_ds = []\n",
    "    for row in (ds):\n",
    "        if detect(str(row)) == 'en':\n",
    "            new_ds.append(row)\n",
    "    return new_ds\n",
    "        \n",
    "# save English text samples\n",
    "def save_jsonl(ds,filename):\n",
    "    with open(f\"../../data/filtered/{filename}.jsonl\", \"w\") as write_file:\n",
    "            json.dump(ds, write_file, indent=4)\n",
    "            print(\"dataset saved in jsonl format ....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600af06",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calling filter function\n",
    "filter_train_samples = remove_nonEnglish_rows(train_samples)\n",
    "filter_test_samples = remove_nonEnglish_rows(raw_test_data )\n",
    "\n",
    "print(\"len of training samples: \",len(filter_train_samples))\n",
    "filter_train_samples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfaed0",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "len of training samples:  1775\n",
    "[{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as ...'},\n",
    " {'text': '### Human: Can you explain contrastive learning in machine learning in simple terms for someone new to the field of ML?### Assistant: Sure! Let\\'s say you want to build a model which can distinguish between images of cats and dogs. You gather your dataset, consisting of many cat and dog pictures. Then you put them through a neural net of your choice, which produces some representation for each image, a sequence of numbers like [0.123, 0.045, 0.334, ...]. The problem is, if your model is unfamiliar with cat and dog images, these representations will be quite random. At one time a cat and ...'},\n",
    " {'text': \"### Human: I want to start doing astrophotography as a hobby, any suggestions what could i do?### Assistant: Getting started in astrophotography can seem daunting, but with some patience and practice, you can become a master of the craft. To begin, you'll need a good camera and lens, a tripod, and a dark sky location free of light pollution. You will also need to learn about the basics of astrophotography, such as what camera settings to use, how to capture star trails, and the best techniques for tracking celestial objects. You can also purchase or rent different types of telescopes, depending on what types of objects you want to capture. Additionally, it's important to keep up with the latest astrophotography news and trends. Once you have the necessary ...\"},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file names  \n",
    "save_train_filename = 'train'\n",
    "save_test_filename = 'test'\n",
    "\n",
    "# save file\n",
    "save_jsonl(filter_train_samples, save_train_filename)\n",
    "save_jsonl(filter_test_samples, save_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ee952",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Transformation\n",
    "\n",
    "In this section, we want to format our dataset to the Llama2 acceptable template for finetuning:\n",
    "\n",
    "```python\n",
    "\n",
    "<s>[INST] {human text} [/INST] {assistant/context} </s>\n",
    "<s>[INST]{human text} [/INST] </s>\n",
    "\n",
    "```\n",
    "- **Human text**: It denotes human instructions to the model. The human text is enclosed within an instruction tag `[INST] [/INST]` \n",
    "- **Assistant**: represents the context that will assist the model in drawing out a response to the instruction issued by a human. The assistant text is nested to the human text within a segment tag `<s>  </s>`.\n",
    "\n",
    "One or more segments can exist within a training sample text. A segment can consist of both human instruction text and assistant context or human instruction text only.\n",
    "\n",
    "\n",
    "<img src=\"images/template.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7cae2",
   "metadata": {},
   "source": [
    "Next, we load the filtered dataset by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a2bbb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('../../data/filtered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c95d07b",
   "metadata": {},
   "source": [
    "Execute the function to transform the filtered dataset to the format explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### credit: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k ### \n",
    "\n",
    "# Define a function to transform the data\n",
    "def transform_to_template(example):\n",
    "    conversation_text = example['text']\n",
    "    segments = conversation_text.split('###')\n",
    "\n",
    "    reformatted_segments = []\n",
    "\n",
    "    # Iterate over pairs of segments\n",
    "    for i in range(1, len(segments) - 1, 2):\n",
    "        human_text = segments[i].strip().replace('Human:', '').strip()\n",
    "\n",
    "        # Check if there is a corresponding assistant segment before processing\n",
    "        if i + 1 < len(segments):\n",
    "            assistant_text = segments[i+1].strip().replace('Assistant:', '').strip()\n",
    "\n",
    "            # Apply the new template\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] {assistant_text} </s>')\n",
    "        else:\n",
    "            # Handle the case where there is no corresponding assistant segment\n",
    "            reformatted_segments.append(f'<s>[INST] {human_text} [/INST] </s>')\n",
    "\n",
    "    return {'text': ''.join(reformatted_segments)}\n",
    "\n",
    "\n",
    "# Apply the transformation\n",
    "template_dataset = dataset.map(transform_to_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319db668",
   "metadata": {},
   "source": [
    "Let's display a sample to inspect if our training set is in the right format as shown in the screenshot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset['train'][2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d60b3c",
   "metadata": {},
   "source": [
    "Save the preprocessed dataset to the directory `../data/ds_preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset.save_to_disk('../../data/ds_preprocess')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c9948",
   "metadata": {},
   "source": [
    "## Fine-tuning LLAMA 2\n",
    "\n",
    "As mentioned earlier in the notebook, our choice model for finetuning is the `Llama-2-7b-chat`. Below is the list of walkthrough steps to complete the task.\n",
    "\n",
    "- Convert the `Llama-2-7b-chat` to Hugging Face transformer format\n",
    "- Set the paths to the model and load the dataset\n",
    "- Load the model tokenizer\n",
    "- Set the training parameter\n",
    "- Configure the Parameter Efficient Fine Tuning with LoRA\n",
    "- Apply 4-bits quantization\n",
    "- Setup the trainer to start the finetuning process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810864f9",
   "metadata": {},
   "source": [
    "### Convert model to Hugging Face Transformers Format\n",
    "We will convert our model checkpoint to the Hugging Face transformer format. The benefits are as follows:  \n",
    "\n",
    "- Delivers a convenient way to finetune the Llama2 model with limited GPU computing resources (like laptops, workstations, or Google Colab) through the use of some technique that a transformer-compatible \n",
    "- Ability to immediately use a model on a given input text using transformer Pipelines. \n",
    "- The use of transformers `Pipelines` group together the pre-trained model with the preprocessing that was used during that training to enable quick inferencing.\n",
    "\n",
    "\n",
    "To convert our model checkpoint, we use the `convert_llama_weights_to_hf.py` script located on [GitHub](https://github.com/cedrickchee/transformers-llama/tree/llama_push/src/transformers/models/llama). \n",
    "\n",
    "- `--input_dir`: denotes the directory to the llama model to be converted\n",
    "- `--model_size`: represents the llama model parameter size\n",
    "- `--out_dir`: the directory to save the converted model\n",
    "\n",
    "<img src=\"images/llama-hf.png\" height=\"550px\" width=\"900px\" />\n",
    "\n",
    "If you already have the Hugging Face format of the model `Llama-2-7b-chat-hf`, you can skip running the cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65618d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/Llama2/llama/convert_llama_weights_to_hf.py \\\n",
    "--input_dir ../../model/Llama-2-7b-chat --model_size 7B --output_dir ../../model/Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6322673",
   "metadata": {},
   "source": [
    "**Exepected Output:**\n",
    "```python\n",
    "...\n",
    "https://github.com/huggingface/transformers/pull/24565\n",
    "Fetching all parameters from the checkpoint at ../model/Llama-2-7b-chat.\n",
    "Loading the checkpoint in a Llama model.\n",
    "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:06<00:00,  4.95it/s]\n",
    "Saving in the Transformers format.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6f5c",
   "metadata": {},
   "source": [
    "Now, we can initialize the path to our transformer format model and load the transformed/preprocessed training dataset from the directory where it was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initailize path to the base model \n",
    "base_model = \"../../model/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# set the path to the dataset template\n",
    "data_path = \"../../data/ds_preprocess/train\"\n",
    "\n",
    "# set the path to the dataset template\n",
    "eval_path = \"../../data/ds_preprocess/test\"\n",
    "\n",
    "# load the transformed dataset\n",
    "dataset = load_from_disk(data_path)\n",
    "eval_dataset = load_from_disk(eval_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b042cb",
   "metadata": {},
   "source": [
    "### Loading tokenizer\n",
    "\n",
    "Tokenization is breaking a text into sentences, words, or sub-words. Each word or sentence in a text is considered as a token. Tokenization allows a detailed text data analysis when broken into smaller units. The [LLaMA tokenizer](https://huggingface.co/docs/transformers/en/model_doc/llama2) is a byte-pair-encoding (BPE) model based on [sentencepiece](https://aclanthology.org/P16-1162/), an unsupervised text tokenizer and detokenizer for Neural Network-based text generation systems that predetermined the vocabulary size prior to the neural model training.\n",
    "\n",
    "In the cell below, we load the tokenizer from our base model directory and set the parameters:\n",
    "\n",
    "- **pad_token**: a special token used to make arrays of tokens the same size for batching purposes. \n",
    "- **padding_side**: side to pad\n",
    "\n",
    "A comprehensive list of Llama tokenizer parameters can be found [here](https://huggingface.co/docs/transformers/en/model_doc/llama2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e098f01",
   "metadata": {},
   "source": [
    "### Set Training Hyperparameters\n",
    "\n",
    "Training hyperparameters are customized using the `TrainingArguments` class. The class provides an API that offers a wide range of options to customize and optimize the training process. Please find a comprehensive description of the hyperparameters [here](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments). You can further modify the values of the hyperparameters in the next cell after the complete finetune process and rerun the cells to see how it impact on the training outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b149b",
   "metadata": {},
   "source": [
    "**Note:** *If running on a single DGX A100 GPU, use the hyperparameter settings below to modify the next cell.*\n",
    "\n",
    "```python \n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"../../model/results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    group_by_length=True,\n",
    "    save_steps=50,\n",
    "    logging_steps=50, \n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "The cell below contains a hyperparameter setting to run on a GPU MiG instance (20GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"../../model/results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    group_by_length=True,\n",
    "    save_steps=25,\n",
    "    logging_steps=25, \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e18bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure PEFT With LoRA\n",
    "\n",
    "Finetuning large language pre-trained models is computationally costly. Our main goal is to accelerate our finetuning process with minimal memory consumption. A method to achieve that is to use a state-of-the-art [Parameter-Efficient Finetuning (PEFT)](https://github.com/huggingface/peft/tree/main) approach. [PEFT](https://arxiv.org/abs/2305.16742) allows finetuning a small number of (extra) model parameters instead of all the model's parameters, and this significantly decreases the computational and storage costs. One of the ways to implement PEFT is to adopt the Low-Rank Adaptation (LoRA) technique. Lora makes finetuning more efficient by greatly reducing the number of trainable parameters for downstream tasks. It does this by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. According to the [authors of LoRA](https://arxiv.org/abs/2106.09685), Aside from reducing the number of trainable parameters by 10k times, it also reduces the GPU consumption by 3x, thus delivering high throughput with no inference latency. For quick background on LoRA, please follow this [link](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora).\n",
    "\n",
    "<center><img src=\"images/lora-arch.png\" height=\"500px\" width=\"900px\"  /></center>\n",
    "<center>  LoRA reparametrization and Weight merging. <a href=\"https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\"> View source</a> </center>\n",
    "\n",
    "LoRA techniques are applied through `LoraConfig`, which provides PEFT parameters that control how the method is applied to the base model. A description of the parameter used in the cell below is given as follows:\n",
    "\n",
    "- **lora_alpha**: LoRA scaling factor\n",
    "- **lora_dropout**: The dropout probability for LoRA layers.\n",
    "- **r**: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "- **bias**: Specifies if the bias parameters should be trained. It can be 'none', 'all', or 'lora_only'.\n",
    "- **task_type**: Possible task types which include `CAUSAL_LM`, `FEATURE_EXTRACTION`, `QUESTION_ANS`, `SEQ_2_SEQ_LM`, and `SEQ_CLS and TOKEN_CLS`.   \n",
    "\n",
    "Because the task we want to perform is text generation, we have set the task_type to Causal language model `(CAUSAL_LM)`, which is frequently used for text generation tasks. Please run the cell below to set up the LoRA configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a836da",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446569eb",
   "metadata": {},
   "source": [
    "### 4-bit quantization configuration\n",
    "\n",
    "Model quantization is a popular deep-learning optimization method in which model data‚Äînetwork parameters and activations‚Äîare converted from floating-point to lower-precision representation, typically using 8-bit integers. Quantization represents data with fewer bits, making it a useful technique for reducing memory usage and accelerating inference, especially in large language models (LLMs). It can be combined with PEFT methods to make it easier to train and load LLMs for inference.\n",
    "\n",
    "<center><img src=\"images/quantization.png\" height=\"400px\" width=\"700px\" /></center>\n",
    "<center> <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\" > source: Using Quantization Aware Training with NVIDIA TensorRT</a></center>\n",
    "\n",
    "Several ways and algorithms to quantize a model including can be found [here](https://huggingface.co/docs/peft/main/en/developer_guides/quantization). A library to easily implement quantization and integrate with transformers is the `bitsandbytes` library. The library provides config parameters to quantize a model to 8 or 4 bits using the `BitsAndBytesConfig` class. The 4 bits parameters used in the cell below are described as follows:\n",
    "\n",
    "- **load_in_4bit**: set `True` to quantize the model to 4-bits when you load it\n",
    "- **bnb_4bit_quant_type**: set to `\"nf4\"` to use a special 4-bit data type for weights initialized from a normal distribution\n",
    "- **bnb_4bit_use_double_quant**: set `True` to use a nested quantization scheme to quantize the already quantized weights\n",
    "- **bnb_4bit_compute_dtype**: set to `torch.float16` or `torch.bfloat16` to use bfloat16 for faster computation \n",
    "\n",
    "Run the cell below to set the 4-bit quantization for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2095193",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf85f21",
   "metadata": {},
   "source": [
    "### Loading Base Model\n",
    "\n",
    "The next step is to load our base model `(Llama-2-7b-chat-hf)` with the causal language model class used for the text generation task. We do this by passing the base model, quantization config, and GPU device ID to the `AutoModelForCausalLM` object, as shown in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec182e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb3660",
   "metadata": {},
   "source": [
    "### Set the Trainer Hyperparameters\n",
    "\n",
    "To initiate our model trainer, we create a trainer object from [Supervised fine-tuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer). SFT is part of the integrated transformer [Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/en/index) tools used to train transformer language models using Reinforcement Learning. Others include [Reward Modeling step (RM)](https://huggingface.co/docs/trl/en/reward_trainer) and  Proximal [Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). In our SFT trainer object, we set our model, training dataset, PEFT config  object, model tokenizer, and training argument parameter. We also specify the field (`text`) to use within our dataset.\n",
    "\n",
    "**Note:** *If running on a single DGX A100 GPU, modify the value of `max_seq_length` to 1024 or set it to none (as default).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0797225",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_params,\n",
    "    args=training_params,\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bfda0",
   "metadata": {},
   "source": [
    "Run the cell below to train the SFT trainer object. *Please note that it takes an hour or more to complete the training (each epoch takes 31 minutes or more)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b27e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee8afe",
   "metadata": {},
   "source": [
    "Save the finetuning model and tokenizer in the directory  `../model/Llama-2-7b-chat-hf-finetune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "new_model = \"../../model/Llama-2-7b-chat-hf-finetune\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6188feb",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "Now that we completed our finetuning process, we can run a quick inference to test how our new model is performing. To do that, we will create the following:\n",
    "\n",
    "- a transformer pipeline with four parameter inputs: `model`, `tokenizer`, `max_length`, and `task`.  \n",
    "- format prompt as: `f\"<s>[INST] {prompt} [/INST]\"`\n",
    "- pass the prompt into the pipeline object and get result.\n",
    "\n",
    "```python\n",
    " \n",
    "inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")\n",
    "prompt = inf_pipeline(f\"<s>[INST] {prompt} [/INST]\")\n",
    "result = inf_pipeline(prompt)\n",
    "print(result[0]['generated_text'])\n",
    "```\n",
    "You can modify the `max_length` to decide the length of text generated by the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference(prompt):\n",
    "    inf_pipeline = pipeline(model=model, tokenizer=tokenizer, max_length=200, task=\"text-generation\")\n",
    "    prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "    result = inf_pipeline(prompt)\n",
    "    print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"explain what is astrophotography?\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa31490",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "<s>[INST] explain what is astrophotography? [/INST] Astrophotography is the branch of photography that deals with the photographing of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. Astrophotographers use specialized cameras and telescopes to capture images of these objects, often in low light conditions. Astrophotography requires a great deal of patience, skill, and knowledge, as photographers must be able to accurately track the movement of celestial objects and compensate for the effects of light pollution and atmospheric distortion. Astrophotography has become increasingly popular in recent years, with many amateur astronomers and professional photographers engaging in this hobby and art form. \n",
    "\n",
    "Astrophotography can be done using a variety of techniques, including long exposure times, tracking mounts, and using narrow-band filters to\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"can you explain further?\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fbb65",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "<s>[INST] can you explain further? [/INST] Sure, I'd be happy to explain further! Could you please provide more context or clarify what you're asking about? üòÉ üëç üí° üñä üìù üìÖ üï∞ üï∞Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∞Ô∏è üï∞Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï≥Ô∏è üï∑Ô∏è üï∏Ô∏è üï∞Ô∏è\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a7af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I want you to explain further on astrophotography\"\n",
    "run_inference(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f05d16",
   "metadata": {},
   "source": [
    "**Likely output:**\n",
    "\n",
    "```bash\n",
    "\n",
    "<s>[INST] I want you to explain futher on astrophotography [/INST] Sure, I d be happy to explain further on astrophotography. Astrophotography is the process of capturing images of celestial objects such as stars, planets, galaxies, and other astronomical phenomena. It involves using specialized cameras and techniques to capture high-quality images of these objects in the night sky.\n",
    "\n",
    "There are several key components to astrophotography:\n",
    "\n",
    "    Camera: Astrophotography cameras are designed specifically for capturing images of celestial objects. They typically have large sensors, fast lenses, and specialized features such as built-in equatorial mounts, motorized tracking systems, and cooling systems to reduce noise and improve image quality.\n",
    "\n",
    "    Telescope: A telescope is used to gather light from the celestial object being photographed. There are several types of\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf34cb9",
   "metadata": {},
   "source": [
    "### Reload model in FP16 and merge with LoRA weights\n",
    "\n",
    "To have our model as a single entity for ease of use and widened task coverage, we reload it in fp16 mode and merge it with the LoRA weights using `model.merge_and_unload()`. The tokenizer is reloaded, pad, and saved along with the merged model in the same directory, `../model/Llama-2-7b-chat-hf-merged`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58537fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "load_base_model = AutoModelForCausalLM.from_pretrained( base_model, torch_dtype=torch.float16, low_cpu_mem_usage=True, return_dict=True, device_map={\"\": 0})\n",
    "\n",
    "model = PeftModel.from_pretrained(load_base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaa035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../../model/Llama-2-7b-chat-hf-merged\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"../../model/Llama-2-7b-chat-hf-merged\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f95d0a",
   "metadata": {},
   "source": [
    "**expected output**:\n",
    "```python\n",
    "('../model/Llama-2-7b-chat-hf-merged/tokenizer_config.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/special_tokens_map.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/tokenizer.model',\n",
    " '../model/Llama-2-7b-chat-hf-merged/added_tokens.json',\n",
    " '../model/Llama-2-7b-chat-hf-merged/tokenizer.json')\n",
    "```\n",
    "\n",
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please close the Jupyter notebook and switch to the TRT-LLM Container to continue with the next lab</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202619b0",
   "metadata": {},
   "source": [
    "---\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eafbf",
   "metadata": {},
   "source": [
    "- https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\n",
    "- https://llama.meta.com/llama2\n",
    "- https://huggingface.co/tasks/text-generation\n",
    "- https://arxiv.org/abs/2304.07327\n",
    "- https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/llama2\n",
    "- https://huggingface.co/docs/peft/main/en/developer_guides/quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056ef38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Licensing\n",
    "\n",
    "Copyright ¬© 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e1b99",
   "metadata": {},
   "source": [
    " <div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "         <a>1</a>\n",
    "          <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "          <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a href=\"triton-llama.ipynb\">4</a>\n",
    "        <a href=\"nemo-guardrails.ipynb\">5</a>\n",
    "        <a href=\"challenge.ipynb\">6</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"trt-llama-chat.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
