{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91c3f1",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-custom-model.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "        <a href=\"challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea7c12",
   "metadata": {},
   "source": [
    "# Deploying A Finetuned Model Using Triton Inference Server (TensorRT-LLM Backend)\n",
    "--- \n",
    "<div style=\"text-align:left; color:#FF0000; height:80px; text-color:red; font-size:20px\">Please note that you can run this lab only by using the TRT-LLM Container </div>\n",
    "\n",
    "In this notebook, our focus would be using TensorRT-LLM Backend to deploy the tensorrt engine built in the previous notebook. TensorRT-LLM Backend aims to let you serve [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) models with Triton Inference Server. You can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/backend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28d7b7",
   "metadata": {},
   "source": [
    "\n",
    "## Using the TensorRT-LLM Backend\n",
    "\n",
    "To use the TensorRT-LLM Backend, follow the steps below:\n",
    "\n",
    "- Clone the TensorRT-LLM Backend repo ([tensorrtllm_backend](https://github.com/triton-inference-server/tensorrtllm_backend)). This has already been done within the container running the lab.\n",
    "   <img src=\"images/trt-clone.png\" />\n",
    "- Navigate into cloned repository and create the model repository `triton_model_repo`\n",
    "   <img src=\"images/trt-model-repo.png\" />\n",
    "- Copy all files in `all_models/inflight_batcher_llm` directory into the `triton_model_repo`\n",
    "   <img src=\"images/trt-copy-folder5.png\" />\n",
    "\n",
    "- There are five model directories within the `all_models/inflight_batcher_llm`. The directories include:\n",
    "    - **preprocessing**: This model is used for tokenizing, meaning the conversion from prompts(string) to input_ids(list of ints).\n",
    "    - **tensorrt_llm**: This model is a wrapper of your TensorRT-LLM model and is used for inferencing\n",
    "    - **postprocessing**: This model is used for de-tokenizing, meaning the conversion from output_ids(list of ints) to outputs(string).\n",
    "    - **ensemble**: This model is used to chain the three models above together as: `preprocessing -> tensorrt_llm -> postprocessing`\n",
    "    - **tensorrt_llm_bls**: This model can also be used to chain the preprocessing, tensorrt_llm and postprocessing models together.\n",
    "\n",
    "\n",
    "\n",
    "Learn more about [ensemble model](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models) and [tensorrt_llm_bls model](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#tensorrt_llm_bls).\n",
    "\n",
    "- Copy the model tensorrt engine from `../../model/trt_engines/fp16/1-gpu` into the `triton_model_repo/tensorrt_llm/1` directory\n",
    "   <img src=\"images/trt-copy-engine.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848762fb",
   "metadata": {},
   "source": [
    "Now, let's execute the above steps. But before that, we have to copy the `tensorrtllm_backend` repo from within our container to the `source_code` directory for ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensorrtllm_backend to source_code if not already present\n",
    "!cp -r /workspace/tensorrtllm_backend ../../source_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4aa14",
   "metadata": {},
   "source": [
    "Next, we navigate into the `tensorrtllm_backend` and execute all the steps mentioned above for `FP16 tensorrt engine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78124d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd ../../source_code/tensorrtllm_backend/\n",
    "mkdir triton_model_repo\n",
    "\n",
    "# Copy the example models to the model repository\n",
    "cp -r all_models/inflight_batcher_llm/* triton_model_repo/\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/1/\n",
    "cp  ../../model/trt_engines/fp16/1-gpu/* triton_model_repo/tensorrt_llm/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9506be8",
   "metadata": {},
   "source": [
    "### Modify the model configuration\n",
    "\n",
    "The following table shows the fields that need to be modified before deployment:\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/ensemble.png\" width=\"1000\"/>\n",
    "</center></div>\n",
    "\n",
    "\n",
    "**Kindly change the key-value pair using the correct configuration by running the script cell bellow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Run this script to set the right key-value pairs automatically.\n",
    "\n",
    "cd ../../source_code/tensorrtllm_backend/\n",
    "\n",
    "export HF_LLAMA_MODEL=\"../../model/Llama-2-7b-chat-hf-merged\"\n",
    "export    ENGINE_PATH=\"../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/1\" \n",
    "export BACKEND=\"tensorrtllm\"\n",
    "\n",
    "\n",
    "python3 tools/fill_template.py -i triton_model_repo/preprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:64,preprocessing_instance_count:1\n",
    "python3 tools/fill_template.py -i triton_model_repo/postprocessing/config.pbtxt tokenizer_dir:${HF_LLAMA_MODEL},triton_max_batch_size:64,postprocessing_instance_count:1\n",
    "python3 tools/fill_template.py -i triton_model_repo/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:64,decoupled_mode:False,bls_instance_count:1,accumulate_tokens:False\n",
    "python3 tools/fill_template.py -i triton_model_repo/ensemble/config.pbtxt triton_max_batch_size:64\n",
    "python3 tools/fill_template.py -i triton_model_repo/tensorrt_llm/config.pbtxt triton_backend:${BACKEND},triton_max_batch_size:64,decoupled_mode:False,max_beam_width:1,engine_dir:${ENGINE_PATH},max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False,batching_strategy:V1,max_queue_delay_microseconds:0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862e1d4",
   "metadata": {},
   "source": [
    "You can look at the `config.pbtxt` files for your reference and also learn more about the [model configuration parameters](https://github.com/triton-inference-server/tensorrtllm_backend/tree/main?tab=readme-ov-file#modify-the-model-configuration).\n",
    "\n",
    "\n",
    "\n",
    "- a) View changes in the **Pre-processing config file** *[triton_model_repo/preprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/preprocessing/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   |Parameters | Value | \n",
    "|-|-|-| \n",
    "|   124   | `tokenizer_dir` | **`/workspace/app/model/Llama-2-7b-chat-hf-merged`**|\n",
    "|   29    | `triton_max_batch_size` |64|\n",
    "|   137   | `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "\n",
    "- b) View changes in the **Post-processing config file**  *[triton_model_repo/postprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/postprocessing/config.pbtxt)*\n",
    "\n",
    "\n",
    "\n",
    "|  Line   | Parameters | Value | \n",
    "|-|-|-| \n",
    "|   97    | `tokenizer_dir` | **`/workspace/app/model/Llama-2-7b-chat-hf-merged`**|\n",
    "|   29    | `triton_max_batch_size` |64|\n",
    "|   110   | `preprocessing_instance_count` | 1|\n",
    "\n",
    "---\n",
    "\n",
    "- c) View changes in the **tensorrt_llm_bls config file**  *[triton_model_repo/tensorrt_llm_bls/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm_bls/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   | Parameters | Value | \n",
    "|-|-|-| \n",
    "|   29    | `triton_max_batch_size` | 64|\n",
    "|   32    | `decoupled_mode` |False|\n",
    "|   244   | `bls_instance_count` | 1|\n",
    "|   226   | `accumulate_tokens` |False|\n",
    "\n",
    "\n",
    "d) View changes in the **Ensemble config file**  *[triton_model_repo/ensemble/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/ensemble/config.pbtxt)*\n",
    "\n",
    " \n",
    "|  Line   | Parameters | Value|\n",
    "|-|-|-|\n",
    "|    29   | `triton_max_batch_size` |64 |\n",
    "\n",
    "---\n",
    "\n",
    "- e)  View changes in the **tensorrt_llm config file**  *[triton_model_repo/tensorrt_llm/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "\n",
    "|  Line   | Name | Value|\n",
    "|-|-|-|\n",
    "|   28    |  `triton_backend`        |    \"tensorrtllm\"                |\n",
    "|   29    |`triton_max_batch_size` | 64 |\n",
    "|   32    |`decoupled_mode` | False|\n",
    "|   350   |`max_beam_width` | 1 |\n",
    "|   368   |`engine_dir` |  **`/workspace/app/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/1`** |\n",
    "|   374   |`max_tokens_in_paged_kv_cache` | 2560|\n",
    "|   380   |max_attention_window_size|2560|\n",
    "|   398   |kv_cache_free_gpu_mem_fraction |0.5 |\n",
    "|   423   |exclude_input_in_output |True |\n",
    "|   453   |enable_kv_cache_reuse | False|\n",
    "|   362   |batching_strategy |V1 |\n",
    "|    37   |max_queue_delay_microseconds | 0|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5cdb9",
   "metadata": {},
   "source": [
    "### Launch Triton server\n",
    "\n",
    "We can launch the Triton server with the following command:\n",
    "\n",
    "- Press `Crtl+Shift+L` and open a new terminal\n",
    "  <center><img src=\"images/terminal.png\"  alt-text=\"terminal\"/></center>\n",
    "- On the terminal, navigate to the launch script folder by running this command: `cd ../../source_code/tensorrtllm_backend`\n",
    "- Start the Triton Server with this command: `python3 scripts/launch_triton_server.py  --world_size=1  --model_repo=triton_model_repo`\n",
    "\n",
    "\n",
    "It will take a few minutes to run and when successfully deployed, the server produces logs similar to the screenshot below.\n",
    "\n",
    "<center><img src=\"images/triton-server.png\"  alt-text=\"server\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b884cb8",
   "metadata": {},
   "source": [
    "## Query the server with the Triton-generated endpoint\n",
    "\n",
    "You can query the server using Triton's\n",
    "[generated endpoint](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_generate.md)\n",
    "with a curl command based on the following general format within your client\n",
    "environment/container:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/${MODEL_NAME}/generate -d '{\"{PARAM1_KEY}\": \"{PARAM1_VALUE}\", ... }'\n",
    "```\n",
    "\n",
    "In the case of the models used in this example, you can replace MODEL_NAME with `ensemble`. Examining the\n",
    "ensemble model's config.pbtxt file, you can see that 4 parameters are required to generate a response\n",
    "for this model:\n",
    "\n",
    "- \"text_input\": Input text to generate a response from\n",
    "- \"max_tokens\": The number of requested output tokens\n",
    "- \"bad_words\": A list of bad words (can be empty)\n",
    "- \"stop_words\": A list of stop words (can be empty)\n",
    "\n",
    "Therefore, we can query the server in the following way:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"explain what is astrophotography?\", \"max_tokens\": 20, \"bad_words\": \"\", \"stop_words\": \"\"}'\n",
    "```\n",
    "*Note: The value of HTTP_PORT is already set within the docker container to 8000.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534fb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"explain what is astrophotography?\", \"max_tokens\": 200, \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa20b5",
   "metadata": {},
   "source": [
    "Which should return a result similar to (formatted for readability):\n",
    "```json\n",
    "{\"context_logits\":0.0,\"cum_log_probs\":0.0,\"generation_logits\":0.0,\"model_name\":\"ensemble\",\"model_version\":\"1\",\"output_log_probs\":[\n",
    "...\n",
    "  \"sequence_end\":false,\"sequence_id\":0,\"sequence_start\":false,\"text_output\":\"[/INST] Astrophotography is a branch of photography that deals with the capture of images of celestial objects such as stars, planets, and galaxies. Astrophotography involves the use of specialized equipment such as telescopes, cameras, and software to capture high-quality images of these objects. Astrophotography can be used to study the structure and behavior of celestial objects, as well as to create beautiful and awe-inspiring images that can be appreciated by people of all ages and backgrounds. \\n\\nAstrophotography is a challenging and rewarding hobby that requires a combination of technical knowledge, creativity, and patience. It is a great way to learn about the universe and to appreciate its beauty and complexity. \\n\\nIf you are interested in astrophotography, there are many resources available to help you get started. You can find books, websites, and online communities\"}\n",
    "```\n",
    "\n",
    "You can ask further details regarding your previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \" I want you to explain further on astrophotography \", \"max_tokens\":200 , \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b9e85",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "\n",
    "\n",
    "```json\n",
    "...\n",
    "0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"sequence_end\":false,\"sequence_id\":0,\"sequence_start\":false,\"text_output\":\"and how to take a picture of the milky way.  I want you to explain how to use a tripod, how to use a camera, how to use a lens, and how to use a remote shutter release. I want you to explain how to use a tripod, how to use a camera, how to use a lens, and how to use a remote shutter release.  I want you to explain how to use aperture, shutter speed, and ISO.  I want you to explain how to use a polarizing filter.  I want you to explain how to use a star tracker.  I want you to explain how to use a camera with a mirror lock-up.  I want you to explain how to use a camera with a mirror lock-up and a remote shutter release.  I want you to explain how to use a camera with a mirror lock-up, a remote shutter release, and a self-timer.  I want you to explain how to use a camera with a mirror lock-up, a remote shutter release, a self-timer, and a flash\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb600a5",
   "metadata": {},
   "source": [
    "### Querying and Formatting using Python\n",
    "\n",
    "We notice the format is not quite useful, let us now try to do the same via Python, here is a snippet in Python that does the same as above, let us run it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289bcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = os.getenv('HTTP_PORT')\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb37831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"explain what is astrophotography?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 200,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0182b",
   "metadata": {},
   "source": [
    "Likely output is a follows: \n",
    "\n",
    "```pythoon\n",
    "Input: explain what is astrophotography?\n",
    "Output: <s>explain what is astrophotography? [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies.\n",
    "\n",
    "Astrophotography is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras, and mounts. The images are often captured in low light conditions and require long exposure times to capture the details of the celestial objects.\n",
    "\n",
    "Astrophotography is used to capture images of celestial objects for scientific research, educational purposes, and for the general public to appreciate the beauty of the night sky.  [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies. It is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras\n",
    "\n",
    "```\n",
    "\n",
    "We see that a lot of lines are repeated, let us now truncate this using a Python function and give it another try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbee115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_repetitive_text(text, n_words=10):\n",
    "    words = text.split()\n",
    "    unique_phrases = set()\n",
    "    output_words = []\n",
    "\n",
    "    for i in range(len(words) - n_words + 1):\n",
    "        phrase = ' '.join(words[i:i + n_words])\n",
    "        if phrase in unique_phrases:\n",
    "            # Once a repetition is found, return the text up to that point\n",
    "            return ' '.join(output_words)\n",
    "        unique_phrases.add(phrase)\n",
    "        output_words.append(words[i])\n",
    "\n",
    "    # If no repetition is found, return the entire text\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b186e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"explain what is astrophotography?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 200,  # Increased number of tokens\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Truncate repetitive text\n",
    "    output_text = truncate_repetitive_text(output_text)\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a5955",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "\n",
    "```text\n",
    "Input: explain what is astrophotography?\n",
    "Output: <s>explain what is astrophotography? [/INST] Astrophotography is a type of photography that focuses on capturing images of celestial objects such as stars, planets, and galaxies. Astrophotography is a challenging form of photography as it requires the use of specialized equipment such as telescopes, cameras, and mounts. The images are often captured in low light conditions and require long exposure times to capture the details of the celestial objects. Astrophotography is used to capture images of celestial objects for scientific research, educational purposes, and for the general public to appreciate the beauty of the night sky.\n",
    "```\n",
    "\n",
    "We see that the output is much better, using some more simple functions, we can completely build a post-processing wrapper that cleans the results that we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4078aa3",
   "metadata": {},
   "source": [
    "## Benchmark Test \n",
    "\n",
    "- **End-to-End test**: The testing script sends requests to the deployed `ensemble` model. The Ensemble model is ensembled by three models: preprocessing, tensorrt_llm and postprocessing. The test checks the total latency of the three parts of an ensemble model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a9594",
   "metadata": {},
   "source": [
    "#### End-to-End Test Using FP16 TRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/tensorrtllm_backend/tools/inflight_batcher_llm/end_to_end_test.py  \\\n",
    "         --dataset  ../../data/simple_data.json  \\\n",
    "         --max-input-len 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a8fd4",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "```python\n",
    "...\n",
    "context_logits.shape: (1, 1, 1)\n",
    "generation_logits.shape: (1, 1, 1, 1)\n",
    "[INFO] Start testing on 13 prompts.\n",
    "[INFO] Functionality test succeed.\n",
    "[INFO] Warm up for benchmarking.\n",
    "[INFO] Start benchmarking on 13 prompts.\n",
    "[INFO] Total Latency: 4769.683 ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5194112",
   "metadata": {},
   "source": [
    "### Shutdown Triton Server\n",
    "\n",
    "Run the below cell to Shutdown the Triton server, otherwise you will get an error while running the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps aux | grep '[t]ritonserver' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731abc8",
   "metadata": {},
   "source": [
    "---\n",
    "### Benchmark Test Using INT8 TRT Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd ../../source_code/tensorrtllm_backend/\n",
    "\n",
    "# make TRT engine folder for triton \n",
    "mkdir -p triton_model_repo/tensorrt_llm/2\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/2\n",
    "cp  ../../model/trt_engines/weight_only/1-gpu/* triton_model_repo/tensorrt_llm/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a719c",
   "metadata": {},
   "source": [
    "- Kindly open the **tensorrt llm config file** and make the changes listed in the table below *[triton_model_repo/tensorrt_llm/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Value|\n",
    "| :----------------------: | :----------------------: | :-----------------------------: |\n",
    "| 368|`engine_dir` | **`/workspace/app/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/2`** |\n",
    "\n",
    "- Launch Triton server\n",
    "    - Press `Crtl+Shift+L` and open a new terminal\n",
    "    - On the terminal, navigate to the launch script folder by running this command: `cd ../../source_code/tensorrtllm_backend`\n",
    "    - Start the Triton Server with this command: `python3 scripts/launch_triton_server.py  --world_size=1  --model_repo=triton_model_repo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"explain what is astrophotography?\", \"max_tokens\": 200, \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3347b",
   "metadata": {},
   "source": [
    "#### End-to-End Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bdf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/tensorrtllm_backend/tools/inflight_batcher_llm/end_to_end_test.py  \\\n",
    "         --dataset  ../../data/simple_data.json \\\n",
    "         --max-input-len 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5230d2",
   "metadata": {},
   "source": [
    "Likely output:\n",
    "\n",
    "```python\n",
    "...\n",
    "context_logits.shape: (1, 1, 1)\n",
    "generation_logits.shape: (1, 1, 1, 1)\n",
    "[INFO] Start testing on 13 prompts.\n",
    "[INFO] Functionality test succeed.\n",
    "[INFO] Warm up for benchmarking.\n",
    "[INFO] Start benchmarking on 13 prompts.\n",
    "[INFO] Total Latency: 3556.254 ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f6f27e",
   "metadata": {},
   "source": [
    "### Shutdown Triton Server\n",
    "\n",
    "Run the below cell to Shutdown the Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps aux | grep '[t]ritonserver' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001d7b1",
   "metadata": {},
   "source": [
    "Congratulations, we've been able to successfully deploy the TensorRT Engine and send an Inference request to the server! \n",
    "\n",
    "There are more features available in [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and [Triton Server](https://github.com/triton-inference-server/tensorrtllm_backend) that would be beneficial to different use-cases. You can refer the respective Github repositories to make use of latest releases and features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72ff4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "Copyright © 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d858351",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"trt-custom-model.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"llama-chat-finetune.ipynb\">1</a>\n",
    "        <a href=\"trt-llama-chat.ipynb\">2</a>\n",
    "        <a href=\"trt-custom-model.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "        <a href=\"challenge.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../LLM-Application.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
