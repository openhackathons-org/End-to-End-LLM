{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Summary.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a >6</a>\n",
    "        <a href=\"qa-riva-deployment.ipynb\">7</a>\n",
    "        <a href=\"challenge.ipynb\">8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"qa-riva-deployment.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering using Train Adapt Optimize (TAO) Toolkit\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) QA model and [**Train/Finetune**](#training) it on the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset\n",
    "- Run [**Inference**](#inference)\n",
    "- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) in a format suitable for deployment in [Riva](https://developer.nvidia.com/riva).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with TAO\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n",
    "\n",
    "`Train Adapt Optimize (TAO) Toolkit` is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\"><\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGC CLI installation and Rgistration\n",
    "\n",
    "Before TAO can be use, you need to register at ngc.nvidia.com and proceed to generate an API Key. A step-by-step process to achieving this is given below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From your browser visit `ngc.nvidia.com`\n",
    "- Click on `Welcome Guest` and you would see a dropdown menu and then click on `Sign In/Sign Up`.  \n",
    "- Click on `continue` button where `NVIDIA Account (use existing or create a new NVIDIA account)` is written.\n",
    "\n",
    "<img src=\"images/ngc_signin.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click on `Create account` and get registered. Thereafter you may proceed to login with your new account credentials.\n",
    "- At the top right corner, click on your `username`, you would see a dropdown menu, then click on `Setup`.\n",
    "\n",
    "<center><img src=\"images/ngc_setup.jpg\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click on download NGC CLI  (optional if you are running the notebook within a container)\n",
    "\n",
    "<img src=\"images/download_cli.jpg\">\n",
    "    \n",
    "- follow the steps listed to install NGC CLI (optional if you are running the notebook within a container)\n",
    "\n",
    "<img src=\"images/ngccli_installation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- proceed and click on `Get API Key` button.\n",
    "- Next, you would find at the top right corner a `Generate API Key` button, click on this button. A dialog box would appear after the click, you must click on the `confirm` button on it.\n",
    "- Finally, copy your generated API Key and Username, and save it somewhere on your local system.\n",
    "\n",
    "<img align=\"center\" src=\"images/ngc_key.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Key\n",
    "\n",
    "- Your API key represents your credentials\n",
    "  - Used for programmatic interaction (e.g., docker, REST API, etc.)\n",
    "  - Uniquely identifies you (think “Username & Password”)\n",
    "  - There can be only one (regenerating your API key invalidates the old one)\n",
    "- Programmatic interface at `nvcr.io`: Use API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the docker image versions and the tasks that perform, use the `tao info` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to installing TAO package, please make sure of the following software requirements:\n",
    "\n",
    "1. python 3.6.9\n",
    "2. docker-ce > 19.03.5\n",
    "3. docker-API 1.40\n",
    "4. nvidia-container-toolkit > 1.3.0-1\n",
    "5. nvidia-container-runtime > 3.4.0-1\n",
    "6. nvidia-docker2 > 2.5.0-1\n",
    "7. nvidia-driver >= 455.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the GPU device(s) is visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question Answering (QA)\n",
    "\n",
    "### Task Description\n",
    "The Question Answering task in NLP pertains to building a model which can answer questions posed in natural language. Many datasets (including [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), the dataset we use in this notebook) pose this as a reading comprehension task i.e. given a question and a context, the goal is to predict the span within the context with a start and end position which indicates the answer to the question. For every word in the training dataset we predict:\n",
    "- likelihood this word is the start of the span\n",
    "- likelihood this word is the end of the span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQuAD Dataset\n",
    "\n",
    "[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a large dataset for QA consisting of reading passages obtained from high-quality Wikipedia articles. With each passage, the dataset contains accompanying reading comprehension questions based on the content of the passage. For each question, there are one or more answers. These questions and corresponding answers were obtained through crowdsourcing.\n",
    "\n",
    "\n",
    "The SQuAD format consists of a JSON file for each dataset split. Each title has one or multiple paragraph entries, each consisting of the text - \"context\", and question-answer entries. Each question-answer entry has:\n",
    "\n",
    "- a question\n",
    "- a boolean flag \"is_impossible\" which shows if the question is answerable or not\n",
    "- a globally unique id\n",
    "- in case the question is answerable one answer entry, which contains the text span and its starting character index in the context. If not answerable, the \"answers\" list is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"Super_Bowl_50\", \n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\", \n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"Where did Super Bowl 50 take place?\", \n",
    "                            \"is_impossible\": \"false\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ee\", \n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"answer_start\": \"403\", \n",
    "                                    \"text\": \"Santa Clara, California\"\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"question\": \"What was the winning score of the Super Bowl 50?\", \n",
    "                            \"is_impossible\": \"true\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ez\", \n",
    "                            \"answers\": [\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation files (for validation and testing) follow the above format except for it can provide more than one answer to the same question. The inference file follows the above format except for it does not require the \"answers\" and \"is_impossible\" keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model for QA\n",
    "In this notebook, we will show how to use a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Bidirectional Encoder Representations from Transformers) model for QA leveraging TAO. The BERT model has made major breakthroughs in Natural Language Understanding in recent years. For most applications, the model is typically trained in two phases, pre-training and fine-tuning. \n",
    "- The BERT core model can be pre-trained on large, generic datasets to generate dense vector representations of input sentence(s). \n",
    "- It can be quickly fine-tuned to perform a wide variety of tasks such as question/answering, sentiment analysis, or named entity recognition.\n",
    "\n",
    "The figure below shows a high-level block diagram of pre-training and fine-tuning BERT for QA.\n",
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/05/bert-model-625x268.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alignment with the above, for pre-training we can take one of two approaches. We can either pre-train the BERT model with our own data, or use a model pre-trained by Nvidia. After we obtain a pre-trained model, the next step would be to fine-tune it for the QA task and run inference on the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/Fig4revised-625x340.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='prepare-data'></a>\n",
    "### Preparing the dataset\n",
    "The SQuAD dataset is available [here](https://rajpurkar.github.io/SQuAD-explorer/). You will find that there are 2 versions of the Squad datasets: v1.1 and v2.0. \n",
    "\n",
    "- SQuAD 1.1, the older version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.\n",
    "- SQuAD 2.0 dataset combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you may use the code below to download the dataset\n",
    "\n",
    "- Set path to a folder where you want you data to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Set path to a folder where you want you data to be saved\n",
    "#DATA_DOWNLOAD_DIR = \"/workspace/data\"\n",
    "DATA_DOWNLOAD_DIR = \"/home/tadesuyi/End-to-End-NLP_bootcamp/workspace/data\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the data and results directories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the data and results directories if they don't exist\n",
    "if not os.path.exists(DATA_DOWNLOAD_DIR):\n",
    "        os.makedirs(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- simple utility function to download the squad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# simple utility function to download the squad dataset\n",
    "def download_squad(save_path):\n",
    "    save_path = save_path + '/squad'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if not os.path.exists(save_path + '/v1.1'):\n",
    "        os.makedirs(save_path + '/v1.1')\n",
    "\n",
    "    if not os.path.exists(save_path + '/v2.0'):\n",
    "        os.makedirs(save_path + '/v2.0')\n",
    "        \n",
    "    # urls for both SQuAD v1.1 and v2.0. You may modify the dict below if you choose to download only one of the two.\n",
    "    download_urls = {\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v1.1.json': 'v1.1/train-v1.1.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v1.1.json': 'v1.1/dev-v1.1.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v2.0.json': 'v2.0/train-v2.0.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v2.0.json': 'v2.0/dev-v2.0.json',\n",
    "    }\n",
    "         \n",
    "    for item in download_urls:\n",
    "        url = item\n",
    "        file = download_urls[item]\n",
    "        print('Downloading:', url)\n",
    "        if os.path.isfile(save_path + '/' + file):\n",
    "            print('** Download file already exists, skipping download **')\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            with open(save_path + '/' + file, \"wb\") as handle:\n",
    "                handle.write(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will download both v1.1 and v2.0 versions of SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download both v1.1 and v2.0 versions of SQuAD dataset\n",
    "download_squad(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Verify that the data is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data is present\n",
    "!ls $DATA_DOWNLOAD_DIR/squad/v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TAO workflow\n",
    "The rest of the notebook shows what a sample TAO workflow looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting TAO Mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downloaded, an important step in using TAO is to set up the directory mounts. The TAO launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tao_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TAO launcher. <br>\n",
    "\n",
    "**Project directory**\n",
    "\n",
    "<img src=\"images/folder_structure.png\"  />\n",
    "\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tao_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tao_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"/home/tadesuyi/End-to-End-NLP_bootcamp/workspace/data\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/home/tadesuyi/End-to-End-NLP_bootcamp/workspace/specs\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/home/tadesuyi/End-to-End-NLP_bootcamp/workspace/results\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/home/tadesuyi/End-to-End-NLP_bootcamp/workspace/.cache\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure the source directories exist, if not, create them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "#! mkdir ~/End-to-End-NLP/workspace/specs\n",
    "#! mkdir ~/End-to-End-NLP/workspace/results\n",
    "! mkdir ~/End-to-End-NLP_bootcamp/workspace/.cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TAO workflow. Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of all commands in TAO lies in the YAML spec files. There are sample spec files already available for you to use directly or as reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "---\n",
    "```\n",
    "trainer:\n",
    "  max_epochs: 100\n",
    "\n",
    "model:\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    "  language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null\n",
    "\n",
    "  token_classifier:\n",
    "    num_layers: 1\n",
    "    dropout: 0.0\n",
    "    num_classes: 2\n",
    "    activation: relu\n",
    "    log_softmax: false\n",
    "    use_transformer_init: true\n",
    "\n",
    "\n",
    "training_ds:\n",
    "  file: ??? # e.g. squad/v1.1/train-v2.0.json\n",
    "  batch_size: 12 # per GPU\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Convert\n",
    "For the QA task, the commands in TAO accepts data in the SQuAD JSON format (refer to `Data Preprocessing Lab `). If you have your data in any other format, be sure to convert it in the SQuAD format. Since we are using the SQuAD dataset for this notebook, we don't need to convert the data into any other format. We can proceed with training/fine-tuning directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is saved here\n",
    "DATA_DIR='/data/squad'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR='/specs/questions_answering'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR='/results/questions_answering'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have access to ngc docker login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngc       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The `-o` argument indicating the folder where the default specification files will be downloaded, and `-r` that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**\n",
    "\n",
    "<img src=\"images/spec.png\"  />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tao question_answering download_specs \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    -o $SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='training'></a>\n",
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model using TAO is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tao-launcher CLI as shown below. For instance, below we override the `training_ds.file`, `validation_ds.file`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n",
    "\n",
    "For training a QA model in TAO, we use the `tao question_answering train` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-g`: Number of GPUs to use\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs <br>\n",
    "\n",
    "More details about these arguments are present in the [TAO Getting Started Guide](https://docs.nvidia.com/tao/tao-toolkit/index.html) <br>\n",
    "`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TAO docker container used in backend.<br>\n",
    "\n",
    "Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since this is a demonstration, we just train for 1 epoch below. You may need to train for more depending on your dataset.\n",
    "!tao question_answering train \\\n",
    "                        -e $SPECS_DIR/train.yaml \\\n",
    "                        -g 1  \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/train \\\n",
    "                        model.language_model.pretrained_model_name=bert-base-uncased \\\n",
    "                        training_ds.file=$DATA_DIR/v1.1/train-v1.1.json \\\n",
    "                        validation_ds.file=$DATA_DIR/v1.1/dev-v1.1.json \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        training_ds.num_workers=4 \\\n",
    "                        validation_ds.num_workers=4 \\\n",
    "                        training_ds.batch_size=4 \\\n",
    "                        validation_ds.num_samples=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "```bash\n",
    "2022-10-17 19:31:15,445 [INFO] root: Registry: ['nvcr.io']\n",
    "2022-10-17 19:31:15,588 [INFO] tlt.components.instance_handler.local_instance: Running command in container: nvcr.io/nvidia/tao/tao-toolkit-pyt:v3.21.11-py3\n",
    "2022-10-17 19:31:20,448 [WARNING] tlt.components.docker_handler.docker_handler: \n",
    "Docker will run the commands as root. If you would like to retain your\n",
    "...\n",
    "                                                                                Epoch 0, global step 8309: val_loss reached 0.60064 (best 0.60064), saving model to \"/results/questions_answering/train/checkpoints/trained-model--val_loss=0.60-epoch=0.ckpt\" as top 3\n",
    "Epoch 0: 100%|█████| 11085/11085 [34:46<00:00,  5.31it/s, loss=1.02, lr=4.03e-7]\n",
    "Validating: 0it [00:00, ?it/s]\n",
    "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\n",
    "Validating: 100%|█████████████████████████████████| 1/1 [00:00<00:00,  3.20it/s][NeMo I 2022-10-17 11:08:41 qa_model:173] val exact match 100.0\n",
    "[NeMo I 2022-10-17 11:08:41 qa_model:174] val f1 100.0\n",
    "Epoch 0: 100%|█████| 11085/11085 [34:47<00:00,  5.31it/s, loss=1.02, lr=4.03e-7]\n",
    "                                                                                Epoch 0, global step 11079: val_loss reached 0.38622 (best 0.38622), saving model to \"/results/questions_answering/train/checkpoints/trained-model--val_loss=0.39-epoch=0.ckpt\" as top 3\n",
    "Epoch 0: 100%|█████| 11085/11085 [35:03<00:00,  5.27it/s, loss=0.99, lr=2.85e-7]\n",
    "[NeMo I 2022-10-17 11:09:13 train:136] Experiment logs saved to '/results/questions_answering/train'\n",
    "[NeMo I 2022-10-17 11:09:13 train:137] Trained model saved to '/results/questions_answering/train/checkpoints/trained-model.tlt'\n",
    "2022-10-17 20:09:18,886 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tips and tricks:\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n",
    "- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches.\n",
    "- You can use the parameter: `trainer.val_check_interval` to define how many times per epoch to see validation accuracy metric calculated and printed. For instance, using `trainer.val_check_interval=0.25` will show the metric 4 times per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fine-Tuning\n",
    "Like many other NLP tasks, since we begin with a pretrained BERT model the step shown above for (re)training with your custom data should do the trick. However, TAO does provide a command for fine-tuning if your use-case demands that. Instead of `tao question_answering train`, we use `tao question_answering finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TAO follow a similar pattern, streamlining the workflow even further!\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao  question_answering finetune \\\n",
    "                        -e $SPECS_DIR/finetune.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/finetune \\\n",
    "                        finetuning_ds.file=$DATA_DIR/v2.0/train-v2.0.json \\\n",
    "                        validation_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        finetuning_ds.num_workers=4 \\\n",
    "                        validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "Epoch 0: 100%|█████████▉| 6002/6008 [40:12<00:02,  2.49it/s, loss=0.83, lr=5e-6]\n",
    "Epoch 0: 100%|█████████▉| 6004/6008 [40:12<00:01,  2.49it/s, loss=0.83, lr=5e-6]\n",
    "Epoch 0: 100%|█████████▉| 6006/6008 [40:12<00:00,  2.49it/s, loss=0.83, lr=5e-6]\n",
    "Epoch 0: 100%|██████████| 6008/6008 [40:28<00:00,  2.47it/s, loss=0.83, lr=5e-6]\n",
    "Validating: 100%|█████████████████████████████| 510/510 [01:05<00:00, 10.06it/s][NeMo I 2022-10-17 11:53:49 qa_model:173] val exact match 71.5320475027373\n",
    "[NeMo I 2022-10-17 11:53:49 qa_model:174] val f1 74.46986178542292\n",
    "Epoch 0: 100%|██████████| 6008/6008 [41:07<00:00,  2.43it/s, loss=0.83, lr=5e-6]\n",
    "                                                                                Epoch 0, global step 5497: val_loss reached 0.98476 (best 0.98476), saving model to \"/results/questions_answering/finetune/checkpoints/finetuned-model--val_loss=0.98-epoch=0.ckpt\" as top 3\n",
    "Epoch 0: 100%|██████████| 6008/6008 [41:24<00:00,  2.42it/s, loss=0.83, lr=5e-6]\n",
    "[NeMo I 2022-10-17 11:54:21 finetune:133] Experiment logs saved to '/results/questions_answering/finetune'\n",
    "[NeMo I 2022-10-17 11:54:21 finetune:134] Fine-tuned model saved to '/results/questions_answering/finetune/checkpoints/finetuned-model.tlt'\n",
    "2022-10-17 20:54:29,916 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec .yaml is as simple as:\n",
    "\n",
    "```\n",
    "test_ds:\n",
    "  file: ??? # e.g. squad/v1.1/dev-v1.1.json \n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "  num_samples: 500\n",
    "```\n",
    "\n",
    "Below, we use `tao question_answering evaluate` and override the test data configuration by specifying `test_ds.file`. Other arguments follow the same pattern as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao question_answering evaluate \\\n",
    "                        -e $SPECS_DIR/evaluate.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/evaluate \\\n",
    "                        test_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n",
    "                        test_ds.batch_size=32 \\\n",
    "                        test_ds.num_samples=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Evaluation should give the exact match/f1 scores for each data point. Remember that we had trained for just 1 epoch since this is a demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='inference'></a>\n",
    "### Inference\n",
    "Inference using a .tlt trained or fine-tuned model uses the `tao question_answering infer` command.  <br>\n",
    "The infer.yaml is also very uncomplicated:\n",
    "```\n",
    "# Name of  file containing data used as inputs during the inference.\n",
    "infer_ds:\n",
    "    file: ???  # e.g. squad/v1.1/dev-v1.1.json\n",
    "\n",
    "```\n",
    "\n",
    "We use the SQuAD 2.0 evaluation file for the sake of demonstration, you can also try out your own custom inputs as an exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao question_answering infer \\\n",
    "                        -e $SPECS_DIR/infer.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/infer \\\n",
    "                        infer_ds.file=$DATA_DIR/v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected result:**\n",
    "```python\n",
    "...\n",
    "[NeMo I 2022-10-17 12:11:51 infer:92] Question: What is the force exerted by standard gravity on one ton of mass?\n",
    "[NeMo I 2022-10-17 12:11:51 infer:93] Answer  : kilogram-force\n",
    "[NeMo I 2022-10-17 12:11:51 infer:92] Question: What force leads to a commonly used unit of mass?\n",
    "[NeMo I 2022-10-17 12:11:51 infer:93] Answer  : kilogram-force\n",
    "[NeMo I 2022-10-17 12:11:51 infer:92] Question: What force is part of the modern SI system?\n",
    "[NeMo I 2022-10-17 12:11:51 infer:93] Answer  : kilogram-force\n",
    "[NeMo I 2022-10-17 12:11:51 infer:96] Experiment logs saved to '/results/questions_answering/infer'\n",
    "2022-10-17 21:11:53,005 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
    "```\n",
    "<img src=\"images/infer.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of inference are saved at `$RESULTS_DIR/infer/prediction.txt` in the format:\n",
    "```\n",
    "{\n",
    "    <question_id>: <answer>,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "A file with n-best results for each question is also saved at `$RESULTS_DIR/infer/nbest.txt` in the format:\n",
    "```\n",
    "{\n",
    "    <question_id>: [\n",
    "        {\n",
    "            \"text\": <answer-1>,\n",
    "            \"probability\": 0.9576789114427999,\n",
    "            \"start_logit\": 7.168248653411865,\n",
    "            \"end_logit\": 6.93817138671875\n",
    "        },\n",
    "        {\n",
    "            \"text\": <answer-2>,\n",
    "            \"probability\": 0.02714239211951417,\n",
    "            \"start_logit\": 7.168248653411865,\n",
    "            \"end_logit\": 3.374755620956421\n",
    "        },\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='export-onnx'></a>\n",
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, easing the path to production. \n",
    "\n",
    "TAO provides commands to export the .tlt model to the ONNX format in an .eonnx archive.  The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "Sample usage of the `tao question_answering export` command is shown in the following code cell. The export.yaml file we use looks like:\n",
    "```\n",
    "# Name of the .tlt EFF archive to be loaded/model to be exported.\n",
    "restore_from: trained-model.tlt\n",
    "\n",
    "# Set export format: ONNX | RIVA\n",
    "export_format: RIVA\n",
    "\n",
    "# Output EFF archive containing ONNX.\n",
    "export_to: exported-model.eonnx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao question_answering export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/export \\\n",
    "                        export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAO provides the capability to use the exported .eonnx model for inference. The command `tao question_answering infer_onnx` is very similar to the inference command for .tlt models. Again, the input file used are just for demo purposes, you may choose to try out their custom input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao question_answering infer_onnx \\\n",
    "                        -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/infer_onnx \\\n",
    "                       infer_ds.file=$DATA_DIR/v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The output of this `infer_onnx` is similar to that of the `infer` command before. A `prediction.txt` and a `nbest.txt` file is generated. You can configure the exact name/location of these results files in the spec yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='export-riva'></a>\n",
    "### Export to Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TAO, you can also export your model in a format that can deployed using [Nvidia Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tao question_answering export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/export_riva \\\n",
    "                        export_format=RIVA \\\n",
    "                        export_to=qa-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Your output should look similar as shown below:**\n",
    "```python\n",
    "...\n",
    "[NeMo I 2022-11-01 12:33:56 export:59] Model restored from '/results/questions_answering/train/checkpoints/trained-model.tlt'\n",
    "[NeMo I 2022-11-01 12:34:11 export:83] Experiment logs saved to '/results/questions_answering/export_riva'\n",
    "[NeMo I 2022-11-01 12:34:11 export:84] Exported model to '/results/questions_answering/export_riva/qa-model.riva'\n",
    "[NeMo I 2022-11-01 12:34:12 export:95] Exported model is compliant with Riva\n",
    "2022-11-01 21:34:17,668 [INFO] tlt.components.docker_handler.docker_handler: Stopping container.\n",
    "\n",
    "```\n",
    "<img src=\"images/riva_export.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `qa-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use TAO to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Summary.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "         <a >6</a>\n",
    "        <a href=\"qa-riva-deployment.ipynb\">7</a>\n",
    "        <a href=\"challenge.ipynb\">8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"qa-riva-deployment.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
