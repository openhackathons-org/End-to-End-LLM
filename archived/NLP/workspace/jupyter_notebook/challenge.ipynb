{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Summary.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a>6</a>\n",
    "        <a  href=\"qa-riva-deployment.ipynb\">7</a>\n",
    "        <a>8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"\"></a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction\n",
    "\n",
    "In the challenge you are expected to replicate the Riva deployment process using custom trained model from the TAO question aswering notebook. To succesfully complete the challenge, you are expected to implement the following steps: \n",
    "\n",
    "- Copy the `qa-model.riva` model to the challenge directory\n",
    "- Set the environment variables\n",
    "- Generate the `.rmir` file using the `riva-speech:2.6.0-servicemaker`\n",
    "- Execute riva-deploy to depoy the `.rmir` file\n",
    "- Modify the `config.sh` and `riva_init.sh`\n",
    "- Set permission for `riva_start.sh` \n",
    "- Run riva start\n",
    "- Run inference\n",
    "- Complete the sample QA implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the qa-model.riva model to challenge directory\n",
    "\n",
    "Run the cell below without modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../results/questions_answering/export_riva/qa-model.riva  ../results/challenge/export_riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set evironment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of ServiceMaker Docker Image\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.6.0-servicemaker\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = os.path.realpath(os.getcwd()+'/../')+\"/results/challenge/export_riva\"\n",
    "\n",
    "############################ complete this section ##################################\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"\"\n",
    "\n",
    "# Key that model is encrypted with\n",
    "KEY = \"\"\n",
    "\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Riva-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execute `riva-build` to create the `.rmir` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --gpus 1 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build qa -f /data/question-answering.rmir:$KEY /data/$MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to view the `question-answering.rmir` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Run Riva-deploy to deploy your .rmir file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --rm --gpus 1 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f /data/question-answering.rmir:$KEY /data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Riva Server\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the Riva QuickStart directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVA_DIR = os.path.realpath(os.getcwd()+'/../')+\"/source_code/challenge_quickstart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify `config.sh` and `riva_start` files at `../source_code/challenge_quickstart`. Please to save the files by pressing `Ctrl S` otherwise the changes made will not be effected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.sh snipet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=false                                 ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=true                                  ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                 ## MAKE CHANGES HERE\n",
    "...\n",
    "```\n",
    "### riva_start.sh snipet\n",
    "```\n",
    "...\n",
    "# speech server is required\n",
    "# check if it's already running first...\n",
    "if [ $(docker ps -q -f \"name=^/$riva_daemon_speech$\" | wc -l) -eq 0 ]; then\n",
    "    echo \"Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\"\n",
    "    docker rm $riva_daemon_speech &> /dev/null\n",
    "    if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "        docker_run_args=\"-p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 --device /dev/bus/usb --device /dev/snd $image_speech_api riva_server $ssl_args\"\n",
    "    else\n",
    "        docker_run_args=\"-p 8000 -p 8001 -p 8002 $image_speech_api start-riva --riva-uri=0.0.0.0:$riva_speech_api_port --asr_service=$service_enabled_asr --tts_service=$service_enabled_tts --nlp_service=$service_enabled_nlp $ssl_args &> /dev/null\"\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set permission to files in the Riva challenge Quickstart directory\n",
    "complete the statement in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ complete the missing part ##################################\n",
    "!cd $RIVA_DIR && chmod +x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Riva Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ complete this section ##################################\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incase the server could not start, uncomment the cell below and run to view log trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "\n",
    "grpc_server =  \"localhost:50051\"\n",
    "channel = grpc.insecure_channel(grpc_server)\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n",
    "req = rnlp.NaturalQueryRequest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create QA request context and query, and get response result from the Riva server.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ complete this section ##################################\n",
    "context_1 = \"\"\n",
    "\n",
    "\n",
    "req.query = \"\"\n",
    "\n",
    "req.context = \n",
    "\n",
    "resp = \n",
    "print(resp)\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another query or question from the context above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ complete the missing part ###################################\n",
    "req.query = \"\"\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample QA implementation \n",
    "\n",
    "In this section, in this section you are given a context and a speech question drawn from the context. You are expected to do the following:\n",
    "- apply a ASR speech to text model to transcribe\n",
    "- send query to riva server and get response\n",
    "- apply TTS model to synthesize response to speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_2 = 'Climate is the average of the weather conditions at a particular point on the Earth. Typically, climate \\\n",
    "is expressed in terms of expected temperature, rainfall and wind conditions based on historical observations. Climate change \\\n",
    "is a change in either the average climate or climate variability that persists over an extended period.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given query speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "sys.path.append('/usr/bin/ffmpeg')\n",
    "\n",
    "\n",
    "audio_sample = \"../data/speech/audio_question_1.wav\"\n",
    "IPython.display.Audio(\"../data/speech/audio_question_1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ASR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "# Import Speech Recognition collection\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import IPython\n",
    "\n",
    "# Here is an example of all CTC-based models:\n",
    "nemo_asr.models.EncDecCTCModel.list_available_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Speech Recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################ complete this section ##################################\n",
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"fill in model here\").cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the ASR model to transcribe the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = asr_model.transcribe([audio_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send transcription as request to RIVA server and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################complete this section ##############\n",
    "req.query = \n",
    "req.context = \n",
    "resp =\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### write your custom function to extract result from the resp ############# \n",
    "def extract_answer(results):\n",
    "    #complete this function\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  extract_answer(resp.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from nemo.collections.tts.models import Tacotron2Model\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "############################ complete this section ##################################\n",
    "\n",
    "# Download and load the pretrained Tacotron model\n",
    "spec_generator = Tacotron2Model.from_pretrained(\"fill in here\")\n",
    "\n",
    "# Download and load the pretrained hifigan model\n",
    "vocoder = HifiGanModel.from_pretrained(model_name=\"fill in here\")\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = spec_generator.parse(response)\n",
    "\n",
    "########## complete this section ###########\n",
    "#produce a spectrogram\n",
    "spectrogram =\n",
    "\n",
    "#converts the spectrogram to audio\n",
    "audio = \n",
    "############################################   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Display response speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_save = \"../data/speech/challenge_answer.wav\"\n",
    "#Save the audio to disk\n",
    "sf.write(path_to_save, answer_speech.to('cpu').detach().numpy()[0], 22050)\n",
    "\n",
    "IPython.display.Audio(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can stop all docker container before shutting down the jupyter kernel. **Caution: The following command will stop all running containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's next?\n",
    "You could train your own custom models in TAO and deploy them in Riva! You could scale up your deployment using Kubernetes with the Riva AI Services Helm Chart, which will pull the relevant Images and download model artifacts from NGC, generate the model repository, start and expose the Riva speech services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In the next lab, you will learn about NeMo Fundamentals, how to execute P-tuning with NeMo Megatron, and Prompt testing on NeMo-megatron GTP 20B paramter model. Please click on the `Start NeMo Lab` link below to get started.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red;height:90px;\"> <b><br/> [Start NeMo Lab](nemo/NeMo_Primer.ipynb) </b> </div></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Summary.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "         <a>6</a>\n",
    "        <a href=\"qa-riva-deployment.ipynb\">7</a>\n",
    "        <a >8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"\"></a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
