{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"question-answering-training.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"question-answering-training.ipynb\">6</a>\n",
    "        <a >7</a>\n",
    "        <a href=\"challenge.ipynb\">8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a Question Answering Model in Riva\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn how to:  \n",
    "- Use Riva ServiceMaker to take a TAO exported .riva and generate a model [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart)\n",
    "- Deploy the model(s) locally  on the Riva Server\n",
    "- Send inference requests from a demo client using Riva API bindings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA RIVA\n",
    "\n",
    "[NVIDIA Riva](https://developer.nvidia.com/riva), is a highly performant application framework for multi-modal conversational AI services using GPUs. It is a GPU-accelerated SDK for building customized Speech AI applications for real-time performance. TAO models can be exported in .riva format, optimized, and deployed on Riva as a speech service. Riva services are delivered as gRPC-based microservices for low-latency streaming, as well as high-throughput offline use cases. One major features of Riva is flexibility, you can modify model architectures, fine-tuning models on your data and customizing pipelines, as well as the ability to deploy on any platform.\n",
    "\n",
    "This notebook explores taking a `.riva model`, the result of `tao question_answering export` from the previous notebook, and leveraging the `Riva ServiceMaker` framework to aggregate all the necessary artifacts for Riva deployment to a target environment. Once the model is deployed in Riva, you can issue inference requests to the server. We will demonstrate how quick and straightforward to acheive this whole process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Riva ServiceMaker\n",
    "Servicemaker is the set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components:\n",
    "- Riva-build\n",
    "- Riva-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called a RMIR) of an end to end pipeline for the supported services within Riva. We are taking a QA model in consideration.<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (.riva files) into a single file containing an intermediate format called `Riva Model Intermediate Representation (.rmir)`. This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. Please check out the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/service-nlp.html#pipeline-configuration) to find out more. \n",
    "\n",
    "Steps to execute `riva-build` is presented as follows:\n",
    "- Have your `qa-model.riva` model ready as shown below\n",
    "\n",
    "<img align=\"center\" src=\"images/qa_model_riva.png\">\n",
    "\n",
    "`NOTE:` The above qa-model.riva is the qa model obtained from the previous notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set variables: \n",
    "    - ServiceMaker Docker Image\n",
    "    - Directory where the .riva model\n",
    "    - Name of the .riva file\n",
    "    - model encryption (Key from NGC used in the previous notebook during TAO training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Name of ServiceMaker Docker Image\n",
    "RIVA_SM_CONTAINER = \"nvcr.io/nvidia/riva/riva-speech:2.6.0-servicemaker\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = os.path.realpath(os.getcwd()+'/../')+\"/results/questions_answering/export_riva\"\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"qa-model.riva\"\n",
    "\n",
    "# Key that model is encrypted with\n",
    "KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pull the ServiceMaker docker by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the ServiceMaker docker\n",
    "!docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You should see similar output as this**:\n",
    "```python\n",
    "2.6.0-servicemaker: Pulling from nvidia/riva/riva-speech\n",
    "\n",
    "b49e2995: Pulling fs layer \n",
    "0e6f7157: Pulling fs layer \n",
    "53daec0c: Pulling fs layer \n",
    "24ede650: Pulling fs layer \n",
    "...\n",
    "c00d2faf: Pull complete  620B/620B2kBBDigest: sha256:88cc5298313eeb3f0105000c9f2a215bbc2ab0e950e1b6dd32898490e020cde8\n",
    "Status: Downloaded newer image for nvcr.io/nvidia/riva/riva-speech:2.6.0-servicemaker\n",
    "nvcr.io/nvidia/riva/riva-speech:2.6.0-servicemaker\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Execute `riva-build` to create the `Riva Model Intermediate Representation (.rmir)` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "!docker run --rm --gpus '\"device=0:0\"' -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build qa -f /data/question-answering.rmir:$KEY /data/$MODEL_NAME:$KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output should look as shown below**:\n",
    "\n",
    "```python\n",
    "==========================\n",
    "=== Riva Speech Skills ===\n",
    "==========================\n",
    "\n",
    "NVIDIA Release  (build 45250447)\n",
    "Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "\n",
    "https://developer.nvidia.com/tensorrt\n",
    "\n",
    "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "...\n",
    "2022-10-21 12:04:55,988 [INFO] Packing binaries for tokenizer/PyTorch : {'vocab': ('nemo.collections.nlp.models.question_answering.qa_model.QAModel', 'tokenizer.vocab_file')}\n",
    "2022-10-21 12:04:55,988 [INFO] Copying vocab:tokenizer.vocab_file -> tokenizer:tokenizer-tokenizer.vocab_file\n",
    "2022-10-21 12:04:55,989 [INFO] Saving to /data/question-answering.rmir\n",
    "```\n",
    "\n",
    "The generated `.rmir` file should be found at `../results/questions_answering/export_riva/` directory\n",
    "\n",
    "<img align=\"center\" src=\"images/rmir.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to view the `question-answering.rmir` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $MODEL_LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more `Riva Model Intermediate Representation (RMIR)` files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output `model` repository directory.\n",
    "\n",
    "Run the cell below to execute `riva-deploy`. Please note that the flag `-v` represents mapping model directory from your local machine to the /data directory within the Riva ServiceMaker container, while the `-f` is to forcefully overwrite any preexiting `models` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "!docker run --rm --gpus '\"device=0:0\"' -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f /data/question-answering.rmir:$KEY /data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "````bash\n",
    "==========================\n",
    "=== Riva Speech Skills ===\n",
    "==========================\n",
    "\n",
    "NVIDIA Release  (build 45250447)\n",
    "Copyright (c) 2016-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "Copyright (c) 2018-2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "https://developer.nvidia.com/tensorrt\n",
    "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "...\n",
    "\n",
    "2022-10-21 12:32:37,291 [INFO] extracting {'vocab': ('nemo.collections.nlp.models.question_answering.qa_model.QAModel', 'tokenizer.vocab_file')} -> /data/models/qa_tokenizer-en-US/1\n",
    "2022-10-21 12:32:37,292 [INFO] Extract_binaries for token_classifier -> /data/models/qa_qa_postprocessor/1\n",
    "2022-10-21 12:32:37,293 [INFO] Extract_binaries for self -> /data/models/riva_qa/1\n",
    "\n",
    "````\n",
    "You should also see the generated `models` subdirectory at `../results/questions_answering/export_riva/`\n",
    "\n",
    "<img align=\"center\" src=\"images/riva_models.png\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Riva Server\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you need to download the Riva QuickStart Resource from NGC. \n",
    "You can access NVIDIA NGC, and download the Riva Quickstart here [resources](https://ngc.nvidia.com/catalog/resources/nvidia:riva:riva_quickstart). Your Riva Quickstart (version 2.6.0) folder should contain list of files shown below. \n",
    "\n",
    "<img align=\"center\" src=\"images/riva_quickstart_v26.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the Riva QuickStart directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVA_DIR = os.path.realpath(os.getcwd()+'/../')+\"/source_code/riva_quickstart_v2.6.0\"\n",
    "RIVA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify `config.sh` to enable relevant Riva services (nlp for Question Answering model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For instance, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC`\n",
    "\n",
    "Pretrained versions of models specified in models_asr/nlp/tts are fetched from NGC. Since we are using our custom model, we can comment it in models_nlp (and any others that are not relevant to your use case). <br>\n",
    "\n",
    "`NOTE:` You can perform this step of editing **config.sh** `../source_code/riva_quickstart_v2.6.0/config.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config.sh snipet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=false                                 ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=true                                  ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                 ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                             ## Set the model encryption key\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "...\n",
    "riva_model_loc=\"<add path>\"                              ## Replace with MODEL_LOC\n",
    "\n",
    "# The default RMIRs are downloaded from NGC by default in the above $riva_rmir_loc directory\n",
    "# If you'd like to skip the download from NGC and use the existing RMIRs in the $riva_rmir_loc\n",
    "# then set the below $use_existing_rmirs flag to true.\n",
    "...\n",
    "use_existing_rmirs=true                                  ## Set to True\n",
    "```\n",
    "Since we are interested in the nlp part, open the `riva_start.sh`, go to line 93, and remove these flags: `--asr_service=$service_enabled_asr --tts_service=$service_enabled_tts`  \n",
    "\n",
    "### riva_start.sh snipet\n",
    "```\n",
    "...\n",
    "85  #speech server is required\n",
    "86 #check if it's already running first...\n",
    "87 if [ $(docker ps -q -f \"name=^/$riva_daemon_speech$\" | wc -l) -eq 0 ]; then\n",
    "88    echo \"Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\"\n",
    "89    docker rm $riva_daemon_speech &> /dev/null\n",
    "90    if [[ $riva_target_gpu_family == \"tegra\" ]]; then\n",
    "91        docker_run_args=\"-p 8000:8000 -p 8001:8001 -p 8002:8002 -p 8888:8888 --device /dev/bus/usb --device /dev/snd $image_speech_api riva_server $ssl_args\"\n",
    "92    else\n",
    "93        docker_run_args=\"-p 8000 -p 8001 -p 8002 $image_speech_api start-riva --riva-uri=0.0.0.0:$riva_speech_api_port --asr_service=$service_enabled_asr --tts_service=$service_enabled_tts --nlp_service=$service_enabled_nlp $ssl_args &> /dev/null\"\n",
    "94 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure you have permission to execute these scripts from the Riva Quickstart directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Riva Start. This will deploy your model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "!cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When you should see the output below, its implies your model(s) is successfully deployed and Riva server is ready for inference request.\n",
    "\n",
    "```bash\n",
    "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
    "d51f487128807235ce4bf258853c7c7387ea9ae019ba1e47a05694673b0b0342\n",
    "Waiting for Riva server to load all models...retrying in 10 seconds\n",
    "Waiting for Riva server to load all models...retrying in 10 seconds\n",
    "Riva server is ready...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "If the Riva server is fails to start, uncomment and run the cell below (`docker logs riva-speech`) to identify the errors within the logs\n",
    "\n",
    "```bash\n",
    "...\n",
    "I1101 13:15:43.179256 103 tensorrt.cc:5454] TRITONBACKEND_ModelInstanceInitialize: riva-trt-riva_qa-nn-bert-base-uncased_0 (GPU device 0)\n",
    "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
    "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
    "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
    "  > Riva waiting for Triton server to load all models...retrying in 1 second\n",
    "I1101 13:15:47.239278 103 logging.cc:49] [MemUsageChange] Init CUDA: CPU +253, GPU +0, now: CPU 287, GPU 1456 (MiB)\n",
    "I1101 13:15:47.473845 103 logging.cc:49] Loaded engine size: 208 MiB\n",
    "\n",
    "...\n",
    "\n",
    "+------------------+------+\n",
    "| Repository Agent | Path |\n",
    "+------------------+------+\n",
    "+------------------+------+\n",
    "\n",
    "I1101 13:15:54.687420 103 server.cc:576] \n",
    "+--------------------+-------------------------------------------------------------------------------+--------+\n",
    "| Backend            | Path                                                                          | Config |\n",
    "+--------------------+-------------------------------------------------------------------------------+--------+\n",
    "| onnxruntime        | /opt/tritonserver/backends/onnxruntime/libtriton_onnxruntime.so               | {}     |\n",
    "| tensorrt           | /opt/tritonserver/backends/tensorrt/libtriton_tensorrt.so                     | {}     |\n",
    "| riva_nlp_qa        | /opt/tritonserver/backends/riva_nlp_qa/libtriton_riva_nlp_qa.so               | {}     |\n",
    "| riva_nlp_tokenizer | /opt/tritonserver/backends/riva_nlp_tokenizer/libtriton_riva_nlp_tokenizer.so | {}     |\n",
    "+--------------------+-------------------------------------------------------------------------------+--------+\n",
    "\n",
    "I1101 13:15:54.687497 103 server.cc:619] \n",
    "+---------------------------------------+---------+--------+\n",
    "| Model                                 | Version | Status |\n",
    "+---------------------------------------+---------+--------+\n",
    "| qa_qa_postprocessor                   | 1       | READY  |\n",
    "| qa_tokenizer-en-US                    | 1       | READY  |\n",
    "| riva-trt-riva_qa-nn-bert-base-uncased | 1       | READY  |\n",
    "| riva_qa                               | 1       | READY  |\n",
    "+---------------------------------------+---------+--------+\n",
    "...\n",
    "\n",
    "I1101 13:15:54.841537 103 http_server.cc:180] Started Metrics Service at 0.0.0.0:8002\n",
    "  > Triton server is ready...\n",
    "ERROR: illegal value '' specified for bool flag 'asr_service'\n",
    "ERROR: illegal value '' specified for bool flag 'tts_service'\n",
    "One of the processes has exited unexpectedly. Stopping container.\n",
    "Signal (15) received.\n",
    "/opt/riva/bin/start-riva: line 1: kill: (197) - No such process\n",
    "I1101 13:16:00.113213 103 server.cc:250] Waiting for in-flight requests to complete.\n",
    "I1101 13:16:00.113252 103 server.cc:266] Timeout 30: Found 0 model versions that have in-flight inferences\n",
    "I1101 13:16:00.113277 103 model_repository_manager.cc:1109] unloading: riva_qa:1\n",
    "I1101 13:16:00.113419 103 model_repository_manager.cc:1109] unloading: riva-trt-riva_qa-nn-bert-base-uncased:1\n",
    "I1101 13:16:00.113504 103 model_repository_manager.cc:1109] unloading: qa_tokenizer-en-US:1\n",
    "I1101 13:16:00.113636 103 model_repository_manager.cc:1109] unloading: qa_qa_postprocessor:1\n",
    "I1101 13:16:00.113837 103 model_repository_manager.cc:1214] successfully unloaded 'riva_qa' version 1\n",
    "\n",
    "```\n",
    "**Solution**\n",
    "\n",
    "The solution is set to `asr_service` and `tts_service` to **false** within the `config.sh` file and ensure that line 93 in the `riva_start.sh` is change from:\n",
    "```bash\n",
    "docker_run_args=\"-p 8000 -p 8001 -p 8002 $image_speech_api start-riva --riva-uri=0.0.0.0:$riva_speech_api_port --asr_service=$service_enabled_asr --tts_service=$service_enabled_tts --nlp_service=$service_enabled_nlp $ssl_args &> /dev/null\"\n",
    "\n",
    "to:\n",
    "\n",
    "docker_run_args=\"-p 8000 -p 8001 -p 8002 $image_speech_api start-riva --riva-uri=0.0.0.0:$riva_speech_api_port  --nlp_service=$service_enabled_nlp $ssl_args &> /dev/null\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send GRPC requests, you can install Riva Python API bindings for client. This is available as a pip .whl in the QuickStart directory. However, if the version of Riva QuickStart is 2.7.0 and above, you don't need to explicitly install the python binding.Therefore, you can skip the installation cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: This is only applicable to riva quickstart v2.6.0 and earlier version\n",
    "# Set the name of the whl file.\n",
    "\n",
    "RIVA_API_WHL = \"riva_api-2.1.0-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install client API bindings\n",
    "!cd $RIVA_DIR && pip install $RIVA_API_WHL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code sample shows how you can perform inference using Riva Python API gRPC bindings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "\n",
    "grpc_server =  \"localhost:50051\"\n",
    "channel = grpc.insecure_channel(grpc_server)\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n",
    "req = rnlp.NaturalQueryRequest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create QA request context and query, and get response result from the Riva server.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = \"In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought.\"\\\n",
    "                \"The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2)\" \\\n",
    "                \" in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part.\" \\\n",
    "                \" The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005\" \\\n",
    "                \"instead 5 gigatons were released and in 2010 8 gigatons were released.\"\n",
    "\n",
    "\n",
    "req.query = \"How many gigatons of carbon are absorbed the Amazon in a typical year?\"\n",
    "\n",
    "req.context = context_1\n",
    "resp = riva_nlp.NaturalQuery(req)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another query or question from the context above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req.query = \"the affected region by drought in 2010 is approximately?\"\n",
    "req.context = context_1\n",
    "resp = riva_nlp.NaturalQuery(req)\n",
    "print(resp.results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample QA implementation \n",
    "\n",
    "In this section, we want to demostrate an inquiry system where a user can make an enquiry through verbally and get response on speech form. First, we create a textual context and two speech queries from the context for a sample scenario forour demostration. Due to the nature of bootcamp computing environ, the queries were captured as .wav files. Each query is sent to the riva server for inferencing. During this process, speech goes through an Automatic Speech Recognition (ASR) model and is being transcribe to text. This output serves as request the QA model deployed on the Riva server. The response from the server is transformed to speech output via a TTS model.\n",
    "\n",
    "<img align=\"center\" src=\"images/sample_application.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR: Speech-To-Text(STT)\n",
    "\n",
    "Automatic Speech Recognition (ASR) takes an audio stream or audio buffer as input and returns one or more text transcripts, along with additional optional metadata.\n",
    "Speech recognition in Riva is a GPU-accelerated compute pipeline, with optimized performance and accuracy. In the cell below, we display list of ASR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "# Import Speech Recognition collection\n",
    "import nemo.collections.asr as nemo_asr\n",
    "import IPython\n",
    "\n",
    "# Here is an example of all CTC-based models:\n",
    "nemo_asr.models.EncDecCTCModel.list_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Expected Output:**\n",
    "```bash\n",
    "[PretrainedModelInfo(\n",
    " \tpretrained_model_name=QuartzNet15x5Base-En,\n",
    " \tdescription=QuartzNet15x5 model trained on six datasets: LibriSpeech, Mozilla Common Voice (validated clips from en_1488h_2019-12-10), WSJ, Fisher, Switchboard, and NSC Singapore English. It was trained with Apex/Amp optimization level O1 for 600 epochs. The model achieves a WER of 3.79% on LibriSpeech dev-clean, and a WER of 10.05% on dev-other. Please visit https://ngc.nvidia.com/catalog/models/nvidia:nemospeechmodels for further details.,\n",
    " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\n",
    " ),\n",
    " PretrainedModelInfo(\n",
    " \tpretrained_model_name=stt_en_quartznet15x5,\n",
    " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_en_quartznet15x5,\n",
    " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\n",
    " ),\n",
    " PretrainedModelInfo(\n",
    " \tpretrained_model_name=stt_en_jasper10x5dr,\n",
    " \tdescription=For details about this model, please visit https://ngc.nvidia.com/catalog/models/nvidia:nemo:stt_en_jasper10x5dr,\n",
    " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_jasper10x5dr/versions/1.0.0rc1/files/stt_en_jasper10x5dr.nemo\n",
    " ),\n",
    " ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Speech Recognition model `stt_en_jasper10x5dr.nemo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"stt_en_jasper10x5dr\").cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_2 = 'Climate is the average of the weather conditions at a particular point on the Earth. Typically, climate \\\n",
    "is expressed in terms of expected temperature, rainfall and wind conditions based on historical observations. Climate change \\\n",
    "is a change in either the average climate or climate variability that persists over an extended period.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set question speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "sys.path.append('/usr/bin/ffmpeg')\n",
    "\n",
    "audio_question_1 = \"../data/speech/audio_question_1.wav\"\n",
    "IPython.display.Audio(audio_question_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the ASR model to transcribe the speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = asr_model.transcribe([audio_question_1])\n",
    "transcribed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send transcription as request to RIVA server and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req.query = str(transcribed_text[0]).strip()\n",
    "req.context = context_2\n",
    "resp = riva_nlp.NaturalQuery(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(results):\n",
    "    dic = results\n",
    "    answer = str(dic[0]).split('\\n')[0].split(':')[1]\n",
    "    answer= answer.replace('\"', '').strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  extract_answer(resp.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS\n",
    "\n",
    "The text-to-speech (TTS) pipeline is based on a two-stage pipeline. First, its generates a mel-spectrogram using the first model, and then generates speech using the second model. This pipeline forms a TTS system that enables synthesis of natural sounding speech from raw transcripts without any additional information such as patterns or rhythms of speech.\n",
    "\n",
    "Run the cells below to apply TTS model to convert response to audio speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Download and load the pretrained fastpitch model\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "spec_generator = FastPitchModel.from_pretrained(\"nvidia/tts_en_fastpitch\")\n",
    "\n",
    "# Download and load the pretrained hifigan model\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "vocoder = HifiGanModel.from_pretrained(model_name=\"tts_en_hifigan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All spectrogram generators start by parsing raw strings to a tokenized version of the string\n",
    "def texttospeech(text):\n",
    "    \n",
    "    parsed = spec_generator.parse(text)\n",
    "    #produce a spectrogram\n",
    "    spectrogram = spec_generator.generate_spectrogram(tokens=parsed)\n",
    "    #converts the spectrogram to audio\n",
    "    audio = vocoder.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Display response speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_speech = texttospeech(response)\n",
    "path_to_save = \"../data/speech/audio_answer_1.wav\"\n",
    "#Save the audio to disk\n",
    "sf.write(path_to_save, answer_speech.to('cpu').detach().numpy()[0], 22050)\n",
    "\n",
    "IPython.display.Audio(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Context_2, create second speech query**\n",
    "\n",
    "Run the cell below to set the speech audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#req.query = \"Based on historical observations, climate can be expressed in terms of what ?\"\n",
    "audio_question_2 = \"../data/speech/audio_question_2.wav\"\n",
    "IPython.display.Audio(audio_question_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to get speech response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text_2 = asr_model.transcribe([audio_question_2])\n",
    "req.query = str(transcribed_text_2[0]).strip()\n",
    "req.context = context_2\n",
    "resp = riva_nlp.NaturalQuery(req)\n",
    "response_2 =  extract_answer(resp.results)\n",
    "\n",
    "answer_speech_2 = texttospeech(response_2)\n",
    "path_to_save = \"../data/speech/audio_answer_2.wav\"\n",
    "\n",
    "#Save the audio to disk in a file called speech.wav\n",
    "sf.write(path_to_save, answer_speech_2.to('cpu').detach().numpy()[0], 22050)\n",
    "\n",
    "IPython.display.Audio(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can stop all running docker container** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $(docker ps -q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is advisable not to run the section below in a Bootcamp session as it takes lot of time to get executed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Model from NGC\n",
    "\n",
    "This section shows you how to make use of available models from the NGC. Rather than building your own custom model, you can pull already built model and the .rmir from the cloud and deploy to Riva. List of these models (ASR, STT, and NLP) exist in the `config.sh` file. The following are the steps to take in deploying the models:\n",
    "\n",
    "- modify the config.sh\n",
    "- modify the riva_start.sh\n",
    "- run the riva_init.sh\n",
    "- run riva_start.sh\n",
    "- start inference\n",
    "\n",
    "### Config.sh\n",
    "Reset the `Config.sh` file to the default settings as follows:\n",
    "\n",
    "```bash\n",
    "# Enable or Disable Riva Services\n",
    "service_enabled_asr=true\n",
    "service_enabled_nlp=true\n",
    "service_enabled_tts=true\n",
    "\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"\n",
    "\n",
    "riva_model_loc=\"riva-model-repo\"\n",
    "\n",
    "use_existing_rmirs=false\n",
    "```\n",
    "You can comment out unneeded models within the script. For demo purpose, we are only going to make use of `rmir_nlp_question_answering_bert_base` while others are put in comment.\n",
    "\n",
    "### Riva_start.sh\n",
    "\n",
    "Reset the script back to the default setting as shown below:\n",
    "\n",
    "```bash\n",
    "...\n",
    "93 docker_run_args=\"-p 8000 -p 8001 -p 8002 $image_speech_api start-riva --riva-uri=0.0.0.0:$riva_speech_api_port --asr_service=$service_enabled_asr -- tts_service=$service_enabled_tts --nlp_service=$service_enabled_nlp $ssl_args &> /dev/null\"\n",
    "94 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run riva_init.sh\n",
    "The script pulls model's `.rmir` file(s) and the prebuilt model artifacts from the NGC. `riva-deploy` is also run behind the scene to deploy the `.rmir` file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $RIVA_DIR && ./riva_init.sh config.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The output should look similar as shown below:**\n",
    "\n",
    "```bash\n",
    "\n",
    "Logging into NGC docker registry if necessary...\n",
    "Pulling required docker images if necessary...\n",
    "Note: This may take some time, depending on the speed of your Internet connection.\n",
    "> Pulling Riva Speech Server images.\n",
    "  > Image nvcr.io/nvidia/riva/riva-speech:2.6.0 exists. Skipping.\n",
    "  > Image nvcr.io/nvidia/riva/riva-speech:2.6.0-servicemaker exists. Skipping.\n",
    "\n",
    "Downloading models (RMIRs) from NGC...\n",
    "\n",
    "---\n",
    "2022-11-01 14:23:14,979 [INFO] processed 330000 lines\n",
    "2022-11-01 14:23:15,043 [INFO] skipped 0 empty lines\n",
    "2022-11-01 14:23:15,043 [INFO] filtered 0 lines\n",
    "2022-11-01 14:23:15,045 [INFO] Extract_binaries for conformer-en-US-asr-offline -> /data/models/conformer-en-US-asr-offline/1\n",
    "2022-11-01 14:23:15,045 [INFO] extracting {'wfst_tokenizer': '/mnt/nvdl/datasets/jarvis_speech_ci/model_files/sp-itn/22.09/en/tokenize_and_classify.far', 'wfst_verbalizer': '/mnt/nvdl/datasets/jarvis_speech_ci/model_files/sp-itn/22.09/en/verbalize.far'} -> /data/models/conformer-en-US-asr-offline/1\n",
    "+ '[' 0 -ne 0 ']'\n",
    "+ [[ amd64 == \\t\\e\\g\\r\\a ]]\n",
    "+ echo\n",
    "\n",
    "+ echo 'Riva initialization complete. Run ./riva_start.sh to launch services.'\n",
    "Riva initialization complete. Run ./riva_start.sh to launch services.\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run riva_start.sh\n",
    "\n",
    "The script starts the riva server.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start.\n",
    "!cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you should see the output below, its implies your model(s) is successfully deployed and Riva server is ready for inference request.\n",
    "\n",
    "```bash\n",
    "Starting Riva Speech Services. This may take several minutes depending on the number of models deployed.\n",
    "d51f487128807235ce4bf258853c7c7387ea9ae019ba1e47a05694673b0b0342\n",
    "Waiting for Riva server to load all models...retrying in 10 seconds\n",
    "Waiting for Riva server to load all models...retrying in 10 seconds\n",
    "Riva server is ready...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker logs riva-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. Let's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "\n",
    "grpc_server =  \"localhost:50051\"\n",
    "channel = grpc.insecure_channel(grpc_server)\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n",
    "req = rnlp.NaturalQueryRequest()\n",
    "\n",
    "context_1 = \"In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought.\"\\\n",
    "                \"The affected region was approximate 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2)\" \\\n",
    "                \" in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part.\" \\\n",
    "                \" The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005\" \\\n",
    "                \"instead 5 gigatons were released and in 2010 8 gigatons were released.\"\n",
    "\n",
    "\n",
    "req.query = \"How many tons of carbon are absorbed the Amazon in a typical year?\"\n",
    "\n",
    "req.context = context_1\n",
    "resp = riva_nlp.NaturalQuery(req)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req.query = \"the affected region by drought in 2010 is approximately?\"\n",
    "req.context = context_1\n",
    "resp = riva_nlp.NaturalQuery(req)\n",
    "print(resp.results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can stop all docker container before shutting down the jupyter kernel. **Caution: The following command will stop all running containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $(docker ps -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/tts/intro.html\n",
    "\n",
    "https://github.com/NVIDIA/NeMo/blob/stable/tutorials/asr/Online_ASR_Microphone_Demo.ipynb\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tts/tts-overview.html\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-overview.html\n",
    "\n",
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright Â© 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"question-answering-training.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "         <a href=\"question-answering-training.ipynb\">6</a>\n",
    "        <a >7</a>\n",
    "        <a href=\"challenge.ipynb\">8</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"challenge.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
