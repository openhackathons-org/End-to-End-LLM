{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80680032",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Overview.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"Overview.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"QandA_data_processing.ipynb\">3</a>\n",
    "        <a href=\"Exercise.ipynb\">4</a>\n",
    "        <a href=\"Summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"QandA_data_processing.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43107c04",
   "metadata": {},
   "source": [
    "# Common Preprocessing Techniques for Raw Text Data \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c61150",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this notebook is to learn some common NLP preprocessing techniques useful for formatting raw text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671de61",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We shall consider 5 preprocessing and cleaning techniques for unstructured text data. Our sample text data would be an example from the Natural Questions (NQ) dataset that we saw in the previous notebook. `document_text` column would be extracted for illustration. Our assumption would be to view the `document_text` as raw text passage sourced from the web and to be used to build a question-answer text file for creating a dataset in `SQUAD` JSON format. \r\n",
    "\r\n",
    "The first step is to fetch the text data and examine it.\r\n",
    "\n",
    "\n",
    "**Fetch Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c3f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "count =1 \n",
    "num_row = 0\n",
    "input_file_path = '../source_code/data/v1.0-simplified_simplified-nq-train.jsonl.gz'\n",
    "\n",
    "text_data= ''\n",
    "with gzip.open(input_file_path, 'rb') as file: \n",
    "    for l in file:\n",
    "        utf8_in = l.decode(\"utf8\", \"strict\")\n",
    "        data_rows = json.loads(utf8_in)               \n",
    "        \n",
    "        text_data = data_rows['document_text']  \n",
    "\n",
    "        print(data_rows)\n",
    "        num_row +=1\n",
    "        if(num_row ==count): break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3612afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38c95d",
   "metadata": {},
   "source": [
    "From the fetched text data, we are only interested in `document_text` as the source for our raw data. As shown in the cell above, the text is noised with `html tags`, `symbols`, `special characters`, and `non-English` characters. Now let’s start with the preprocessing by removing the html tags:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8918eba",
   "metadata": {},
   "source": [
    "### Removal of HTML tags\n",
    "\n",
    "This technique is useful especially when the text data scrapped from websites. The HTML tags are removed using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bc831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_reg_exp = re.compile('<.*?>')\n",
    "text_data1 = html_reg_exp.sub(r'', text_data)\n",
    "\n",
    "text_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7443c4",
   "metadata": {},
   "source": [
    "Now, we have the HTML tags removed. The next step is to remove URLs in the text data.\r\n",
    "\r\n",
    "### Removal of URL\r\n",
    "\r\n",
    "If you look closely into the output text of the previous cell, you can spot a URL like the one given below:\r\n",
    "\n",
    "\n",
    "```bash\n",
    ".......Definitions and Implementation Under the CAN -- SPAM Act ; Final Rule '' ( PDF ) . FTC.gov . May 21 , 2008 .   Retrieved from `` https://en.wikipedia.org/w/index.php?title=Email_marketing&oldid=814071202 '' Categories :   Advertising by medium   Email.......................................................................\n",
    "```\n",
    "What we intend to do is to remove this: `https://en.wikipedia.org/w/index.php?title=Email_marketing&oldid=814071202`, using regular expression. Run the next cell below to remove the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dc460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_reg_exp = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "text_data2 =  url_reg_exp.sub(r'', text_data1)\n",
    "\n",
    "text_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f6588",
   "metadata": {},
   "source": [
    "Now, we have successfully removed the URL. Please kindly note that the URL is not always removed from the text, but the action is determined by the purpose you intend to achieve with the text data. In the case of creating a corpus or QA where none of the questions would require an answer that would contain a URL then, the URL removal technique is justified otherwise, ignore.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7519005",
   "metadata": {},
   "source": [
    "### Removal of Punctuations\n",
    "\n",
    "Looking through our last output text, our next step would be to remove unwanted punctuations but retain commas and full stops.  One way to achieve this purpose is to use the Python function `string.punctuation` and then apply `string.translate` function on `maketrans` function. However, we must be careful because `string.punctuation` contains  ```!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}`~```, hence, we shall use a customized string punctuation function. Also, the use of this technique is subject to text data use case. Let’s run the cell below to remove the unwanted punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c764dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_2_RM = string.punctuation\n",
    "\n",
    "PUNCT_2_RM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774369cc",
   "metadata": {},
   "source": [
    "Exclude `.` and `,` from the punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_PUNCT_2_RM = \"!\\\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4b4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data3= text_data2.translate(str.maketrans('', '', CUSTOM_PUNCT_2_RM))\n",
    "\n",
    "#text_data3 = re.sub(r'[^\\w\\s]', '', text_data2)\n",
    "\n",
    "text_data3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa474733",
   "metadata": {},
   "source": [
    "### Removal of non-English Words\n",
    "\n",
    "In our text data, non-English words like `ಕನ್ನಡ `, `日本 語`, ` فارسی `, and `Русский`, have to be removed because we consider English language characters for our use case.  One of the easiest ways to do that is to use the `nltk word corpus` to filter non-English words from the text data.  The `Spacy` library is another option to achieve the same purpose.  Run the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cb236",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp.add_pipe(\"ner\", source=spacy.load(\"en_core_web_sm\"))\n",
    "text = 'This is an english text.'\n",
    "doc = nlp(text)\n",
    "# document level language detection. Think of it like average language of the document!\n",
    "print(doc._.Language)\n",
    "'''\n",
    "import string\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "text_data4 = \" \".join(w for w in nltk.wordpunct_tokenize(text_data3) if w.lower() in words or not w.isalpha())\n",
    "text_data4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d664d",
   "metadata": {},
   "source": [
    "From the output, we can see that the non-English words have been removed but also left some unknown symbols. These symbols can be tracked by finding their indexes within the text and then filtering them away. However, the indexes sometimes require adding or subtracting 1~5 to get the exact index. There are several other ways to remove non-English words, it all depends on your Python coding knowledge The next three cells below show how to do that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_start_sym = text_data4.find(' াং')\n",
    "index_of_start_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_end_sym =text_data4.find(' ు')\n",
    "index_of_end_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365ffd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#12672\n",
    "index_of_end_sym = index_of_end_sym + 4\n",
    "text_data5 = text_data4[0:index_of_start_sym] + text_data4[index_of_end_sym:]\n",
    "\n",
    "text_data5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48bd8b",
   "metadata": {},
   "source": [
    "For Question Answering purposes, at this junction, further preprocessing may distort the text data in terms of the sentence or paragraph semantics. It is important to have a good and clear understanding of the text content to know whether the figures/ numbers within the text data are useful for `quantification`, `date purposes`, `percentages`, `ratio`, etc., before removing some/all of it. The text contains several nested full stops (.) which can be replaced with single ones. The text data could also be checked for correct spelling to aid the correctness of the sentences and paragraphs within the text data.\r\n",
    "\r\n",
    "\r\n",
    "Running the cell below will remove nested full stops `...` by replacing them with nothing:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79040a9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data6 = text_data5.replace('...', '')\n",
    "text_data6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f88131",
   "metadata": {},
   "source": [
    "Running the cell below will replace three spaced full stops ` . . . `  within the text with just one `.`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff3f51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data7 = text_data6.replace('. . .', '.')\n",
    "text_data7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a5b35",
   "metadata": {},
   "source": [
    "The text data is looking better as compared to what it was when we got started. Because text preprocessing is subjective to the text data containment and its’ use cases, therefore, we would like to briefly touch other text preprocessing techniques which are important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57964f1",
   "metadata": {},
   "source": [
    "## Other Text Preprocessing Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58804ddd",
   "metadata": {},
   "source": [
    "### Word Correction\n",
    "\n",
    "Checking for spelling correctness is an important task in text preprocessing before proceeding to build a dataset to train a model or for data analysis. Words that are not correctly spelled or as a result of typos should be replaced with correct ones. Two of the ways to achieve that is using the `Jaccard distance method` from the `NLTK python` library or the `pyspellchecker` library. However, the spelling correction is not perfect. For example, you may intend to write the word `contain` and because of typos the word is written as `contan`. There is a likelihood that the word is corrected as `constant` rather than `contain`.\r\n",
    "\r\n",
    "\r\n",
    "Please, run the next two cells below:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit to: https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/ where this code was adapted\n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "  \n",
    "  \n",
    "word_list = words.words()\n",
    "\n",
    "typo_words = 'some of the sentnce cntain some errores'.split()\n",
    "for word in typo_words:\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)), set(ngrams(w, 2))),w) for w in word_list if w[0]==word[0]]\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65daac",
   "metadata": {},
   "source": [
    "**Apply spell checker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca1f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "text = \"some of the sentnce cntain some errores\"\n",
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c569e",
   "metadata": {},
   "source": [
    "### Removal of Emojis\n",
    "\n",
    "When text data is scrapped from an online social media, there is a tendency that it may contain emojis that are not needed during data analysis or training, hence the need to remove the emojis.  For more information on how to remove `symbols & pictures`, `transport & map symbols`, `flags (iOS)`, and `Chinese char` visit [slowkow/remove-emoji.py GitHub](https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfff482",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "emojis1 = ' 🔥🔥😀 😃 😄 😁 😆 '\n",
    "emojis2 = ' 😅 😂 👍👀😍😱🤪🥂'\n",
    "emoji_len = len(emojis1)\n",
    "\n",
    "text_data8 = text_data7[:1000]+ emojis1 + text_data7[1001+emoji_len:]+ emojis2 \n",
    "text_data8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96a572",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#source: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "text_data9 = emoji_pattern.sub(r'', text_data8)\n",
    "\n",
    "text_data9 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec90ddb",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of breaking up a text into sentences or words. Each word or sentence in a text is considered as a token. Tokenization allows a detailed analysis of text data when it is broken into smaller units. Examples given below include sentence tokenization and world tokenization.\n",
    "\n",
    "**Sentence tokenization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5eba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = text_data9 \n",
    "\n",
    "text_into_sentences = nltk.sent_tokenize(text.lower())\n",
    "print (\"Total sentence: \",len(text_into_sentences))\n",
    "text_into_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf08cd",
   "metadata": {},
   "source": [
    "Let’s take the first sentence and then tokenize it into words. You can as well tokenize the entire text into words.\n",
    "\n",
    "**Word tokenization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_into_words = nltk.word_tokenize(text_into_sentences[0].lower())\n",
    "print (\"Total words: \",len(sentence_into_words))\n",
    "sentence_into_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dba8d8",
   "metadata": {},
   "source": [
    "### Removal of stopwords\n",
    "\n",
    "Stopwords are a collection of words that commonly occur in any language but are of no importance to text analysis except for part of speech (POS) tagging in sentences. They are usually determinants, pronounces, some verbs, and adverbs like `a`, `his`, `herself`, `the`, `in`,` out`, etc.  Stopwords do not only exist in the English language but also other languages. Examples are given in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3df93b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#no_stopword_text = \" \".join([word for word in text.split() if str(word).strip() not in stopwords.words('english')])\n",
    "no_stopword_text = \" \".join([word for word in str(text_into_sentences[0]).split() if word not in set(stopwords.words('english'))])\n",
    "print('original text: {}'.format(text_into_sentences[0]))\n",
    "print('stopword removed: {}'.format(no_stopword_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417bb4cd",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words i.e., reducing words to their root form especially words in past and continuous form e.g., `dancing` is reduced to its root `dance`. The most widely used stemming algorithm is the Porter Stemmer within the `nltk` library. Oftentimes some words are wrongly stemmed for example `navigation` may be reduced to `navig`. Run the cells below to see the effect of stemming on our text `(no_stopword_text)`.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stem_words =''\n",
    "for word in no_stopword_text.split():\n",
    "   stem_words += stemmer.stem(word)+' '\n",
    "\n",
    "print('original text: {}'.format(no_stopword_text))\n",
    "print('stemmed text: {}'.format(stem_words))\n",
    "# \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3736ca",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Due to the limitations of Stemming, Lemmatization is used to recover words that are not properly stemmed. Lemmatization uses POS (part of speech) to understand the context we want to lemmatize our word. By default, the POS is set to Noun.  You can check out [sudalairaj kumar](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing) on [Kaggle](https://www.kaggle.com/) for more in-depth. Now, you can run the cell below to see how our previous text is lemmatized without chopping off letters from words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from: https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "pos_tagged_text = nltk.pos_tag(no_stopword_text.split())\n",
    "lemma_words = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "print('original text: {}'.format(no_stopword_text))\n",
    "print('stemmed text: {}'.format(lemma_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7b0f4",
   "metadata": {},
   "source": [
    "We have learned how to apply different NLP preprocessing techniques to preprocess text data. The next phase would be to build a SQuAD format JSON file from the processed text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22295d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/\n",
    "- https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "- https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cdfc7",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Overview.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"Overview.ipynb\">1</a>\n",
    "        <a >2</a>\n",
    "        <a href=\"QandA_data_processing.ipynb\">3</a>\n",
    "        <a href=\"Exercise.ipynb\">4</a>\n",
    "        <a href=\"Summary.ipynb\">5</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"QandA_data_processing.ipynb\">Next Notebook</a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../Start_Here.ipynb\">Home Page</a> </center> </p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
