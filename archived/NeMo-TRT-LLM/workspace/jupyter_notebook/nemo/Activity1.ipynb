{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd7bfe7",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Question_Answering.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "       \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc731a89",
   "metadata": {},
   "source": [
    "# Lab Activity 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34e342",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "This lab activity is vital to test your understanding and assist you in perfecting what you have learned in the previous notebook. \n",
    "You are to reproduce the training and inferencing process for Question-answering using the already-used QuAC dataset. To complete the lab activity, you are to implement the following steps:\n",
    "\n",
    "- Understand the structure of the QuAC dataset and process the `quac_v0.2.json` file into a training and validation set. You can decide the split ratio and preprocess the dataset into SQuAD JSON format.\n",
    "- Import important libraries and use the Bert model `BERTQAModel` for training and inferencing\n",
    "- Use `omegaconf` to:\n",
    "    - set dataset config values\n",
    "    - set trainer config values \n",
    "    - set experiment manager config values\n",
    "- Train and Test your Model\n",
    "- Run inference using the custom dataset\n",
    "\n",
    "\n",
    "Part of the solution code is written for you. You are to complete the rest by filling in the statements with the missing value(s) in the commented areas of the notebook. We recommend consciously setting/modifying the `config.trainer.max_epochs` value as it determines the time to complete this lab. The lab activity should not exceed `45 mins`; therefore, the value of `config.trainer.max_epochs` should be between `1 and 6` as one epoch may take up to `7 mins`. If you plan to exceed these values, run this notebook after the Bootcamp active hours.\n",
    "\n",
    "Note: *You are not expected to get the best result as this activity is for learning. To achieve better results, you can modify parameters: the number of epochs, learning rate, batch size, max step, training, and validation set sample size*.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d943572",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left; color:#FF0000; height:40px; text-color:red; font-size:20px\">Before you run this notebook, please close and shut down the kernel of the previous notebooks. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad48940",
   "metadata": {},
   "source": [
    "## QuAC Dataset\n",
    "\n",
    "Question Answering in Context is a dataset for modeling, understanding, and participating in information-seeking dialog. QuAC contains 98,407 QA pairs from 13,594 discussions. The dialogues were conducted on 8,854 unique sections from 3,611 unique Wikipedia articles, and every talk had between four and twelve questions. More information can be found on [Hugging Face dataset page](https://huggingface.co/datasets/quac).\n",
    "The Data instances consist of an interactive dialog between two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts (spans) from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context. You can read more from [QuAC's official website](https://quac.ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6e819",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "\n",
    "- **dialogue_id**: ID of the dialogue.\n",
    "- **wikipedia_page_title**: title of the Wikipedia page.\n",
    "- **background**: the first paragraph of the main Wikipedia article.\n",
    "- **section_tile**: Wikipedia section title.\n",
    "- **context**: Wikipedia section text.\n",
    "- **turn_ids**: list of identification of dialogue turnsâ€”one list of IDs per dialogue.\n",
    "- **question**: list of questions in the dialogue. One list of questions per dialogue.\n",
    "- **followups**: list of followup actions in the dialogue. One list of followups per dialogue. y: follow, m: maybe follow yp, n: don't follow up.\n",
    "- **yesno**: list of yes/no in the dialogue. One list of yes/no per dialogue. y: yes, n: no, x: neither.\n",
    "- **answers**: dictionary of responses to the questions (validation step of data collection)\n",
    "- **answer_starts**: list of list of starting offsets. For training, a list of single-element lists (one answer per question).\n",
    "- **texts**: list of list of span texts answering questions. For training, a list of single-element lists (one answer per question).\n",
    "- **orig_answers**: a dictionary of original answers (the ones provided by the teacher in the dialogue)\n",
    "- **answer_starts**: list of starting offsets\n",
    "- **texts**: list of span texts answering questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e08b4",
   "metadata": {},
   "source": [
    "## File Formats\n",
    "\n",
    "The QuAC dataset is in JSON format. A validation example is given below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c42149",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```json\n",
    "{'paragraphs': [{'context': 'Walton was born in La Mesa, California, the son of Gloria Anne (nee Hickey) and William Theodore \"Ted\" Walton. His listed adult playing height was 6 feet 11 inches; it has been reported that Walton is actually taller (7 feet 2 inches, or more) but does not like being categorized as a seven-footer. He played high school basketball at Helix High School. At age 17, Walton played for the United States men\\'s national basketball team at the 1970 FIBA World Championship. He played college basketball for John Wooden at the University of California, Los Angeles (UCLA) from 1971 to 1974, winning the national title in 1972 over Florida State and again in 1973 with an 87-66 win over Memphis State in which Walton made 21 of 22 field goal attempts and scored 44 points, representing more than half his team\\'s total. The Walton-led 1971-72 UCLA basketball team had a record of 30-0, in the process winning its games by an average margin of more than 30 points. He was the backbone of two consecutive 30-0 seasons and was also part of UCLA\\'s NCAA men\\'s basketball record 88-game winning streak. The UCLA streak contributed to a personal winning streak that lasted almost five years, in which Walton\\'s high school, UCLA freshman (freshmen were ineligible for the varsity at that time) and UCLA varsity teams did not lose a game from the middle of his junior year of high school to the middle of his senior year in college. Walton was the 1973 recipient of the James E. Sullivan Award as the top amateur athlete in the United States. Walton also received the USBWA College Player of the Year and Naismith College Player of the Year as the top college basketball player in the country three years in a row while attending UCLA, at the same time earning Academic All-American honors three times. Some college basketball historians rate Walton as the greatest who ever played the game at the college level. In Walton\\'s senior year during the 1973-74 season, the school\\'s 88-game winning streak ended with a 71-70 loss to Notre Dame. During the same season, UCLA\\'s record seven consecutive national titles was broken when North Carolina State defeated the Bruins 80-77 in double overtime in the NCAA semi-finals. With Walton\\'s graduation in 1974 and Bruin coach John Wooden\\'s retirement after UCLA\\'s 1975 national title, the UCLA dynasty came to an end. Prior to joining the varsity team, Walton (18.1, 68.6 percent), along with Greg Lee (17.9 ppg) and Keith Wilkes (20.0 ppg), was a member of the 20-0 UCLA Freshman team. CANNOTANSWER',\n",
    "   'qas': [{'followup': 'y',\n",
    "     'yesno': 'x',\n",
    "     'question': 'Where was he born?',\n",
    "     'answers': [{'text': 'Walton was born in La Mesa, California,',\n",
    "       'answer_start': 0},\n",
    "      {'text': 'La Mesa, California,', 'answer_start': 19},\n",
    "      {'text': 'Walton was born in La Mesa, California,', 'answer_start': 0},\n",
    "      {'text': 'La Mesa, California,', 'answer_start': 19},\n",
    "      {'text': 'Walton was born in La Mesa, California,', 'answer_start': 0}],\n",
    "     'id': 'C_ece8b3ecad8a47d5a1380955ce47184a_1_q#0',\n",
    "     'orig_answer': {'text': 'Walton was born in La Mesa, California,',\n",
    "      'answer_start': 0}},\n",
    "    {'followup': 'y',\n",
    "     'yesno': 'y',\n",
    "     'question': 'Did he play college basketball?',\n",
    "     'answers': [{'text': 'Championship. He played college basketball for John Wooden at the University of California, Los Angeles (',\n",
    "       'answer_start': 455},\n",
    "      {'text': 'He played high school basketball', 'answer_start': 299},\n",
    "      {'text': 'He played college basketball for John Wooden at the University of California, Los Angeles (UCLA) from 1971 to 1974,',\n",
    "       'answer_start': 469},\n",
    "      {'text': 'He played college basketball for John Wooden at the University of California,',\n",
    "       'answer_start': 469},\n",
    "      {'text': 'He played college basketball for John Wooden at the University of California, Los Angeles (UCLA)',\n",
    "       'answer_start': 469}],\n",
    "     'id': 'C_ece8b3ecad8a47d5a1380955ce47184a_1_q#1',\n",
    "     'orig_answer': {'text': 'He played college basketball for John Wooden at the University of California, Los Angeles (UCLA)',\n",
    "      'answer_start': 469}},\n",
    "\n",
    "     ...\n",
    "    \n",
    "    {'followup': 'y',\n",
    "     'yesno': 'y',\n",
    "     'question': 'Did he have any notable games?',\n",
    "     'answers': [{'text': 'The Walton-led 1971-72 UCLA basketball team had a record of 30-0, in the process winning its games by an average margin of more than 30 points.',\n",
    "       'answer_start': 812},\n",
    "      {'text': \"UCLA's record seven consecutive national titles was broken when North Carolina State defeated the Bruins 80-77 in double overtime in the NCAA semi-finals.\",\n",
    "       'answer_start': 2046},\n",
    "      {'text': 'winning the national title in 1972 over Florida State and again in 1973 with an 87-66 win over Memphis State',\n",
    "       'answer_start': 585},\n",
    "      {'text': \"In Walton's senior year during the 1973-74 season, the school's 88-game winning streak ended with a 71-70 loss to Notre Dame.\",\n",
    "       'answer_start': 1896},\n",
    "      {'text': 'The Walton-led 1971-72 UCLA basketball team had a record of 30-0, in the process winning its games by an average margin of more than 30 points.',\n",
    "       'answer_start': 812}],\n",
    "     'id': 'C_ece8b3ecad8a47d5a1380955ce47184a_1_q#5',\n",
    "     'orig_answer': {'text': 'The Walton-led 1971-72 UCLA basketball team had a record of 30-0, in the process winning its games by an average margin of more than 30 points.',\n",
    "      'answer_start': 812}}],\n",
    "   'id': 'C_ece8b3ecad8a47d5a1380955ce47184a_1'}],\n",
    " 'section_title': 'Early life and college career',\n",
    " 'background': \"William Theodore Walton III (born November 5, 1952) is an American retired basketball player and television sportscaster. Walton became known playing for John Wooden's powerhouse UCLA Bruins in the early 1970s, winning three successive College Player of the Year Awards, while leading the Bruins to two Division I national titles. He then went on to have a prominent career in the National Basketball Association (NBA) where he was a league Most Valuable Player (MVP) and won two NBA championships.\",\n",
    " 'title': 'Bill Walton'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec96e5a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### 1. Download a sample QuAC dataset\n",
    "To get started, you need to download the QuAC dataset by running the `dataset_quac.py` script in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d64a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/dataset_quac.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ed6d3f",
   "metadata": {},
   "source": [
    "- Set the path to the following directories:\n",
    "    - DATA_DIR: the path to the datasets\n",
    "    - WORK_DIR:  path to the directory for storing trained models and logs\n",
    "    - script_dir: path the source codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0126335",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/workspace/data/activity1\"\n",
    "WORK_DIR = \"/workspace/results/activity1/nemo_question_answering\"\n",
    "script_dir = \"/workspace/source_code/activity1/nemo_question_answering\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f86c59",
   "metadata": {},
   "source": [
    "### 2. Dataset Splitting: Training and Testing Samples\n",
    "\n",
    "- The QuAC dataset in JSON format is provided as a single file containing 1000 examples.\n",
    "- You must split the dataset into separate training and validation sets to facilitate model training and evaluation.\n",
    "- Before splitting the dataset, define the desired ratio between the training and validation sets. In this example, we set train_ratio = 0.8, corresponding to an 80% train and 20% validation split. The split_dataset function is called with the data and train_ratio as arguments to split the dataset accordingly. The train data is saved as `quac_2_squad_train.json`, and the validation data as `quac_2_squad_val.json`. These files are stored in the `/workspace/data/activity1/quac/` directory.\n",
    "- Use the code snippet below, which demonstrates the dataset coversion into SQuAD format, and then execute the splitting process.\n",
    "\n",
    "Before running the code snippet for dataset conversion and splitting, please ensure that the `quac_v0.2.json` dataset file exists in your project directory by running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92585acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -LR {DATA_DIR}/quac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8666d5",
   "metadata": {},
   "source": [
    "Code snippet for converting QuAC to SQuAD format. You can rewrite the code block or use it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import json\n",
    "\n",
    "class MakeDataset():\n",
    "    def __init__(self, data_row):\n",
    "        self.data_row = data_row\n",
    "        self.final_json = {}\n",
    "        self.final_json['version'] = \"v2.0\"\n",
    "        self.final_json['data'] = []\n",
    "\n",
    "    def reader(self):\n",
    "        verbose = 1\n",
    "        train_file_path = f\"{DATA_DIR}/quac/quac_val_v0.2.json\"\n",
    "        if verbose:\n",
    "            print(\"Reading the json file\")    \n",
    "        file_train = json.loads(open(train_file_path).read())\n",
    "        if verbose:\n",
    "            print(\"processing...\")\n",
    "        self.data_row = [topic for topic in file_train['data']]\n",
    "        \n",
    "              \n",
    "    \n",
    "    # make json format    \n",
    "    def make_json(self):\n",
    "        for i in range(len(self.data_row)):\n",
    "            self.brace_in_data ={}\n",
    "            self.brace_in_data['title'] = self.data_row[i]['title']\n",
    "            self.brace_in_data['paragraphs'] = []\n",
    "            paragraphs = self.data_row[i]['paragraphs']\n",
    "            for j in range(len(paragraphs)):\n",
    "                brace_in_paragaraphs = {}\n",
    "                brace_in_paragaraphs['context'] = paragraphs[j]['context']\n",
    "                qas = paragraphs[j]['qas']\n",
    "                brace_in_paragaraphs['qas'] = []    \n",
    "                for k in range(len(qas)):\n",
    "                    brace_in_qas = {}\n",
    "                    brace_in_qas['question'] = qas[k]['question']\n",
    "                    brace_in_qas['id'] = qas[k]['id']\n",
    "                    answer = qas[k]['answers']\n",
    "                    if len(answer) == 0:\n",
    "                        brace_in_qas['answers'] = [] \n",
    "                        brace_in_qas['is_impossible'] = True\n",
    "                    else:\n",
    "                        brace_in_qas['answers'] =[{'text':answer[0]['text'], 'answer_start':answer[0]['answer_start']}]\n",
    "                        brace_in_qas['is_impossible'] = False\n",
    "\n",
    "                    brace_in_paragaraphs['qas'].append(brace_in_qas)\n",
    "                self.brace_in_data['paragraphs'].append(brace_in_paragaraphs)\n",
    "            self.final_json['data'].append(self.brace_in_data)        \n",
    "    \n",
    "    #save the json file               \n",
    "    def save_json(self, filename):\n",
    "        with open(f\"{DATA_DIR}/quac/{filename}.json\", \"w\") as write_file:\n",
    "            json.dump(self.final_json, write_file, indent=4)\n",
    "            print(\"{} saved in SQauD json format ....\".format(filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf11f1",
   "metadata": {},
   "source": [
    "Run the cell below to set the directory path to store the dataset and the output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the quac dataset\n",
    "verbose = 1\n",
    "train_file_path = f\"{DATA_DIR}/quac/quac_v0.2.json\"\n",
    "if verbose:\n",
    "    print(\"Reading the json file\")    \n",
    "file_train = json.loads(open(train_file_path).read())\n",
    "if verbose:\n",
    "    print(\"processing...\")\n",
    "data_row = [topic for topic in file_train['data']]\n",
    "\n",
    "##############################################################################################################################################\n",
    "##### split the dataset into 800 examples for train set and 200 for validation set. You can modify the ratio based on your preferences #######\n",
    "train = data_row[:800]\n",
    "val = data_row[800:]\n",
    "##############################################################################################################################################\n",
    "\n",
    "#create objects of MakeDataset class and pass split set\n",
    "train_Obj = MakeDataset(train)\n",
    "val_Obj = MakeDataset(val)\n",
    "\n",
    "# call the reader function: Obj.reader()\n",
    "train_Obj.make_json()\n",
    "val_Obj.make_json()\n",
    "\n",
    "# set the filenames \n",
    "train_filename = 'quac_2_squad_train'\n",
    "val_filename = 'quac_2_squad_val'\n",
    "\n",
    "\n",
    "# generate the SQuAD json format\n",
    "train_Obj.save_json(train_filename)\n",
    "val_Obj.save_json(val_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c773376",
   "metadata": {},
   "source": [
    "Run the cell below to view an example from the training set in SQuAD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{DATA_DIR}/quac/quac_2_squad_train.json\"\n",
    "\n",
    "train = json.loads(open(path).read())\n",
    "rows = [topic for topic in train['data']]\n",
    "\n",
    "rows[799]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38084ead",
   "metadata": {},
   "source": [
    "Import important libraries `pytorch_lightning,` `OmegaConf,` `BERTQAModel,` and `exp_manager.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b2bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "################## Fill the space with the required libraries. ##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "pl.seed_everything(42)\n",
    "gc.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec2b4e",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Defined the Model in the config file. The config file has multiple important sections that include:\n",
    "\n",
    "- **Model**: All arguments that will relate to the Model - language model, span prediction, optimizer and schedulers, datasets, and any other related information\n",
    "- **trainer**: the training object argument to be passed to PyTorch Lightning\n",
    "- **exp_manager**: All arguments used for setting up the experiment manager - target directory, name, logger information\n",
    "\n",
    "We will set the path to the default config file `qa_conf.yaml` and edit the necessary values for training different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = '/workspace/source_code/activity1/nemo_question_answering/conf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d25da",
   "metadata": {},
   "source": [
    "Run the cell below to print the entire default config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = f'{config_dir}/qa_conf.yaml'\n",
    "print(config_path)\n",
    "\n",
    "\n",
    "################# load the OmegaConf ################################\n",
    "\n",
    "config = \n",
    "\n",
    "####################################################################\n",
    "print(\"Default Config - \\n\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf50c3",
   "metadata": {},
   "source": [
    "### Set dataset config values\n",
    "\n",
    "Set essential parameters like the path to the train, validation, and text sets; batch size; and the number of training, validation, and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True, model will load features from cache if file is present, or\n",
    "# create features and dump to cache file if not already present\n",
    "config.model.dataset.use_cache = False\n",
    "\n",
    "# indicates whether the dataset has unanswerable questions\n",
    "config.model.dataset.version_2_with_negative = True\n",
    "\n",
    "# indicates whether the dataset is of extractive nature or not\n",
    "# if True, context spans/chunks that do not contain answer are treated as unanswerable \n",
    "config.model.dataset.check_if_answer_in_context = True\n",
    "\n",
    "# set file paths for train, validation, and test datasets\n",
    "config.model.train_ds.file = f\"{DATA_DIR}/quac/quac_2_squad_train.json\"\n",
    "config.model.validation_ds.file = f\"{DATA_DIR}/quac/quac_2_squad_val.json\"\n",
    "config.model.test_ds.file = f\"{DATA_DIR}/quac/quac_2_squad_val.json\"\n",
    "\n",
    "# set batch sizes for train, validation, and test datasets\n",
    "config.model.train_ds.batch_size = 8\n",
    "config.model.validation_ds.batch_size = 8\n",
    "config.model.test_ds.batch_size = 8\n",
    "\n",
    "# set number of samples to be used from dataset. setting to -1 uses entire dataset\n",
    "config.model.train_ds.num_samples = 5000\n",
    "config.model.validation_ds.num_samples = 1000\n",
    "config.model.test_ds.num_samples = 100  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596fad5",
   "metadata": {},
   "source": [
    "### Set trainer config values\n",
    "\n",
    "These values include the maximum number of epochs, max steps, device, accelerator, and trainer strategy. (*Depending on the device, one epoch may take up to 7 minutes.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Set the maximum epoch ########################\n",
    "\n",
    "config.trainer.max_epochs = \n",
    "#############################################################\n",
    "\n",
    "config.trainer.max_steps = -1 # takes precedence over max_epochs\n",
    "config.trainer.precision = 16\n",
    "config.trainer.devices = [0] # 0 for CPU, or list of the GPUs to use [0] this tutorial does not support multiple GPUs. If needed please use NeMo/examples/nlp/question_answering/question_answering.py\n",
    "config.trainer.accelerator = \"gpu\"\n",
    "config.trainer.strategy=\"dp\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11058b3",
   "metadata": {},
   "source": [
    "### Set experiment manager config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04859372",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.exp_manager.exp_dir = WORK_DIR\n",
    "config.exp_manager.name = \"QA-SQuAD2\"\n",
    "config.exp_manager.create_wandb_logger=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a544e821",
   "metadata": {},
   "source": [
    "## Training and Testing Models\n",
    "\n",
    "\n",
    "### BERT Model\n",
    "\n",
    "- Set Model Config Values\n",
    "    - `bert-base-uncased` is set as the pretrained model and also as the tokenizer name.\n",
    "    - Set the model optimizer learning rate `config.model.optim.lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029797c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Set the language model and tokenizer name. #####################\n",
    "\n",
    "config.model.language_model\n",
    "config.model.tokenizer\n",
    "\n",
    "################################################################################\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bert_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "\n",
    "#################### set the learning rate #####################################\n",
    "\n",
    "config.model\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415e30b",
   "metadata": {},
   "source": [
    "- Create Trainer and Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44e0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############  Create the trainer and Bert model object  ##################################################\n",
    "\n",
    "trainer = pl.Trainer(\"fill in here\")\n",
    "model =\n",
    "\n",
    "##########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268aeeb",
   "metadata": {},
   "source": [
    "- Train, Test, and Save the Model *(Depending on the device, one epoch may take up to 7 minutes.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52772ba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################# Fit, test, and save the model  #######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42743a",
   "metadata": {},
   "source": [
    "#### Load the Saved Model and Run Inference\n",
    "\n",
    "While running the Inference, it is possible to see that not all responses matched the expected output. It is because of the number of epochs used to avoid long training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b59288",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bmodel = BERTQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "Bmodel.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = False\n",
    "exp_dir = exp_manager(Bmodel.trainer, config.exp_manager)\n",
    "output_nbest_file = os.path.join(exp_dir, \"output_nbest_file.json\")\n",
    "output_prediction_file = os.path.join(exp_dir, \"output_prediction_file.json\")\n",
    "\n",
    "all_preds, all_nbest = Bmodel.inference(\n",
    "    config.model.test_ds.file,\n",
    "    output_prediction_file=output_prediction_file,\n",
    "    output_nbest_file=output_nbest_file,\n",
    "    num_samples=20, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44407cba",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```python\n",
    "...\n",
    "CANNOTANSWER\n",
    "CANNOTANSWER\n",
    "CANNOTANSWER\n",
    "\n",
    "CANNOTANSWER\n",
    "Hogan demanded a match against Slaughter at WrestleMania VII in Los Angeles, California, and Slaughter accepted. Slaughter was defeated by Hogan, thus losing his championship.\n",
    "\n",
    "\n",
    "August 4, 1997\n",
    "Initially popular, he eventually became the target of D-Generation X, who called him \"Sgt. Slobber\".\n",
    "After a hiatus, he returned to WWF television on the August 4, 1997\n",
    "On an episode of Raw Is War, he put Shawn Michaels and Triple H in a match for Michaels' European Championship.\n",
    "he handcuffed himself to Chyna to prevent her from interfering with Helmsley's match against Owen Hart.\n",
    "CANNOTANSWER\n",
    "Initially popular, he eventually became the target of D-Generation X, who called him \"Sgt. Slobber\".\n",
    "CANNOTANSWER\n",
    "the Doors performed a now infamous concert at New Haven Arena in New Haven, Connecticut,\n",
    "Morrison was arrested by local police.\n",
    "The concert was abruptly ended when Morrison was dragged offstage by the police.\n",
    "Morrison became the first rock artist ever to be arrested onstage during a concert performance.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07935b31",
   "metadata": {},
   "source": [
    "## Inference on Custom Sample Dataset in SQuAD Format\n",
    "\n",
    "Create new test data for inferencing. The essence is to measure how well our trained model behaves in the presence of unseen data. The test data consists of two contexts with questions only. The answers are deducible from the context. It is expected that both the BERT model should be able to answer at least 80% of the questions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceda23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Sample dataset content\n",
    "dataset = {\n",
    "  \"version\": \"1.0\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"title\": \"This is a sample custom dataset\",\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"context\": \"In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the 2005 drought. \\\n",
    "          The affected region was approximately 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles (1,900,000 km2) \\\n",
    "          in 2005. The 2010 drought had three epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern part. \\\n",
    "          The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 gigatons of carbon dioxide; during 2005 instead \\\n",
    "          5 gigatons were released and in 2010 8 gigatons were released.\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"question\": \"How many gigatons of carbon are absorbed by the Amazon in a typical year?\",\n",
    "              \"id\": \"q1\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"What was the affected region by the drought in 2010 approximately?\",\n",
    "              \"id\": \"q2\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"What were the findings regarding the droughts published in?\",\n",
    "              \"id\": \"q3\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"How many gigatons of carbon were released during the 2005 drought?\",\n",
    "              \"id\": \"q4\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"How did the 2010 drought differ from the 2005 drought in terms of epicenters?\",\n",
    "              \"id\": \"q5\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "          {\n",
    "          \"context\": \"The sun is a massive ball of hot, glowing gases at the center of our solar system. It provides light, heat, and energy that sustains \\\n",
    "          life on Earth. The sun's surface temperature is around 5,500 degrees Celsius (9,932 degrees Fahrenheit), while its core temperature reaches about \\\n",
    "          15 million degrees Celsius (27 million degrees Fahrenheit). The sun's energy is generated through a process called nuclear fusion, where hydrogen \\\n",
    "          atoms combine to form helium, releasing immense amounts of energy in the process.\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"question\": \"What is the approximate surface temperature of the sun?\",\n",
    "              \"id\": \"q6\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"How does the sun generate its energy?\",\n",
    "              \"id\": \"q7\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"What is the core temperature of the sun?\",\n",
    "              \"id\": \"q8\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"What process is responsible for the sun's energy generation, where hydrogen atoms combine to form helium?\",\n",
    "              \"id\": \"q9\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# Save the dataset as a JSON file\n",
    "output_file = f\"{DATA_DIR}/quac/sample_dataset.json\"\n",
    "with open(output_file, \"w\") as json_file:\n",
    "    json.dump(dataset, json_file, indent=4)\n",
    "\n",
    "print(f\"Dataset saved as '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8faaca",
   "metadata": {},
   "source": [
    "#### Modify the Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.test_ds.file = f\"{DATA_DIR}/quac/sample_dataset.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ceaf0d",
   "metadata": {},
   "source": [
    "#### Run Inference with BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Load the saved model and run inference using the sample_dataset.json #####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ffdaf",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```python\n",
    "...\n",
    "1.5 gigatons of carbon dioxide;\n",
    "The affected region was approximately 1,160,000 square miles (3,000,000 km2) of rainforest,\n",
    "The findings were published in the journal Science.\n",
    "1.5 gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were released.\n",
    "The 2010 drought had three epicenters where vegetation died off,\n",
    "The sun's surface temperature is around 5,500 degrees Celsius (9,932 degrees Fahrenheit),\n",
    "The sun's energy is generated through a process called nuclear fusion,\n",
    "The sun's surface temperature is around 5,500 degrees Celsius (9,932 degrees Fahrenheit),\n",
    "nuclear fusion, where hydrogen atoms combine to form helium,\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae25a37",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Personal Json Test data\n",
    "\n",
    "Using the cell below, create a json test set with a single `context` and three `questions`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c58e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "import json\n",
    "# Sample dataset content\n",
    "test_set = {\n",
    "  \"version\": \"1.0\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"title\": \"This is a sample custom dataset\",\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"context\": \" \",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"question\": \"?\",\n",
    "              \"id\": \"q1\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"?\",\n",
    "              \"id\": \"q2\"\n",
    "            },\n",
    "            {\n",
    "              \"question\": \"?\",\n",
    "              \"id\": \"q3\"\n",
    "            },\n",
    "          ]\n",
    "        },\n",
    "          \n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# Save the test as a JSON file\n",
    "test_file = f\"{DATA_DIR}/quac/sample_test_set.json\"\n",
    "with open(test_file, \"w\") as json_file:\n",
    "    json.dump(test_set, json_file, indent=4)\n",
    "\n",
    "print(f\"Dataset saved as '{test_file}'\")\n",
    "\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e6e44",
   "metadata": {},
   "source": [
    "#### Run Inference Using Your Created Personal Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Write the code to run the inference ############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e68ee",
   "metadata": {},
   "source": [
    "---\n",
    "After the Bootcamp session, you can check for the solution prototype [here](Solution_Activity1.ipynb). Please shut down the notebook kernel and click on the link below to proceed to the next lab.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red; height:80px;\"> <b><br/>[Prompt Tuning/P-Tuning](Multitask_Prompt_and_PTuning.ipynb)</b> </div> </center>\n",
    "---\n",
    "\n",
    "### Resources\n",
    "Below are resourceful links to guide you and assist you in learning more.\n",
    "- [NeMo Models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/core.html)\n",
    "- [Core APIs](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/api.html)\n",
    "- [Experiment Manager](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/exp_manager.html)\n",
    "- [Exporting NeMo Models](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/export.html)\n",
    "- [Prompt Learning](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/prompt_learning.html)\n",
    "- [NeMo Megatron API](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfadd7",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "Copyright Â© 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
