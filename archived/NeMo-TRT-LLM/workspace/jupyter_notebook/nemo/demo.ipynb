{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "822297fe",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Multitask_Prompt_and_PTuning.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"NeMo_Primer.ipynb\">1</a>\n",
    "        <a href=\"Question_Answering.ipynb\">2</a>\n",
    "        <a href=\"Multitask_Prompt_and_PTuning.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"\"></a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435da0e",
   "metadata": {},
   "source": [
    "## NeMo Megatron-GPT 1.3B: Language Model Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2238b0d",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "NVIDIA NeMo™ framework is an end-to-end, cloud-native enterprise framework to build, customize, and deploy generative AI models with billions of parameters. The NeMo framework supports the development of text-to-text, text-to-image, and image-to-image foundation models.\n",
    "NeMo framework makes it possible for researchers to easily compose complex neural network architectures for conversational AI using reusable components - Neural Modules. Neural Modules are conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations.\n",
    "\n",
    "<img src=\"images/NeMo_Framework.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53631a",
   "metadata": {},
   "source": [
    "### State-of-the-Art Training Techniques\n",
    "\n",
    "The NeMo framework delivers high levels of training efficiency, making training of large-scale foundation models possible, using 3D parallelism techniques such as:\n",
    "- Tensor parallelism to scale models within nodes\n",
    "- Data and pipeline parallelism to scale data and models across thousands of GPUs \n",
    "- Sequence parallelism to distribute activation memory across tensor parallel devices\n",
    "- In addition, selective activation recomputing optimizes recomputation and memory usage across tensor parallel devices during backpropagation.\n",
    "\n",
    "<img src=\"images/NeMo_technique.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846583e4",
   "metadata": {},
   "source": [
    "### Megatron\n",
    "\n",
    "Megatron is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. NeMo Megatron supports several types of models:\n",
    "- GPT-style models (decoder only)\n",
    "\n",
    "- T5/BART/UL2-style models (encoder-decoder)\n",
    "\n",
    "- BERT-style models (encoder only)\n",
    "\n",
    "- RETRO model (decoder only)\n",
    "\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer) Architecture\n",
    "<table>\n",
    "    <th></th><th></th>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/gpt_arc_original.png\"></td><td><img src=\"images/gpt_arc_openai.jpg\"> </td>\n",
    "    </tr>\n",
    " </table>   \n",
    "<center><a href=\"https://en.wikipedia.org/wiki/GPT-2\"> Source: Original GPT architecture (left); </a><a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\"> Improved GPT architecture by OpenAI (right) </a></center> <br/>\n",
    "\n",
    "\n",
    "GPT is a Transformer-based architecture and training procedure for natural language processing tasks. The architecture in the orange frame is based on a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning.\n",
    "The training follows a two-stage procedure:\n",
    "First, a language modeling objective is used on the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.\n",
    "The Transformer in the architecture provides a more structured memory for handling long-term dependencies in text, resulting in robust transfer performance across diverse tasks. During the transfer, task-specific input adaptations derived from traversal-style approaches that process structured text input as a single contiguous sequence of tokens are utilized. \n",
    "These adaptations enable the ability to fine-tune effectively with minimal changes to the architecture of the pre-trained model.\n",
    "For more information, please read the paper: `Improving Language Understanding by Generative Pre-Training` from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321e2ad",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "Megatron-GPT 5B is a transformer-based language model. GPT refers to a class of transformer decoder-only models similar to GPT-2 and 3 while 1.3B refers to the total trainable parameter count (1.3 Billion).\n",
    "This model was trained with NeMo Megatron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1efc3",
   "metadata": {},
   "source": [
    "Run the cell below to download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b23d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/megatron-gpt-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bdd6fc",
   "metadata": {},
   "source": [
    "#### Start The Server\n",
    "\n",
    "Before sending prompts to the deployed model, you must ensure that the server is started by following these steps:\n",
    "\n",
    "\n",
    "**Step 1**\n",
    "- Open a terminal from the Jupyter Notebook Launcher or press `Ctrl + Shift + L` and click on the `Terminal.`\n",
    "\n",
    "<img src=\"images/terminal.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a9fdb",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "This step aims to clone the NeMo repository on GitHub into your workspace directory and switch (checkout) to the released version compatible with the NeMo container's PyTorch version. To successfully checkout to the target release, navigate to the directory: `NeMo/examples/nlp/language_modeling.`    \n",
    "\n",
    "- Run `cd /workspace`\n",
    "- Next, run `git clone https://github.com/NVIDIA/NeMo.git`\n",
    "- Run `cd NeMo/examples/nlp/language_modeling`\n",
    "- Run `git checkout v1.20.0`\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/terminal2.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29533ce4",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "This notebook is based on a client-server process. Hence, it is dependent on port number accessibility. In a Bootcamp environment, participants are running the notebook simultaneously on the DGX (MIG instance), which may result in a port access issue (default port is 5555). Therefore, each participant is required to have a unique port number. This step is optional if you run the notebook on your workstation or Laptop.\n",
    "\n",
    "- Among the list of files by the left side of this notebook, you will see the cloned folder named `NeMo.` Double-click the `NeMo` folder and navigate to the `NeMo/examples/nlp/language_modeling/conf` directory. Open the file `megatron_gpt_inference.yaml` and replace the port number on line 32 with the one from the `megatron_gpt_port` file (`port: 5555 #the port number for the inference server`) and press `Ctrl + S` to save the file. This process is illustrated in the screenshot below.\n",
    "\n",
    "\n",
    "<img src=\"images/terminal3.png\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "<img src=\"images/terminal4.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bc70a",
   "metadata": {},
   "source": [
    "\n",
    "**Step 4**\n",
    "\n",
    "- Return to the terminal and execute `python megatron_gpt_eval.py gpt_model_file=/workspace/source_code/nemo_gpt1.3B_fp16.nemo server=True tensor_model_parallel_size=1 trainer.devices=1` as shown in the screenshot below.\n",
    "\n",
    "<img src=\"images/run_inference.JPG\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "On successful execution, you will see the output below, which indicates the server is ready to receive client requests.\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "Predicting DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n",
    "***************************\n",
    "[{'sentences': [\"Q: How are you?\\n\\nSo I'm currently working on a project called BioFlaviarium, It's all about learning and evolving. The main objective of the research was to\", 'Q: How big is the universe?\\n=============\\nThe universe was formed from 4 other hot enough objects after Big Bang. The individual pieces of these little \"stars\" escaped the violence'], 'tokens': [['<|endoftext|>', 'Q', ':', ' How', ' are', ' you', '?', '\\n', '\\n', 'So', ' I', \"'m\", ' currently', ' working', ' on', ' a', ' project', ' called', ' Bio', 'Fl', 'avi', 'arium', ',', ' It', \"'s\", ' all', ' about', ' learning', ' and', ' evolving', '.', ' The', ' main', ' objective', ' of', ' the', ' research', ' was', ' to'], ['<|endoftext|>', 'Q', ':', ' How', ' big', ' is', ' the', ' universe', '?', '\\n', '============', '=', '\\n', 'The', ' universe', ' was', ' formed', ' from', ' 4', ' other', ' hot', ' enough', ' objects', ' after', ' Big', ' Bang', '.', ' The', ' individual', ' pieces', ' of', ' these', ' little', ' \"', 'stars', '\"', ' escaped', ' the', ' violence']], 'logprob': tensor([[ -5.9030,  -4.1616, -12.5939,  -5.1403,  -2.6953,  -4.6859,  -0.2996,\n",
    "          -0.0257,  -4.7703,  -0.9610,  -1.5600,  -3.5917,  -1.9838,  -0.4869,\n",
    "          -0.6473,  -1.5841,  -3.9211,  -7.8870,  -7.9592,  -6.2775,  -4.5393,\n",
    "          -1.5208,  -5.8103,  -0.4281,  -5.9338,  -0.8410,  -4.4464,  -2.7895,\n",
    "          -5.6546,  -1.9112,  -2.9843,  -2.3277,  -3.1398,  -0.8901,  -0.6078,\n",
    "          -7.5004,  -3.5645,  -0.1311],\n",
    "        [ -5.9030,  -4.1616, -12.5939,  -6.1246,  -0.6708,  -0.8335,  -3.6403,\n",
    "          -0.2452,  -0.1454, -10.2567,  -1.0977,  -0.0335,  -4.1342,  -2.0603,\n",
    "          -4.1026,  -3.0159,  -2.3344,  -3.9328,  -7.3087,  -7.3857,  -7.1271,\n",
    "          -2.6817,  -6.4934,  -2.6193,  -0.0155,  -0.8606,  -2.6678,  -7.2869,\n",
    "          -4.4489,  -0.9535,  -2.4077,  -7.4268,  -4.2662,  -3.7853,  -0.0732,\n",
    "          -8.0704,  -1.3797,  -7.9673]]), 'full_logprob': None, 'token_ids': [[50256, 48, 25, 1374, 389, 345, 30, 198, 198, 2396, 314, 1101, 3058, 1762, 319, 257, 1628, 1444, 16024, 7414, 15820, 17756, 11, 632, 338, 477, 546, 4673, 290, 21568, 13, 383, 1388, 9432, 286, 262, 2267, 373, 284], [50256, 48, 25, 1374, 1263, 318, 262, 6881, 30, 198, 25609, 28, 198, 464, 6881, 373, 7042, 422, 604, 584, 3024, 1576, 5563, 706, 4403, 9801, 13, 383, 1981, 5207, 286, 777, 1310, 366, 30783, 1, 13537, 262, 3685]], 'offsets': [[0, 0, 1, 2, 6, 10, 14, 15, 16, 17, 19, 21, 23, 33, 41, 44, 46, 54, 61, 65, 67, 70, 75, 76, 79, 81, 85, 91, 100, 104, 113, 114, 118, 123, 133, 136, 140, 149, 153], [0, 0, 1, 2, 6, 10, 13, 17, 26, 27, 28, 40, 41, 42, 45, 54, 58, 65, 70, 72, 78, 82, 89, 97, 103, 107, 112, 113, 117, 128, 135, 138, 144, 151, 153, 158, 159, 167, 171]]}]\n",
    "***************************\n",
    " * Serving Flask app 'nemo.collections.nlp.modules.common.text_generation_server'\n",
    " * Debug mode: off\n",
    "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
    " * Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:5000\n",
    " * Running on http://10.155.45.137:5000\n",
    "Press CTRL+C to quit\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f9691",
   "metadata": {},
   "source": [
    "#### Run the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1450a5",
   "metadata": {},
   "source": [
    "- Initialize port number to access model deployed on the server\n",
    "  - Copy and paste the port number from the `megatron_gpt_port` file into the cell below. For example: `port_num = 10001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "port_num = \"add port number here\" \n",
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb1d2a",
   "metadata": {},
   "source": [
    "- Create data request method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_data(data):\n",
    "    resp = requests.put('http://localhost:{}/generate'.format(port_num),\n",
    "                        data=json.dumps(data),\n",
    "                        headers=headers)\n",
    "    sentences = resp.json()['sentences']\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c38ba",
   "metadata": {},
   "source": [
    "- Create a data dictionary, including the prompt sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sentences\": [\"Tell me an interesting fact about space travel.\"]*1,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"add_BOS\": True,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"greedy\": False,\n",
    "    \"all_probs\": False,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"min_tokens_to_generate\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6df614",
   "metadata": {},
   "source": [
    "- Execute your prompt request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7250800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = request_data(data)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098bb017",
   "metadata": {},
   "source": [
    "##### One of the random output you may see: \n",
    "````bash\n",
    "['Tell me an interesting fact about space travel.\\nThe first to discover a distant planet was Captain Cook! You\\'re welcome, from the MCG people. [Laughs] Wow, did you do something amazing with that? I remember hearing he \"outlasted\" his other competitors\\' food supplies']\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a8bda",
   "metadata": {},
   "source": [
    "Add your prompt to the story and set a value for the rest of the parameters using the example above. You can play with different values of the parameters to see the effect on the prompt output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your prompt to the story variable\n",
    "story = [\" \"]\n",
    "data[\"sentences\"] = story*1\n",
    "\n",
    "data[\"tokens_to_generate\"] = \n",
    "data[\"temperature\"] =\n",
    "data[\"top_p\"] = \n",
    "data[\"greedy\"] = \n",
    "data[\"repetition_penalty\"]= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5fb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = request_data(data)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d208d8b",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f532b0",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/nvidia/nemo-megatron-gpt-20B\n",
    "1. https://huggingface.co/nvidia/nemo-megatron-gpt-5B\n",
    "1. https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B\n",
    "1. https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a4b72",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54d07e",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Multitask_Prompt_and_PTuning.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"NeMo_Primer.ipynb\">1</a>\n",
    "        <a href=\"Question_Answering.ipynb\">2</a>\n",
    "        <a href=\"Multitask_Prompt_and_PTuning.ipynb\">3</a>\n",
    "        <a>4</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\" \"></a></span>\n",
    "</div>\n",
    "<br/>\n",
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
