{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb26416",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"TRT-LLM-Part1.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"TRT-LLM-Part1.ipynb\">1</a>\n",
    "        <a>2</a>\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191517a-8582-4c90-926b-768d2faf2428",
   "metadata": {},
   "source": [
    "# TensorRT-LLM Backend: Llama-2-7B Deployment using Triton Inference Server \n",
    " \n",
    "\n",
    "\n",
    "The Triton backend for [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).\n",
    "You can learn more about Triton backends in the [backend repo](https://github.com/triton-inference-server/backend).\n",
    "The goal of TensorRT-LLM Backend is to let you serve [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) models with Triton Inference Server. \n",
    "\n",
    "## Building the TensorRT-LLM Backend\n",
    "\n",
    "**NOTE : TensorRT-LLM Backend is already installed if built using a container environment and hence need not be installed**, but there are several ways to access the TensorRT-LLM Backend depending on the deployment scenario, such as: \n",
    "\n",
    "#### Option 1. Run the Docker Container\n",
    "\n",
    "Starting with Triton 23.10 release, Triton includes a container with the TensorRT-LLM\n",
    "Backend and Python Backend. This container should have everything to run a\n",
    "TensorRT-LLM model. You can find this container on the\n",
    "[Triton NGC page](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "\n",
    "#### Option 2. Build via the build.py Script in Server Repo\n",
    "\n",
    "Starting with Triton 23.10 release, you can follow steps described in the\n",
    "[Building With Docker](https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/build.md#building-with-docker)\n",
    "guide and use the\n",
    "[build.py](https://github.com/triton-inference-server/server/blob/main/build.py)\n",
    "script.\n",
    "\n",
    "A sample command to build a Triton Server container with all options enabled is\n",
    "shown below, which will build the same TRT-LLM container as the one on the NGC.\n",
    "\n",
    "```bash\n",
    "BASE_CONTAINER_IMAGE_NAME=nvcr.io/nvidia/tritonserver:23.10-py3-min\n",
    "TENSORRTLLM_BACKEND_REPO_TAG=release/0.5.0\n",
    "PYTHON_BACKEND_REPO_TAG=r23.10\n",
    "\n",
    "# Run the build script. The flags for some features or endpoints can be removed if not needed.\n",
    "./build.py -v --no-container-interactive --enable-logging --enable-stats --enable-tracing \\\n",
    "              --enable-metrics --enable-gpu-metrics --enable-cpu-metrics \\\n",
    "              --filesystem=gcs --filesystem=s3 --filesystem=azure_storage \\\n",
    "              --endpoint=http --endpoint=grpc --endpoint=sagemaker --endpoint=vertex-ai \\\n",
    "              --backend=ensemble --enable-gpu --endpoint=http --endpoint=grpc \\\n",
    "              --image=base,${BASE_CONTAINER_IMAGE_NAME} \\\n",
    "              --backend=tensorrtllm:${TENSORRTLLM_BACKEND_REPO_TAG} \\\n",
    "              --backend=python:${PYTHON_BACKEND_REPO_TAG}\n",
    "```\n",
    "\n",
    "The `BASE_CONTAINER_IMAGE_NAME` is the base image that will be used to build the\n",
    "container. By default it is set to the most recent min image of Triton, on NGC,\n",
    "that matches the Triton release you are building for. You can change it to a\n",
    "different image if needed by setting the `--image` flag like the command below.\n",
    "The `TENSORRTLLM_BACKEND_REPO_TAG` and `PYTHON_BACKEND_REPO_TAG` are the tags of\n",
    "the TensorRT-LLM backend and Python backend repositories that will be used\n",
    "to build the container. You can also remove the features or endpoints that you\n",
    "don't need by removing the corresponding flags.\n",
    "\n",
    "\n",
    "## Using the TensorRT-LLM Backend\n",
    "\n",
    "We will look at 4 steps to serve the TensorRT-LLM model with the Triton TensorRT-LLM Backend on a 1-GPU environment. The example uses the LLaMA-v2 7B model from\n",
    "the [TensorRT-LLM repository](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama).\n",
    "\n",
    "### 1. Prepare TensorRT-LLM engines\n",
    "\n",
    "We can skip this step as we already have the engines ready.  If you are looking for some other model deployment, please follow the [guide](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/) in\n",
    "TensorRT-LLM repository for specific details on how to prepare respective engines for deployment.\n",
    "\n",
    "\n",
    "### 2. Create the model repository\n",
    "\n",
    "There are four models in the `all_models/gpt`\n",
    "directory that will be used in this example:\n",
    "- \"preprocessing\": This model is used for tokenizing, meaning the conversion from prompts(string) to input_ids(list of ints).\n",
    "- \"tensorrt_llm\": This model is a wrapper of your TensorRT-LLM model and is used for inferencing\n",
    "- \"postprocessing\": This model is used for de-tokenizing, meaning the conversion from output_ids(list of ints) to outputs(string).\n",
    "- \"ensemble\": This model is used to chain the three models above together:\n",
    "preprocessing -> tensorrt_llm -> postprocessing\n",
    "\n",
    "To learn more about ensemble model, please see\n",
    "[here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd9403-8361-4a19-9c26-b371c94f2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create the model repository that will be used by the Triton server\n",
    "cd ../../source_code/tensorrtllm_backend \n",
    "mkdir triton_model_repo\n",
    "\n",
    "# Copy the example models to the model repository\n",
    "cp -r all_models/inflight_batcher_llm/* triton_model_repo/\n",
    "\n",
    "# Copy the TRT engine to triton_model_repo/tensorrt_llm/1/\n",
    "cp ../models/llama-engines/fp16/1-gpu/* triton_model_repo/tensorrt_llm/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872f9e7-0a08-4268-8b75-15421cce7e15",
   "metadata": {},
   "source": [
    "### 3. Modify the model configuration\n",
    "\n",
    "The following table shows the fields that need to be modified before deployment:\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/ensemble.png\" width=\"1000\"/>\n",
    "</center></div>\n",
    "\n",
    "\n",
    "**Kindly change the key-value pair using the below table to set the correct configuration, and do not forget to save it.**\n",
    "\n",
    "- a) Kindly open the Pre-processing config file and make the changes listed in the table below *[triton_model_repo/preprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/preprocessing/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description | \n",
    "| :----------------------:| :----------------------: | :-----------------------------: | \n",
    "| 83 | `tokenizer_dir` | The path to the tokenizer for the model. In this example, the path should be set to `/code/tensorrt_llm/source_code/models/llama-2-7b`|\n",
    "| 90 | `tokenizer_type` | The type of the tokenizer for the model, `t5`, `auto` and `llama` are supported. In this example, the type should be set to `llama` |\n",
    "\n",
    "- b) Kindly open the tensorrt config file and make the changes listed in the table below *[triton_model_repo/tensorrt_llm/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description\n",
    "| :----------------------: | :----------------------: | :-----------------------------: |\n",
    "| 32|`decoupled` | Controls streaming. Decoupled mode must be set to `True` if using the streaming option from the client. In this example, we set it to `False` |\n",
    "| 170|`gpt_model_type` | Set to `inflight_fused_batching` when enabling in-flight batching support. To disable in-flight batching, set to `V1` , In this example, we need to set it to `V1`|\n",
    "| 176|`gpt_model_path` | Path to the TensorRT-LLM engines for deployment. In this example, the path should be set to `/code/tensorrt_llm/source_code/tensorrtllm_backend/triton_model_repo/tensorrt_llm/1` |\n",
    "\n",
    "- c) Kindly open the Post-processing config file and make the changes listed in the table below *[triton_model_repo/postprocessing/config.pbtxt](../../source_code/tensorrtllm_backend/triton_model_repo/postprocessing/config.pbtxt)*\n",
    "\n",
    "| Line # | Name | Description\n",
    "| :----------------------:| :----------------------: | :-----------------------------: |\n",
    "| 48 | `tokenizer_dir` | The path to the tokenizer for the model. In this example, the path should be set to `/code/tensorrt_llm/source_code/models/llama-2-7b` |\n",
    "| 55 | `tokenizer_type` | The type of the tokenizer for the model, `t5`, `auto` and `llama` are supported. In this example, the type should be set to `llama` |\n",
    "\n",
    "### 4. Launch Triton server\n",
    "\n",
    "We can launch the Triton server with the following command:\n",
    "\n",
    "- **1. Press `Crtl+Shift+L`**\n",
    "- **2. Kindly Open a New Terminal**\n",
    "- **3. Running the following commands:**\n",
    "```bash\n",
    "# Move to the correct folder of the launch script\n",
    "cd /code/tensorrt_llm/source_code/tensorrtllm_backend\n",
    "# Run the Triton server \n",
    "python3 scripts/launch_triton_server.py --tritonserver \"/opt/tritonserver/bin/tritonserver --http-port $HTTP_PORT --allow-grpc False --allow-metrics False\" \\\n",
    "                                        --world_size=1 \\\n",
    "                                        --model_repo=/code/tensorrt_llm/source_code/tensorrtllm_backend/triton_model_repo\n",
    "```\n",
    "\n",
    "It will take a few minutes to run and when successfully deployed, the server produces logs similar to the following ones.\n",
    "```\n",
    "I1117 17:38:57.345404 1954858 http_server.cc:4497] Started HTTPService at 0.0.0.0:9876\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec076e-f1e9-4b22-b5ef-d750cf2bd136",
   "metadata": {},
   "source": [
    "## Query the server with the Triton-generated endpoint\n",
    "\n",
    "You can query the server using Triton's\n",
    "[generate endpoint](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_generate.md)\n",
    "with a curl command based on the following general format within your client\n",
    "environment/container:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/${MODEL_NAME}/generate -d '{\"{PARAM1_KEY}\": \"{PARAM1_VALUE}\", ... }'\n",
    "```\n",
    "\n",
    "In the case of the models used in this example, you can replace MODEL_NAME with `ensemble`. Examining the\n",
    "ensemble model's config.pbtxt file, you can see that 4 parameters are required to generate a response\n",
    "for this model:\n",
    "\n",
    "- \"text_input\": Input text to generate a response from\n",
    "- \"max_tokens\": The number of requested output tokens\n",
    "- \"bad_words\": A list of bad words (can be empty)\n",
    "- \"stop_words\": A list of stop words (can be empty)\n",
    "\n",
    "Therefore, we can query the server in the following way:\n",
    "\n",
    "```bash\n",
    "curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"What is machine learning?\", \"max_tokens\": 20, \"bad_words\": \"\", \"stop_words\": \"\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b304f9f-6ea7-417d-be84-799ee526b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \"What is machine learning?\", \"max_tokens\": 20, \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4c6d-50e7-454e-97b7-49fe8709b692",
   "metadata": {},
   "source": [
    "Which should return a result similar to (formatted for readability):\n",
    "```json\n",
    "{\n",
    "  \"model_name\": \"ensemble\",\n",
    "  \"model_version\": \"1\",\n",
    "  \"sequence_end\": false,\n",
    "  \"sequence_id\": 0,\n",
    "  \"sequence_start\": false,\n",
    "  \"text_output\": \"What is machine learning? Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly programmed. Machine\"\n",
    "}\n",
    "```\n",
    "\n",
    "You can now try to edit and send an Inference request to the deployed server using a modified version of the above command!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5acac4-32a0-42e7-865c-4f1a39b8323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST localhost:$HTTP_PORT/v2/models/ensemble/generate -d '{\"text_input\": \" xxxxxxxxxxxxxxxx \", \"max_tokens\": xxxx , \"bad_words\": \"\", \"stop_words\": \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e203688-4484-4ece-8672-8034beb20c19",
   "metadata": {},
   "source": [
    "### Querying and Formatting using Python\n",
    "\n",
    "We notice the format is not quite useful, let us now try to do the same via Python, here is a snippet in Python that does the same as above, let us run it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b05da-d23b-4615-984d-4b07cbb23e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Retrieve the HTTP port from environment variables\n",
    "http_port = os.getenv('HTTP_PORT')\n",
    "\n",
    "# Check if HTTP_PORT is set\n",
    "if http_port is None:\n",
    "    print(\"Error: HTTP_PORT environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "# Set the URL with the HTTP port\n",
    "url = f'http://localhost:{http_port}/v2/models/ensemble/generate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f78c1e-1fb0-492c-9f16-4e1ca4091100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"What is machine learning?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 100,\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\\n\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c6991-535d-40df-9a72-7c56a3652900",
   "metadata": {},
   "source": [
    "Expected output is a follows: \n",
    "\n",
    "```text\n",
    "Input: What is machine learning?\n",
    "Output: <s>What is machine learning? Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly programmed. Machine learning is a subset of artificial intelligence. Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly programmed. Machine learning is a subset of artificial intelligence. Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly programmed. Machine learning is a subset of artificial intelligence. Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly\n",
    "```\n",
    "\n",
    "We see that a lot of lines are repeated, let us now truncate this using a Python function and give it another try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6b4b6-d1b5-4605-96f1-11b510d49b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_repetitive_text(text, n_words=10):\n",
    "    words = text.split()\n",
    "    unique_phrases = set()\n",
    "    output_words = []\n",
    "\n",
    "    for i in range(len(words) - n_words + 1):\n",
    "        phrase = ' '.join(words[i:i + n_words])\n",
    "        if phrase in unique_phrases:\n",
    "            # Once a repetition is found, return the text up to that point\n",
    "            return ' '.join(output_words)\n",
    "        unique_phrases.add(phrase)\n",
    "        output_words.append(words[i])\n",
    "\n",
    "    # If no repetition is found, return the entire text\n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f2d35-845c-44ff-ba6b-ee29806ec38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the payload\n",
    "input_text = \"What is machine learning?\"\n",
    "payload = {\n",
    "    \"text_input\": input_text,\n",
    "    \"max_tokens\": 100,  # Increased number of tokens\n",
    "    \"bad_words\": \"\",\n",
    "    \"stop_words\": \"\"\n",
    "}\n",
    "\n",
    "# Make a POST request\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response\n",
    "    data = response.json()\n",
    "    output_text = data.get('text_output')\n",
    "\n",
    "    # Truncate repetitive text\n",
    "    output_text = truncate_repetitive_text(output_text)\n",
    "\n",
    "    # Format and print the output\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Output: {output_text}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de661a82-4940-4b63-adcc-2fc10b7e5b09",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "Input: What is machine learning?\n",
    "Output: <s>What is machine learning? Machine learning is a branch of artificial intelligence that allows computers to learn without being explicitly programmed. Machine learning is a subset of artificial intelligence.\n",
    "```\n",
    "\n",
    "We see that the output is much better, using some more simple functions, we can completely build a post-processing wrapper that cleans the results that we get. Let us now go ahead and shutdown the Triton Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072fe76-c2b5-455e-b898-73972f021437",
   "metadata": {},
   "source": [
    "### Shutdown Triton Server\n",
    "\n",
    "Run the below cell to Shutdown the Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c66d7-26ed-445d-8b13-1658262df01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps aux | grep '[t]ritonserver' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24404461-4e9a-48e8-b74a-d64dbf25ce4b",
   "metadata": {},
   "source": [
    "Congratulations, we've been able to successfully deploy the TensorRT Engine and send an Inference request to the server! \n",
    "\n",
    "There are more features available in [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and [Triton Server](https://github.com/triton-inference-server/tensorrtllm_backend) that would be beneficial to different use-cases. You can refer the respective Github repositories to make use of latest releases and features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b63bd5-f402-4337-b439-dd51701f49a6",
   "metadata": {},
   "source": [
    "## Licensing\n",
    "Copyright Â© 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a012ea",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"TRT-LLM-Part1.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"TRT-LLM-Part1.ipynb\">1</a>\n",
    "        <a>2</a>\n",
    "    </span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
