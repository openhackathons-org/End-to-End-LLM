{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e65228",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "        <a>1</a>\n",
    "        <a href=\"TRT-LLM-Part2.ipynb\">2</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"TRT-LLM-Part2.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebbdbc0-1287-45a7-a6d7-6d0cb489eb0f",
   "metadata": {},
   "source": [
    "# TensorRT-LLM: Llama-2-7B Engine Generation  \n",
    "\n",
    "### TensorRT Toolbox for LLMs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a38d6-559d-4b3b-8527-b97500fd885b",
   "metadata": {},
   "source": [
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server.  Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism).\n",
    "\n",
    "To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes. TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the SmoothQuant technique.\n",
    "\n",
    "<div><center>\n",
    "<img src=\"images/inference-visual-tensor-rt-llm.png\" width=\"1000\"/>\n",
    "</center></div>\n",
    "\n",
    "\n",
    "\n",
    "The Python API of TensorRT-LLM is architectured to look similar to the PyTorch API. It provides users with a functional module containing functions like einsum, softmax, matmul or view. The layer module bundles useful building blocks to assemble LLMs; like an Attention block, a MLP or the entire Transformer layer. Model-specific components, like GPTAttention or BertAttention, can be found in the model module.\n",
    "\n",
    "\n",
    "### Models\n",
    "\n",
    "TensorRT-LLM provides users with predefined models that can easily be modified and extended. The current version of TensorRT-LLM ( Release v0.5.0 ) supports: \n",
    "\n",
    "* [Baichuan](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/baichuan)\n",
    "* [Bert](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/bert)\n",
    "* [Blip2](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/blip2)\n",
    "* [BLOOM](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/bloom)\n",
    "* [ChatGLM-6B](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/chatglm6b)\n",
    "* [ChatGLM2-6B](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/chatglm2-6b/)\n",
    "* [Falcon](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/falcon)\n",
    "* [GPT](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gpt)\n",
    "* [GPT-J](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gptj)\n",
    "* [GPT-Nemo](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gpt)\n",
    "* [GPT-NeoX](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gptneox)\n",
    "* [LLaMA](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/llama)\n",
    "* [LLaMA-v2](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/llama)\n",
    "* [MPT](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/mpt)\n",
    "* [OPT](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/opt)\n",
    "* [SantaCoder](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gpt)\n",
    "* [StarCoder](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0/examples/gpt)\n",
    "\n",
    "\n",
    "### Key Features\n",
    "\n",
    "TensorRT-LLM contains examples that implement the following features.\n",
    "\n",
    "* Multi-head Attention([MHA](https://arxiv.org/abs/1706.03762))\n",
    "* Multi-query Attention ([MQA](https://arxiv.org/abs/1911.02150))\n",
    "* Group-query Attention([GQA](https://arxiv.org/abs/2307.09288))\n",
    "* In-flight Batching\n",
    "* Paged KV Cache for the Attention\n",
    "* Tensor Parallelism\n",
    "* Pipeline Parallelism\n",
    "* INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)\n",
    "* [SmoothQuant](https://arxiv.org/abs/2211.10438)\n",
    "* [GPTQ](https://arxiv.org/abs/2210.17323)\n",
    "* [AWQ](https://arxiv.org/abs/2306.00978)\n",
    "* [FP8](https://arxiv.org/abs/2209.05433)\n",
    "* Greedy-search\n",
    "* Beam-search\n",
    "* RoPE\n",
    "\n",
    "In this release of TensorRT-LLM, some of the features are not enabled for all\n",
    "the models listed in the [examples](examples/.) folder.\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "### Performance comparison\n",
    "\n",
    "Summarizing articles is just one of the many applications of LLMs. The following benchmarks show performance improvements brought by TensorRT-LLM on the latest NVIDIA Hopper architecture. \n",
    "\n",
    "The following figures reflect article summarization using an NVIDIA A100 and NVIDIA H100 with CNN/Daily Mail, a well-known dataset for evaluating summarization performance\n",
    "<div><center>\n",
    "<img src=\"images/Performance-Llama2.png\" width=\"1000\"/>\n",
    "</center></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89fc7e-691d-4aa2-9e31-598a93919191",
   "metadata": {},
   "source": [
    "## Hands-on: LLaMA-v2 7B Engine generation using TensorRT-LLM\n",
    "\n",
    "We have 3 steps that are needed to create our Engine file for Inferencing, they are as follows: \n",
    "\n",
    "- **Step 1: Set up TRT-LLM Environment**\n",
    "- **Step 2: Build Engine file for the LLM Model**\n",
    "- **Step 3: Test the Inference Engine before deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6ac24-3a8f-4f89-a46c-f1789e161704",
   "metadata": {},
   "source": [
    "### Step 1. Setup TRT-LLM Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e585f-e9ef-4b3b-b057-c5eaf37caec8",
   "metadata": {},
   "source": [
    "TensorRT-LLM must be built from source depending on the Hardware environment we use it in to achieve ideal performance. \n",
    "\n",
    "**Note: This step should be skipped if you have already built this via Dockerfile or Singularity recipe or running this in a bootcamp environment.**\n",
    "\n",
    "There are multiple ways to set up TensorRT-LLM; one recommended approach is as follows: \n",
    "\n",
    "\n",
    "#### 1). Download sources \n",
    "\n",
    "```bash\n",
    "# TensorRT-LLM uses git-lfs, which needs to be installed in advance.\n",
    "apt-get update && apt-get -y install git git-lfs cmake\n",
    "\n",
    "# Clone Repository and set it up\n",
    "git clone https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "cd TensorRT-LLM\n",
    "git submodule update --init --recursive\n",
    "git lfs install\n",
    "git lfs pull\n",
    "```\n",
    "\n",
    "#### 2). Using Docker \n",
    "\n",
    "```bash\n",
    "# Using the make to build using Docker\n",
    "make -C docker release_build\n",
    "```\n",
    "\n",
    "For a complete list of options to set up TRT-LLM kindly refer [**here**](https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/installation.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755f4f4-3699-4f4a-92cc-ed91bd2ca4ae",
   "metadata": {},
   "source": [
    "### Step 2. Build Engine file for the LLM Model.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The TensorRT-LLM LLaMA implementation can be found in [examples/llama/model.py](../../source_code/tensorrtllm_backend/tensorrt_llm/tensorrt_llm/models/llama/model.py). The TensorRT-LLM LLaMA example code is located in [`examples/llama`](../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py). There are three main files in that folder:\n",
    "\n",
    " * [`build.py`](../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py) to build the [TensorRT](https://developer.nvidia.com/tensorrt) engine(s) needed to run the LLaMA model,\n",
    " * [`run.py`](../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/run.py) to run the inference on an input text,\n",
    " * [`summarize.py`](../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/summarize.py) to summarize the articles in the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset using the model.\n",
    "\n",
    "#### Support Matrix\n",
    "  * FP16\n",
    "  * FP8\n",
    "  * INT8 & INT4 Weight-Only\n",
    "  * FP8 KV CACHE\n",
    "  * Tensor Parallel\n",
    "  * STRONGLY TYPED\n",
    "\n",
    "#### Build TensorRT engine(s)\n",
    "\n",
    "Need to prepare the HF LLaMA checkpoint first by following the guides [here](https://huggingface.co/docs/transformers/main/en/model_doc/llama). \n",
    "\n",
    "**Note: But in case if case of the bootcamp environment the model is already downloaded and provided to you. If you are running this on your own machine, kindly obtain the model from [here](https://github.com/facebookresearch/llama?fbclid=IwAR3Och2meRdeP-Z0KKA5vU75Gy9MM0Y55-IUAGX-aKuS59-8utRkVXCw45Q) and place it in the `/source_code/models/llama-2-7b` folder.**\n",
    "\n",
    "TensorRT-LLM LLaMA builds TensorRT engine(s) from checkpoint. If no checkpoint directory is specified, TensorRT-LLM will build engine(s) with dummy weights.\n",
    "\n",
    "Normally `build.py` only requires single GPU, but if you've already got all the GPUs needed while inferencing, you could enable parallelly building to make the engine building process faster by adding `--parallel_build` argument. Please note that currently `parallel_build` feature only supports single node.\n",
    "\n",
    "Let us now build it using a Single GPU, we can use the following command: \n",
    "\n",
    "```bash\n",
    "# Build the LLaMA-v2 7B model using a single GPU and FP16 from Meta Checkpoint\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                  --meta_ckpt_dir ../../source_code/models/llama-2-7b/ \\               ## Location of the Meta Checkpoint file\n",
    "                  --remove_input_padding \\                                          ## Remove Input padding\n",
    "                  --max_batch_size 1 \\                                              ## Set batch size to 1\n",
    "                  --dtype float16 \\                                                 ## Set Precision\n",
    "                  --use_gpt_attention_plugin float16 \\ \n",
    "                  --use_gemm_plugin float16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/fp16/1-gpu/      ## Location to store the engine file.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddb01e9-001c-436d-a44a-965109ff1210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the LLaMA-v2 7B model using a single GPU and FP16 from Meta Checkpoint\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "                  --meta_ckpt_dir ../../source_code/models/llama-2-7b/ \\\n",
    "                  --dtype float16 \\\n",
    "                  --remove_input_padding \\\n",
    "                  --max_batch_size 1 \\\n",
    "                  --use_gpt_attention_plugin float16 \\\n",
    "                  --use_gemm_plugin float16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f200b-f1bd-4319-a34e-0f0cd8cdf152",
   "metadata": {},
   "source": [
    "This should ideally complete in around 2 minutes time.\n",
    "\n",
    "\n",
    "#### Other ways to build the model: \n",
    "\n",
    "```bash\n",
    "# Build the LLaMA-v2 7B model using a single GPU and FP16 from HuggingFace Checkpoint\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                  --model_dir ../../source_code/models/llama-2-7b/ \\                   ## Location of the HF Checkpoint file\n",
    "                  --dtype float16 \\                                                 ## Set Precision\n",
    "                  --use_gpt_attention_plugin float16 \\ \n",
    "                  --use_gemm_plugin float16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/fp16/1-gpu/      ## Location to store the engine file.\n",
    "\n",
    "\n",
    "# Build the LLaMA-v2 7B model using a single GPU and BF16 from HuggingFace Checkpoint\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                  --model_dir ../../source_code/models/llama-2-7b/ \\                   ## Location of the HF Checkpoint file\n",
    "                  --dtype bfloat16 \\                                                ## Set Precision\n",
    "                  --use_gpt_attention_plugin bfloat16 \\ \n",
    "                  --use_gemm_plugin bfloat16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/bf16/1-gpu/      ## Location to store the engine file.\n",
    "\n",
    "# Build the LLaMA-v2 7B model using a Single GPU and apply INT8 weight-only quantization.\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                --model_dir ../../source_code/models/llama-2-7b/ \\                     ## Location of the HF Checkpoint file\n",
    "                --dtype float16 \\\n",
    "                --use_gpt_attention_plugin float16 \\\n",
    "                --use_gemm_plugin float16 \\ \n",
    "                --use_weight_only \\                                                 ## Use Weight only quantisation\n",
    "                --output_dir ../../source_code/models/llama-engines/int8/1-gpu/\n",
    "\n",
    "\n",
    "# Build LLaMA-v2 7B using 2-way tensor parallelism.\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                  --model_dir ../../source_code/models/llama-2-7b/ \\                   ## Location of the HF Checkpoint file\n",
    "                  --dtype float16 \\                                                 ## Set Precision\n",
    "                  --use_gpt_attention_plugin float16 \\ \n",
    "                  --use_gemm_plugin float16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/fp16/2-gpu/      ## Location to store the engine file.\n",
    "                  --world_size 2                                                    ## Set Tensor Parallelism\n",
    "\n",
    "# Build LLaMA-v2 30B using 2-way tensor parallelism.\n",
    "!python ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\   ## Location of the build file\n",
    "                  --model_dir ../../source_code/models/llama-2-30b/ \\                  ## Location of the HF Checkpoint file\n",
    "                  --dtype float16 \\                                                 ## Set Precision\n",
    "                  --use_gpt_attention_plugin float16 \\ \n",
    "                  --use_gemm_plugin float16 \\\n",
    "                  --output_dir ../../source_code/models/llama-engines/fp16/2-gpu/      ## Location to store the engine file.\n",
    "                  --world_size 2                                                    ## Set Tensor Parallelism\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b010624-9153-4337-ac10-25e22e2499cd",
   "metadata": {},
   "source": [
    "### Step 3: Test Inference Engine before Deployment\n",
    "\n",
    "Let us test TensorRT-LLM LLaMA-v2 model using the engines generated by `build.py`\n",
    "\n",
    "```bash\n",
    "# Run Inference using the run.py scipt for the created Engine File\n",
    "!python3 ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\      ## Location of the Run file\n",
    "                --max_output_len=50 \\                                                 ## Maximum output Token length\n",
    "                --tokenizer_dir ../../source_code/models/llama-2-7b/ \\                   ## Tokenizer Directory\n",
    "                --engine_dir=../../source_code/models/llama-engines/fp16/1-gpu/          ## Engine output Directory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040175e8-f7fb-44e3-ab0c-e6a4556787be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../../source_code/tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "                --max_output_len=50 \\\n",
    "                --tokenizer_dir ../../source_code/models/llama-2-7b/ \\\n",
    "                --engine_dir=../../source_code/models/llama-engines/fp16/1-gpu/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c1d71-763f-4134-adaf-6e3602827dd5",
   "metadata": {},
   "source": [
    "Expected output is as follows: \n",
    "\n",
    "```bash\n",
    "Running the float16 engine ...\n",
    "Input: \"Born in north-east France, Soyer trained as a\"\n",
    "Output: \"chef in Paris and London before moving to the United States in 1851. He was appointed chef to the Union Army in 1862, and his cookbook, The Gastronomic Regulations and Hints for the\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36a92f-720a-47b6-9b5b-bc8a98c33038",
   "metadata": {},
   "source": [
    "We have successfully created our Engine file and ran a simple test on it!.  In the next notebook, let us now deploy and serve the model using Triton Inference server and send sample inference request to test it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27edc8c-7e21-4f04-8d8a-4d834b114f99",
   "metadata": {},
   "source": [
    "## Licensing\n",
    "Copyright Â© 2023 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials may include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106eea9",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 75%; text-align: center;\">\n",
    "        <a>1</a>\n",
    "        <a href=\"TRT-LLM-Part2.ipynb\">2</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 23%; text-align: right;\"><a href=\"TRT-LLM-Part2.ipynb\">Next Notebook</a></span>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
