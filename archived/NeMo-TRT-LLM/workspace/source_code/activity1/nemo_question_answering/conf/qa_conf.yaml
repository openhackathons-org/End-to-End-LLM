# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

pretrained_model: null # pretrained model from list_available_models()
do_training: true # true for training mode, false for testing 
trainer:
  devices: [0] # 0 for CPU, or list of the GPUs to use e.g. [0, 1] or [0]
  num_nodes: 1
  max_epochs: 3
  max_steps: -1 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  gradient_clip_val: 1.0
  precision: 16 # should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  log_every_n_steps: 5  # interval of logging.
  val_check_interval: 1.0  # set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: null # the path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # provided by exp_manager
  logger: False  # provided by exp_manager
  strategy: ddp

model:
  tensor_model_parallel_size: 1
  nemo_path: null # filename to save the model and associated artifacts to .nemo file
  library: huggingface # [huggingface, megatron]. Used by S2SQAModel and GPTQAModel
  save_model: False # save validation model checkpoints

  tokens_to_generate: 32 # used by S2SQAModel and GPTQAModel to limit number of generated tokens

  dataset:
    version_2_with_negative: true # if true, dataset contains some questions that do not have an answer
    doc_stride: 128 # stride for splitting long documents into chunks
    max_query_length: 64
    max_seq_length: 512 # max sequence length for input to the model
    max_answer_length: 30 # max ground truth answer length
    use_cache: false
    do_lower_case: true

    # if true, context spans/chunks that do not contain answer are treated as unanswerable,
    #   useful for extractive datasets like SQuAD
    # if false, all context spans/chunks are treated as relevant for answering given query,
    #   useful for generative datasets where answer is not necessarily in the context
    # used by S2SQAModel and GPTQAModel   
    check_if_answer_in_context: true

    # if all, keep all doc spans
    # if only_positive, keep doc spans containing answer only
    # if limited_negative, keep 10 doc spans closest to answer per question
    # used by BERTQAModel
    keep_doc_spans: all # [all, only_positive, limited_negative]

    null_score_diff_threshold: 0.0 # If null_score - best_non_null is greater than the threshold predict null.
    n_best_size: 20

    num_workers: 1
    pin_memory: false
    drop_last: false

  train_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: true
    num_samples: -1

    # default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  validation_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: false
    num_samples: -1

    # default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  test_ds:
    file: null # .json file
    batch_size: 24 # per GPU
    shuffle: false
    num_samples: -1
    
    # default values for the following params are retrieved from dataset config section, but you may override them
    num_workers: ${model.dataset.num_workers}
    drop_last: ${model.dataset.drop_last}
    pin_memory: ${model.dataset.pin_memory}

  language_model:
    pretrained_model_name: bert-base-uncased # main config to select model (between bert, gpt2, t5/bart based models)
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null

  token_classifier: # used only by BERTQAModel for defining the extractive QA head
    num_layers: 1
    dropout: 0.
    num_classes: 2
    activation: relu
    log_softmax: false
    use_transformer_init: true

  tokenizer:
    tokenizer_name: ${model.language_model.pretrained_model_name} # tokenizer that inherits from TokenizerSpec
    vocab_file: null # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece

    # expand the following to a dictionary if special tokens need to be added.
    # only necessary for adding transformer/bert-specific special tokens to tokenizer if the tokenizer does not already have these inherently.
    special_tokens: null

  optim:
    name: adamw
    lr: 5e-5

    # optimizer arguments
    betas: [0.9, 0.999]
    weight_decay: 0.

    # scheduler setup
    sched:
      name: SquareRootAnnealing

      # scheduler params
      warmup_steps: null
      warmup_ratio: 0.
      last_epoch: -1

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

exp_manager:
  exp_dir: null  # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
  name: "QnA"  # the name of your model
  create_wandb_logger: False
  wandb_logger_kwargs:
    name: ???
    project: QnA
  create_tensorboard_logger: True  # whether you want exp_manger to create a tb logger
  create_checkpoint_callback: True  # whether you want exp_manager to create a modelcheckpoint callback
  resume_if_exists: false
  resume_ignore_no_checkpoint: false