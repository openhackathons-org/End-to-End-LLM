{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31553908",
   "metadata": {},
   "source": [
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>\n",
    "\n",
    " \n",
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"Multitask_Prompt_and_PTuning.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "        <a href=\"NeMo_Primer.ipynb\">9</a>\n",
    "        <a href=\"Multitask_Prompt_and_PTuning.ipynb\">10</a>\n",
    "        <a>11</a>\n",
    "    </span>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e05bb1",
   "metadata": {},
   "source": [
    "## NeMo Megatron-GPT 1.3B Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49267aa6",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "NVIDIA NeMo™ framework is an end-to-end, cloud-native enterprise framework to build, customize, and deploy generative AI models with billions of parameters. The NeMo framework supports the development of text-to-text, text-to-image, and image-to-image foundation models.\n",
    "NeMo framework makes it possible for researchers to easily compose complex neural network architectures for conversational AI using reusable components - Neural Modules. Neural Modules are conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations.\n",
    "\n",
    "<img src=\"images/NeMo_Framework.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37fcad",
   "metadata": {},
   "source": [
    "### State-of-the-Art Training Techniques\n",
    "\n",
    "The NeMo framework delivers high levels of training efficiency, making training of large-scale foundation models possible, using 3D parallelism techniques such as:\n",
    "- Tensor parallelism to scale models within nodes\n",
    "- Data and pipeline parallelism to scale data and models across thousands of GPUs \n",
    "- Sequence parallelism to distribute activation memory across tensor parallel devices\n",
    "- In addition, selective activation recomputing optimizes recomputation and memory usage across tensor parallel devices during backpropagation.\n",
    "\n",
    "<img src=\"images/NeMo_technique.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044c7c7",
   "metadata": {},
   "source": [
    "### Megatron\n",
    "\n",
    "Megatron is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. NeMo Megatron supports several types of models:\n",
    "- GPT-style models (decoder only)\n",
    "\n",
    "- T5/BART/UL2-style models (encoder-decoder)\n",
    "\n",
    "- BERT-style models (encoder only)\n",
    "\n",
    "- RETRO model (decoder only)\n",
    "\n",
    "\n",
    "### GPT (Generative Pre-trained Transformer) Architecture\n",
    "<table>\n",
    "    <th></th><th></th>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/gpt_arc_original.png\"></td><td><img src=\"images/gpt_arc_openai.jpg\"> </td>\n",
    "    </tr>\n",
    " </table>   \n",
    "<center><a href=\"https://en.wikipedia.org/wiki/GPT-2\"> Source: Original GPT architecture (left); </a><a href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\"> Improved GPT architecture by OpenAI (right) </a></center> <br/>\n",
    "\n",
    "\n",
    "GPT is a Transformer-based architecture and training procedure for natural language processing tasks. The architecture in the orange frame is based on a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised fine-tuning.\r\n",
    "The training follows a two-stage procedure:\r\n",
    "First, a language modeling objective is used on the unlabeled data to learn the initial parameters of a neural network model. Subsequently, these parameters are adapted to a target task using the corresponding supervised objective.\r\n",
    "The Transformer in the architecture provides a more structured memory for handling long-term dependencies in text which resulted in robust transfer performance across diverse tasks. During the transfer, task-specific input adaptations derived from traversal-style approaches which process structured text input as a single contiguous sequence of tokens were utilized. \r\n",
    "These adaptations enable the ability to fine-tune effectively with minimal changes to the architecture of the pre-trained model.\r\n",
    "For more information, please read the paper: Improving Language Understanding by Generative Pre-Training from Ope\n",
    "em Op\r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2836d",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "Megatron-GPT 1.3B is a transformer-based language model. GPT refers to a class of transformer decoder-only models similar to GPT-2 and 3 while 1.3B refers to the total trainable parameter count (1.3 Billion).\n",
    "This model was trained with NeMo Megatron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdeb933",
   "metadata": {},
   "source": [
    "#### Start The Server\n",
    "\n",
    "Before sending prompts to the deployed model, you must ensure that the sever is started by following this steps:\n",
    "\n",
    "- Open a terminal\n",
    "- Run `git clone https://github.com/NVIDIA/NeMo.git` \n",
    "- Navigate into the directory by running `cd /workspace/NeMo/examples/nlp/language_modeling`\n",
    "- Run `git checkout v1.17.0`\n",
    "- Execute `python megatron_gpt_eval.py gpt_model_file=nemo_gpt1.3B_fp16.nemo server=True tensor_model_parallel_size=1 trainer.devices=1`\n",
    "\n",
    "On successful execution, you will see the output below which indicates the server is ready to received client request\n",
    "\n",
    "```python\n",
    "...\n",
    "\n",
    "Predicting DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n",
    "***************************\n",
    "[{'sentences': [\"Q: How are you?\\n\\nSo I'm currently working on a project called BioFlaviarium, It's all about learning and evolving. The main objective of the research was to\", 'Q: How big is the universe?\\n=============\\nThe universe was formed from 4 other hot enough objects after Big Bang. The individual pieces of these little \"stars\" escaped the violence'], 'tokens': [['<|endoftext|>', 'Q', ':', ' How', ' are', ' you', '?', '\\n', '\\n', 'So', ' I', \"'m\", ' currently', ' working', ' on', ' a', ' project', ' called', ' Bio', 'Fl', 'avi', 'arium', ',', ' It', \"'s\", ' all', ' about', ' learning', ' and', ' evolving', '.', ' The', ' main', ' objective', ' of', ' the', ' research', ' was', ' to'], ['<|endoftext|>', 'Q', ':', ' How', ' big', ' is', ' the', ' universe', '?', '\\n', '============', '=', '\\n', 'The', ' universe', ' was', ' formed', ' from', ' 4', ' other', ' hot', ' enough', ' objects', ' after', ' Big', ' Bang', '.', ' The', ' individual', ' pieces', ' of', ' these', ' little', ' \"', 'stars', '\"', ' escaped', ' the', ' violence']], 'logprob': tensor([[ -5.9030,  -4.1616, -12.5939,  -5.1403,  -2.6953,  -4.6859,  -0.2996,\n",
    "          -0.0257,  -4.7703,  -0.9610,  -1.5600,  -3.5917,  -1.9838,  -0.4869,\n",
    "          -0.6473,  -1.5841,  -3.9211,  -7.8870,  -7.9592,  -6.2775,  -4.5393,\n",
    "          -1.5208,  -5.8103,  -0.4281,  -5.9338,  -0.8410,  -4.4464,  -2.7895,\n",
    "          -5.6546,  -1.9112,  -2.9843,  -2.3277,  -3.1398,  -0.8901,  -0.6078,\n",
    "          -7.5004,  -3.5645,  -0.1311],\n",
    "        [ -5.9030,  -4.1616, -12.5939,  -6.1246,  -0.6708,  -0.8335,  -3.6403,\n",
    "          -0.2452,  -0.1454, -10.2567,  -1.0977,  -0.0335,  -4.1342,  -2.0603,\n",
    "          -4.1026,  -3.0159,  -2.3344,  -3.9328,  -7.3087,  -7.3857,  -7.1271,\n",
    "          -2.6817,  -6.4934,  -2.6193,  -0.0155,  -0.8606,  -2.6678,  -7.2869,\n",
    "          -4.4489,  -0.9535,  -2.4077,  -7.4268,  -4.2662,  -3.7853,  -0.0732,\n",
    "          -8.0704,  -1.3797,  -7.9673]]), 'full_logprob': None, 'token_ids': [[50256, 48, 25, 1374, 389, 345, 30, 198, 198, 2396, 314, 1101, 3058, 1762, 319, 257, 1628, 1444, 16024, 7414, 15820, 17756, 11, 632, 338, 477, 546, 4673, 290, 21568, 13, 383, 1388, 9432, 286, 262, 2267, 373, 284], [50256, 48, 25, 1374, 1263, 318, 262, 6881, 30, 198, 25609, 28, 198, 464, 6881, 373, 7042, 422, 604, 584, 3024, 1576, 5563, 706, 4403, 9801, 13, 383, 1981, 5207, 286, 777, 1310, 366, 30783, 1, 13537, 262, 3685]], 'offsets': [[0, 0, 1, 2, 6, 10, 14, 15, 16, 17, 19, 21, 23, 33, 41, 44, 46, 54, 61, 65, 67, 70, 75, 76, 79, 81, 85, 91, 100, 104, 113, 114, 118, 123, 133, 136, 140, 149, 153], [0, 0, 1, 2, 6, 10, 13, 17, 26, 27, 28, 40, 41, 42, 45, 54, 58, 65, 70, 72, 78, 82, 89, 97, 103, 107, 112, 113, 117, 128, 135, 138, 144, 151, 153, 158, 159, 167, 171]]}]\n",
    "***************************\n",
    " * Serving Flask app 'nemo.collections.nlp.modules.common.text_generation_server'\n",
    " * Debug mode: off\n",
    "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
    " * Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:5000\n",
    " * Running on http://10.155.45.137:5000\n",
    "Press CTRL+C to quit\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f2b08",
   "metadata": {},
   "source": [
    "#### Run the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aaa63e",
   "metadata": {},
   "source": [
    "- Initialize port number to access model deployed on the server\n",
    "  - Copy and paste the port number from the `megatron_gpt_port` file into the cell below\n",
    "  - Navigate to the directory `NeMo/examples/nlp/language_modeling/conf`. Open the file `megatron_gpt_inference.yaml` and replace the port number on line 32 with the one from the `megatron_gpt_port` file (`port: 5000 # the port number for the inference server`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "port_num = \"paste port number here\" \n",
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb462f",
   "metadata": {},
   "source": [
    "- Create data request method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_data(data):\n",
    "    resp = requests.put('http://localhost:{}/generate'.format(port_num),\n",
    "                        data=json.dumps(data),\n",
    "                        headers=headers)\n",
    "    sentences = resp.json()['sentences']\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93420c1a",
   "metadata": {},
   "source": [
    "- Create data dictionary including the prompt sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sentences\": [\"Tell me an interesting fact about space travel.\"]*1,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"temperature\": 1.0,\n",
    "    \"add_BOS\": True,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"greedy\": False,\n",
    "    \"all_probs\": False,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"min_tokens_to_generate\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a8d8d",
   "metadata": {},
   "source": [
    "- Execute your prompt request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = request_data(data)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b8ccd",
   "metadata": {},
   "source": [
    "##### One of the random output you may see: \n",
    "````bash\n",
    "['Tell me an interesting fact about space travel.\\nThe first to discover a distant planet was Captain Cook! You\\'re welcome, from the MCG people. [Laughs] Wow, did you do something amazing with that? I remember hearing he \"outlasted\" his other competitors\\' food supplies']\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215d71c",
   "metadata": {},
   "source": [
    " Add your prompt to the story variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a37a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your prompt to the story variable\n",
    "story = [\"\"]\n",
    "data[\"sentences\"] = story*1\n",
    "#data[\"tokens_to_generate\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be83342",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = request_data(data)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be4f78",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e49e1",
   "metadata": {},
   "source": [
    "1. https://huggingface.co/nvidia/nemo-megatron-gpt-20B\n",
    "1. https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B\n",
    "1. https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c7f68",
   "metadata": {},
   "source": [
    "---\n",
    "## Licensing\n",
    "\n",
    "Copyright © 2022 OpenACC-Standard.org. This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). These materials include references to hardware and software developed by other entities; all applicable licensing and copyrights apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d66531",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <span style=\"float: left; width: 33%; text-align: left;\"><a href=\"../challenge.ipynb\">Previous Notebook</a></span>\n",
    "    <span style=\"float: left; width: 33%; text-align: center;\">\n",
    "         <a href=\"NeMo_Primer.ipynb\">9</a>\n",
    "        <a href=\"Multitask_Prompt_and_PTuning.ipynb\">10</a>\n",
    "        <a>11</a>\n",
    "    </span>\n",
    "    <span style=\"float: left; width: 33%; text-align: right;\"><a href=\"NeMo_Primer.ipynb\">Next Notebook </a></span>\n",
    "</div>\n",
    "\n",
    "<p> <center> <a href=\"../../Start_Here.ipynb\">Home Page</a> </center> </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
